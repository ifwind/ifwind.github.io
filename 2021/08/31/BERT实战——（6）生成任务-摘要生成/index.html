<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>BERT实战——（6）生成任务-摘要生成 | 冬于的博客</title><meta name="keywords" content="深度学习,NLP,BERT"><meta name="author" content="冬于"><meta name="copyright" content="冬于"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="BERT实战——（6）生成任务-摘要生成  引言 这一篇将介绍如何使用 🤗 Transformers代码库中的模型来解决生成任务中的摘要生成问题。  任务介绍 摘要生成，用一些精炼的话（摘要）来概括整片文章的大意，用户通过读文摘就可以了解到原文要表达。 主要分为以下几个部分：  数据加载； 数据预处理； 微调预训练模型：使用transformer中的**Seq2SeqTrainer接口**对预">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT实战——（6）生成任务-摘要生成">
<meta property="og:url" content="https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/index.html">
<meta property="og:site_name" content="冬于的博客">
<meta property="og:description" content="BERT实战——（6）生成任务-摘要生成  引言 这一篇将介绍如何使用 🤗 Transformers代码库中的模型来解决生成任务中的摘要生成问题。  任务介绍 摘要生成，用一些精炼的话（摘要）来概括整片文章的大意，用户通过读文摘就可以了解到原文要表达。 主要分为以下几个部分：  数据加载； 数据预处理； 微调预训练模型：使用transformer中的**Seq2SeqTrainer接口**对预">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg">
<meta property="article:published_time" content="2021-08-31T05:59:10.000Z">
<meta property="article:modified_time" content="2021-09-01T12:01:18.351Z">
<meta property="article:author" content="冬于">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LVK1P2D38K","apiKey":"8cbbb0bcbb5c7448f68b4fae01d4ccd5","indexName":"DongYu","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BERT实战——（6）生成任务-摘要生成',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-09-01 20:01:18'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">89</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">54</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">冬于的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BERT实战——（6）生成任务-摘要生成</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-31T05:59:10.000Z" title="发表于 2021-08-31 13:59:10">2021-08-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-01T12:01:18.351Z" title="更新于 2021-09-01 20:01:18">2021-09-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2,623</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="BERT实战——（6）生成任务-摘要生成"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="bert实战6生成任务-摘要生成"><a class="markdownIt-Anchor" href="#bert实战6生成任务-摘要生成"></a> BERT实战——（6）生成任务-摘要生成</h1>
<h2 id="引言"><a class="markdownIt-Anchor" href="#引言"></a> 引言</h2>
<p>这一篇将介绍如何使用 <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">🤗 Transformers</a>代码库中的模型来解决<strong>生成任务中的摘要生成问题。</strong></p>
<h3 id="任务介绍"><a class="markdownIt-Anchor" href="#任务介绍"></a> 任务介绍</h3>
<p>摘要生成，用一些精炼的话（摘要）来概括整片文章的大意，用户通过读文摘就可以了解到原文要表达。</p>
<p>主要分为以下几个部分：</p>
<ol>
<li>数据加载；</li>
<li>数据预处理；</li>
<li>微调预训练模型：使用transformer中的**<code>Seq2SeqTrainer</code>接口**对预训练模型进行微调（注意这里是<code>Seq2SeqTrainer</code>接口，之前的任务都是调用<code>Trainer</code>接口）。</li>
</ol>
<h3 id="前期准备"><a class="markdownIt-Anchor" href="#前期准备"></a> 前期准备</h3>
<p>安装以下库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install datasets transformers rouge-score nltk</span><br><span class="line"><span class="comment">#transformers==4.9.2</span></span><br><span class="line"><span class="comment">#datasets==1.11.0</span></span><br><span class="line"><span class="comment">#rouge-score==0.0.4</span></span><br><span class="line"><span class="comment">#nltk==3.6.2</span></span><br></pre></td></tr></table></figure>
<h2 id="数据加载"><a class="markdownIt-Anchor" href="#数据加载"></a> 数据加载</h2>
<h3 id="数据集介绍"><a class="markdownIt-Anchor" href="#数据集介绍"></a> 数据集介绍</h3>
<p>我们使用<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.08745.pdf">XSum dataset</a>数据集，其中包含了多篇BBC的文章和一句对应的摘要。</p>
<h3 id="加载数据"><a class="markdownIt-Anchor" href="#加载数据"></a> 加载数据</h3>
<p>该数据的加载方式在transformers库中进行了封装，我们可以通过以下语句进行数据加载：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;xsum&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>给定一个数据切分的key（train、validation或者test）和下标即可查看数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># &#123;&#x27;document&#x27;: &#x27;Recent reports have linked some France-based players with returns to Wales.\n&quot;I\&#x27;ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\&#x27;d rather spend that money on keeping players in Wales,&quot; said Davies.\nThe WRU provides £2m to the fund and £1.3m comes from the regions.\nFormer Wales and British and Irish Lions fly-half Davies became WRU chairman on Tuesday 21 October, succeeding deposed David Pickering following governing body elections.\nHe is now serving a notice period to leave his role as Newport Gwent Dragons chief executive after being voted on to the WRU board in September.\nDavies was among the leading figures among Dragons, Ospreys, Scarlets and Cardiff Blues officials who were embroiled in a protracted dispute with the WRU that ended in a £60m deal in August this year.\nIn the wake of that deal being done, Davies said the £3.3m should be spent on ensuring current Wales-based stars remain there.\nIn recent weeks, Racing Metro flanker Dan Lydiate was linked with returning to Wales.\nLikewise the Paris club\&#x27;s scrum-half Mike Phillips and centre Jamie Roberts were also touted for possible returns.\nWales coach Warren Gatland has said: &quot;We haven\&#x27;t instigated contact with the players.\n&quot;But we are aware that one or two of them are keen to return to Wales sooner rather than later.&quot;\nSpeaking to Scrum V on BBC Radio Wales, Davies re-iterated his stance, saying keeping players such as Scarlets full-back Liam Williams and Ospreys flanker Justin Tipuric in Wales should take precedence.\n&quot;It\&#x27;s obviously a limited amount of money [available]. The union are contributing 60% of that contract and the regions are putting £1.3m in.\n&quot;So it\&#x27;s a total pot of just over £3m and if you look at the sorts of salaries that the... guys... have been tempted to go overseas for [are] significant amounts of money.\n&quot;So if we were to bring the players back, we\&#x27;d probably get five or six players.\n&quot;And I\&#x27;ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\&#x27;d rather spend that money on keeping players in Wales.\n&quot;There are players coming out of contract, perhaps in the next year or so… you\&#x27;re looking at your Liam Williams\&#x27; of the world; Justin Tipuric for example - we need to keep these guys in Wales.\n&quot;We actually want them there. They are the ones who are going to impress the young kids, for example.\n&quot;They are the sort of heroes that our young kids want to emulate.\n&quot;So I would start off [by saying] with the limited pot of money, we have to retain players in Wales.\n&quot;Now, if that can be done and there\&#x27;s some spare monies available at the end, yes, let\&#x27;s look to bring players back.\n&quot;But it\&#x27;s a cruel world, isn\&#x27;t it?\n&quot;It\&#x27;s fine to take the buck and go, but great if you can get them back as well, provided there\&#x27;s enough money.&quot;\nBritish and Irish Lions centre Roberts has insisted he will see out his Racing Metro contract.\nHe and Phillips also earlier dismissed the idea of leaving Paris.\nRoberts also admitted being hurt by comments in French Newspaper L\&#x27;Equipe attributed to Racing Coach Laurent Labit questioning their effectiveness.\nCentre Roberts and flanker Lydiate joined Racing ahead of the 2013-14 season while scrum-half Phillips moved there in December 2013 after being dismissed for disciplinary reasons by former club Bayonne.&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;id&#x27;: &#x27;29750031&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;summary&#x27;: &#x27;New Welsh Rugby Union chairman Gareth Davies believes a joint £3.3m WRU-regions fund should be used to retain home-based talent such as Liam Williams, not bring back exiled stars.&#x27;&#125;</span></span><br></pre></td></tr></table></figure>
<p>下面的函数将从数据集里随机选择几个例子进行展示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, datasets.ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show_random_elements(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">1</span>:])</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>document</th>
      <th>id</th>
      <th>summary</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Media playback is unsupported on your device\n18 December 2014 Last updated at 10:28 GMT\nMalaysia has successfully tackled poverty over the last four decades by drawing on its rich natural resources.\nAccording to the World Bank, some 49% of Malaysians in 1970 were extremely poor, and that figure has been reduced to 1% today. However, the government's next challenge is to help the lower income group to move up to the middle class, the bank says.\nUlrich Zahau, the World Bank's Southeast Asia director, spoke to the BBC's Jennifer Pak.</td>
      <td>30530533</td>
      <td>In Malaysia the "aspirational" low-income part of the population is helping to drive economic growth through consumption, according to the World Bank.</td>
    </tr>
  </tbody>
</table>
<h2 id="数据预处理"><a class="markdownIt-Anchor" href="#数据预处理"></a> 数据预处理</h2>
<p>在将数据喂入模型之前，我们需要对数据进行预处理。</p>
<p>仍然是两个数据预处理的基本流程：</p>
<ol>
<li>分词；</li>
<li>转化成对应任务输入模型的格式；</li>
</ol>
<p><code>Tokenizer</code>用于上面两步数据预处理工作：<code>Tokenizer</code>首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。</p>
<h3 id="初始化tokenizer"><a class="markdownIt-Anchor" href="#初始化tokenizer"></a> 初始化Tokenizer</h3>
<p><a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer">之前的博客</a>已经介绍了一些Tokenizer的内容，并做了Tokenizer分词的示例，这里不再重复。<code>use_fast=True</code>指定使用fast版本的tokenizer。</p>
<p>我们使用<a target="_blank" rel="noopener" href="https://huggingface.co/t5-small"><code>t5-small</code></a>模型checkpoint来做该任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">model_checkpoint = <span class="string">&quot;t5-small&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="转化成对应任务输入模型的格式"><a class="markdownIt-Anchor" href="#转化成对应任务输入模型的格式"></a> 转化成对应任务输入模型的格式</h3>
<p>模型的输入为待翻译的句子。</p>
<p><strong>注意：为了给模型准备好翻译的targets，使用<code>as_target_tokenizer</code>来为targets设置tokenizer：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tokenizer.as_target_tokenizer():</span><br><span class="line">    <span class="built_in">print</span>(tokenizer([<span class="string">&quot;Hello, this one sentence!&quot;</span>, <span class="string">&quot;This is another sentence.&quot;</span>]))</span><br><span class="line"><span class="comment">#&#123;&#x27;input_ids&#x27;: [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], &#x27;attention_mask&#x27;: [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]&#125;</span></span><br></pre></td></tr></table></figure>
<p><strong>如果使用的是T5预训练模型的checkpoints（比如我们这里用的t5-small），需要对特殊的前缀进行检查。T5使用特殊的前缀（<code>&quot;summarize: &quot;</code>）来告诉模型具体要做的任务</strong>，具体前缀例子如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> model_checkpoint <span class="keyword">in</span> [<span class="string">&quot;t5-small&quot;</span>, <span class="string">&quot;t5-base&quot;</span>, <span class="string">&quot;t5-larg&quot;</span>, <span class="string">&quot;t5-3b&quot;</span>, <span class="string">&quot;t5-11b&quot;</span>]:</span><br><span class="line">    prefix = <span class="string">&quot;summarize: &quot;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    prefix = <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>现在我们可以把上面的内容放在一起组成预处理函数<code>preprocess_function</code>。对样本进行预处理的时候，<strong>使用<code>truncation=True</code>参数来确保超长文本被截断。默认情况下，对与比较短的句子会自动padding。</strong><code>max_input_length</code>控制了输入文本的长度，<code>max_target_length</code>控制了摘要的最长长度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_input_length = <span class="number">1024</span></span><br><span class="line">max_target_length = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    inputs = [prefix + doc <span class="keyword">for</span> doc <span class="keyword">in</span> examples[<span class="string">&quot;document&quot;</span>]]</span><br><span class="line">    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Setup the tokenizer for targets</span></span><br><span class="line">    <span class="keyword">with</span> tokenizer.as_target_tokenizer():</span><br><span class="line">        labels = tokenizer(examples[<span class="string">&quot;summary&quot;</span>], max_length=max_target_length, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    model_inputs[<span class="string">&quot;labels&quot;</span>] = labels[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> model_inputs</span><br></pre></td></tr></table></figure>
<p>以上的预处理函数可以处理一个样本，也可以处理多个样本exapmles。如果是处理多个样本，则返回的是多个样本被预处理之后的结果list。</p>
<p>接下来<strong>使用map函数</strong>对数据集**datasets里面三个样本集合的所有样本进行预处理，**将预处理函数<code>preprocess_function</code>应用到（map)所有样本上。参数<code>batched=True</code>可以批量对文本进行编码。这是为了充分利用前面加载fast_tokenizer的优势，它将使用多线程并发地处理批中的文本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="微调预训练模型"><a class="markdownIt-Anchor" href="#微调预训练模型"></a> 微调预训练模型</h2>
<p>数据已经准备好了，我们需要下载并加载预训练模型，然后微调预训练模型。</p>
<h3 id="加载预训练模型"><a class="markdownIt-Anchor" href="#加载预训练模型"></a> 加载预训练模型</h3>
<p>做<strong>seq2seq任务，那么需要一个能解决这个任务的模型类。我们使用<code>AutoModelForSeq2SeqLM</code> 这个类</strong>。</p>
<p>和之前几篇博客提到的加载方式相同不再赘述。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM, </span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
<h3 id="设定训练参数"><a class="markdownIt-Anchor" href="#设定训练参数"></a> 设定训练参数</h3>
<p>为了能够得到一个**<code>Seq2SeqTrainer</code>训练工具**，我们还需要<strong>训练的设定/参数 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments"><code>Seq2SeqTrainingArguments</code></a>。这个训练设定包含了能够定义训练过程的所有属性</strong>。</p>
<p>由于数据集比较大，<code>Seq2SeqTrainer</code>训练时会同时不断保存模型，我们用<code>save_total_limit=3</code>参数控制至多保存3个模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainingArguments</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">args = Seq2SeqTrainingArguments(</span><br><span class="line">    <span class="string">&quot;test-summarization&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    save_total_limit=<span class="number">3</span>,<span class="comment">#至多保存3个模型</span></span><br><span class="line">    num_train_epochs=<span class="number">1</span>,</span><br><span class="line">    predict_with_generate=<span class="literal">True</span>,</span><br><span class="line">    fp16=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="数据收集器data-collator"><a class="markdownIt-Anchor" href="#数据收集器data-collator"></a> 数据收集器data collator</h3>
<p>接下来需要告诉<code>Trainer</code>如何从预处理的输入数据中构造batch。我们使用数据收集器<code>DataCollatorForSeq2Seq</code>，将经预处理的输入分batch再次处理后喂给模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForSeq2Seq</span><br><span class="line">data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)</span><br></pre></td></tr></table></figure>
<h3 id="定义评估方法"><a class="markdownIt-Anchor" href="#定义评估方法"></a> 定义评估方法</h3>
<p>我们使用<code>'rouge'</code>指标，利用<code>metric.compute</code>计算该指标对模型进行评估。</p>
<p>使用<code>metric.compute</code>对比predictions和labels，从而计算得分。predictions和labels都需要是一个list。具体格式见下面的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fake_preds = [<span class="string">&quot;hello there&quot;</span>, <span class="string">&quot;general kenobi&quot;</span>]</span><br><span class="line">fake_labels = [<span class="string">&quot;hello there&quot;</span>, <span class="string">&quot;general kenobi&quot;</span>]</span><br><span class="line">metric.compute(predictions=fake_preds, references=fake_labels)</span><br><span class="line"><span class="comment">#&#123;&#x27;rouge1&#x27;: AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),</span></span><br><span class="line"><span class="comment"># &#x27;rouge2&#x27;: AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),</span></span><br><span class="line"><span class="comment"># &#x27;rougeL&#x27;: AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),</span></span><br><span class="line"><span class="comment"># &#x27;rougeLsum&#x27;: AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))&#125;</span></span><br></pre></td></tr></table></figure>
<p>将模型预测送入评估之前，还需要写<code>postprocess_text</code>函数做一些数据后处理。</p>
<p><a target="_blank" rel="noopener" href="http://www.nltk.org/">nltk</a>是一个自然语言处理的python工具包，我们这里用到了其中一个按句子分割的函数<code>nltk.sent_tokenize()</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># Replace -100 in the labels as we can&#x27;t decode them.</span></span><br><span class="line">    labels = np.where(labels != -<span class="number">100</span>, labels, tokenizer.pad_token_id)</span><br><span class="line">    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Rouge expects a newline after each sentence</span></span><br><span class="line">    decoded_preds = [<span class="string">&quot;\n&quot;</span>.join(nltk.sent_tokenize(pred.strip())) <span class="keyword">for</span> pred <span class="keyword">in</span> decoded_preds] <span class="comment">#按句子分割后换行符拼接</span></span><br><span class="line">    decoded_labels = [<span class="string">&quot;\n&quot;</span>.join(nltk.sent_tokenize(label.strip())) <span class="keyword">for</span> label <span class="keyword">in</span> decoded_labels]</span><br><span class="line">    </span><br><span class="line">    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># Extract a few results</span></span><br><span class="line">    result = &#123;key: value.mid.fmeasure * <span class="number">100</span> <span class="keyword">for</span> key, value <span class="keyword">in</span> result.items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add mean generated length</span></span><br><span class="line">    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) <span class="keyword">for</span> pred <span class="keyword">in</span> predictions]</span><br><span class="line">    result[<span class="string">&quot;gen_len&quot;</span>] = np.mean(prediction_lens)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;k: <span class="built_in">round</span>(v, <span class="number">4</span>) <span class="keyword">for</span> k, v <span class="keyword">in</span> result.items()&#125;</span><br></pre></td></tr></table></figure>
<h3 id="开始训练"><a class="markdownIt-Anchor" href="#开始训练"></a> 开始训练</h3>
<p>将数据/模型/参数传入<code>Trainer</code>即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainer</span><br><span class="line">trainer = Seq2SeqTrainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>调用<code>train</code>方法开始训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.7-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90.md">4.7-生成任务-摘要生成.md</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/transformers">transformers官方文档</a></p>
<p><a href="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E6%8A%8ABERT%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1/">BERT相关——（7）将BERT应用到下游任务</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">冬于</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/">https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ifwind.github.io" target="_blank">冬于的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a></div><div class="post_share"><div class="social-share" data-image="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><img class="prev-cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">BERT实战——（7）生成任务-语言模型</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%885%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"><img class="next-cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">BERT实战——（5）生成任务-机器翻译</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/20/BERT相关——（1）语言模型/" title="BERT相关——（1）语言模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（1）语言模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（2）Contextualized_Word_Embedding和ELMO模型/" title="BERT相关——（2）Contextualized Word Embedding和ELMO模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（2）Contextualized Word Embedding和ELMO模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（3）BERT模型/" title="BERT相关——（3）BERT模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（3）BERT模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（4）GPT模型/" title="BERT相关——（4）GPT-2模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（4）GPT-2模型</div></div></a></div><div><a href="/2021/08/22/BERT相关——（5）Pre-train Model/" title="BERT相关——（5）Pre-train Model"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-22</div><div class="title">BERT相关——（5）Pre-train Model</div></div></a></div><div><a href="/2021/08/24/BERT相关——（7）把BERT应用到下游任务/" title="BERT相关——（7）将BERT应用到下游任务"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-24</div><div class="title">BERT相关——（7）将BERT应用到下游任务</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bert%E5%AE%9E%E6%88%986%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90"><span class="toc-number">1.</span> <span class="toc-text"> BERT实战——（6）生成任务-摘要生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text"> 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.1.</span> <span class="toc-text"> 任务介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87"><span class="toc-number">1.1.2.</span> <span class="toc-text"> 前期准备</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.2.</span> <span class="toc-text"> 数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 数据集介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 加载数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text"> 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96tokenizer"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 初始化Tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E5%8C%96%E6%88%90%E5%AF%B9%E5%BA%94%E4%BB%BB%E5%8A%A1%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.3.2.</span> <span class="toc-text"> 转化成对应任务输入模型的格式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text"> 微调预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text"> 加载预训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text"> 设定训练参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%99%A8data-collator"><span class="toc-number">1.4.3.</span> <span class="toc-text"> 数据收集器data collator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.4.</span> <span class="toc-text"> 定义评估方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.4.5.</span> <span class="toc-text"> 开始训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.5.</span> <span class="toc-text"> 参考文献</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 冬于</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
      appKey: 'YKdl4HKX9W6jLSdPlypgEtDM',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: 'https://e4lpmfet.lc-cn-n1-shared.com',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.17.0/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://e4lpmfet.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
        "X-LC-Key": 'YKdl4HKX9W6jLSdPlypgEtDM',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>