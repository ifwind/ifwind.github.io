<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>BERT实战——（7）生成任务-语言模型 | 冬于的博客</title><meta name="keywords" content="深度学习,BERT,NLP"><meta name="author" content="冬于"><meta name="copyright" content="冬于"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="BERT实战——（7）生成任务-语言模型 引言 之前的分别介绍了使用 🤗 Transformers代码库中的模型开展one-class任务(文本分类、多选问答问题)、class for each token任务(序列标注)、copy from input任务(抽取式问答)以及general sequence任务（机器翻译、摘要抽取）。 这一篇将介绍如何使用语言模型任务微调 🤗 Trans">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT实战——（7）生成任务-语言模型">
<meta property="og:url" content="https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="冬于的博客">
<meta property="og:description" content="BERT实战——（7）生成任务-语言模型 引言 之前的分别介绍了使用 🤗 Transformers代码库中的模型开展one-class任务(文本分类、多选问答问题)、class for each token任务(序列标注)、copy from input任务(抽取式问答)以及general sequence任务（机器翻译、摘要抽取）。 这一篇将介绍如何使用语言模型任务微调 🤗 Trans">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg">
<meta property="article:published_time" content="2021-08-31T05:59:23.000Z">
<meta property="article:modified_time" content="2021-09-04T12:47:02.698Z">
<meta property="article:author" content="冬于">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="BERT">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LVK1P2D38K","apiKey":"8cbbb0bcbb5c7448f68b4fae01d4ccd5","indexName":"DongYu","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BERT实战——（7）生成任务-语言模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-09-04 20:47:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/iconfont.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">冬于的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BERT实战——（7）生成任务-语言模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-31T05:59:23.000Z" title="发表于 2021-08-31 13:59:23">2021-08-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-04T12:47:02.698Z" title="更新于 2021-09-04 20:47:02">2021-09-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3,760</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="BERT实战——（7）生成任务-语言模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="bert实战7生成任务-语言模型">BERT实战——（7）生成任务-语言模型</h1>
<h2 id="引言">引言</h2>
<p>之前的分别介绍了使用 <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">🤗 Transformers</a>代码库中的模型开展one-class任务(<a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a>、<a href="https://ifwind.github.io/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%883%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E5%A4%9A%E9%80%89%E9%97%AE%E7%AD%94/">多选问答问题</a>)、class for each token任务(<a href="https://ifwind.github.io/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%882%EF%BC%89%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/">序列标注</a>)、copy from input任务(<a href="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/">抽取式问答</a>)以及general sequence任务（机器翻译、摘要抽取）。</p>
<p>这一篇将介绍<strong>如何使用语言模型任务微调 <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">🤗 Transformers</a>模型</strong>（关于什么是语言模型，回看<a href="https://ifwind.github.io/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">之前的博客-语言模型</a>）。</p>
<h3 id="任务介绍">任务介绍</h3>
<p>我们这里主要完成两类语言建模任务：</p>
<ul>
<li><strong>因果语言模型（Causal language modeling，CLM）</strong>：<strong>模型需要预测句子中的下一位置处的字符</strong>（<strong>类似BERT类模型的decoder和GPT</strong>，从左往右输入字符）。模型会使用矩阵对角线attention mask机制防止模型提前看到答案。例如，当模型试图预测句子中的<span class="math inline">\(i+1\)</span>位置处的字符时，这个掩码将阻止它访问<span class="math inline">\(i\)</span>位置之后的字符。</li>
</ul>
<blockquote>
<p><img src="/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20210901190347576.png" style="zoom:80%;"></p>
</blockquote>
<ul>
<li><strong>掩码语言建模（Masked language modeling，MLM）</strong>：模型需要恢复输入中被&quot;MASK&quot;掉的一些字符（BERT类模型的预训练任务，只用transformer的encoder部分）。模型可以看到整个句子，因此模型可以根据“[MASK]”标记之前和之后的字符来预测该位置被“[MASK]”之前的字符。</li>
</ul>
<blockquote>
<p><img src="/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/masked_language_modeling.png" style="zoom:80%;"></p>
</blockquote>
<p>主要分为以下几个部分：</p>
<ol type="1">
<li>数据加载；</li>
<li>数据预处理；</li>
<li>微调预训练模型：使用transformer中的<strong><code>Seq2SeqTrainer</code>接口</strong>对预训练模型进行微调（注意这里是<code>Seq2SeqTrainer</code>接口，之前的任务都是调用<code>Trainer</code>接口）。</li>
</ol>
<h3 id="前期准备">前期准备</h3>
<p>安装以下库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install datasets transformers sacrebleu sentencepiece</span><br><span class="line"><span class="comment">#transformers==4.9.2</span></span><br><span class="line"><span class="comment">#datasets==1.11.0</span></span><br></pre></td></tr></table></figure>
<h2 id="数据加载">数据加载</h2>
<h3 id="数据集介绍">数据集介绍</h3>
<p>我们使用<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wikitext#data-instances">Wikitext 2</a>数据集，其中包括了从Wikipedia上经过验证的Good和Featured文章集中提取的超过1亿个token的集合。</p>
<h3 id="加载数据">加载数据</h3>
<p>该数据的加载方式在transformers库中进行了封装，我们可以通过以下语句进行数据加载：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">datasets = load_dataset(<span class="string">&#x27;wikitext&#x27;</span>, <span class="string">&#x27;wikitext-2-raw-v1&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果碰到以下错误： <img src="/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/request_error.png" alt="request Error"></p>
<p>解决方案:</p>
<p>MAC用户: 在 <code>/etc/hosts</code> 文件中添加一行 <code>199.232.68.133  raw.githubusercontent.com</code></p>
<p>Windows用户: 在 <code>C:\Windows\System32\drivers\etc\hosts</code> 文件中添加一行 <code>199.232.68.133  raw.githubusercontent.com</code></p>
</blockquote>
<p>如果想加载自己的数据集可以参考<a href="https://ifwind.github.io/2021/08/26/BERT实战——（1）文本分类/#加载自己的数据或来自网络的数据">之前的博客-定位词：加载自己的数据或来自网络的数据</a>。</p>
<p>数据加载完毕后，给定一个数据切分的key（train、validation或者test）和下标即可查看数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets[<span class="string">&quot;train&quot;</span>][<span class="number">10</span>]</span><br><span class="line"><span class="comment">#&#123;&#x27;text&#x27;: &#x27; The game \&#x27;s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \&#x27; turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific &quot; Potentials &quot; , skills unique to each character . They are divided into &quot; Personal Potential &quot; , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and &quot; Battle Potentials &quot; , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique &quot; Masters Table &quot; , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate &quot; Direct Command &quot; and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her &quot; Valkyria Form &quot; and become invincible , while Imca can target multiple enemy units with her heavy weapon . \n&#x27;&#125;</span></span><br></pre></td></tr></table></figure>
<p>下面的函数将从数据集里随机选择几个例子进行展示，可以看到一些文本是维基百科文章的完整段落，而其他的只是标题或空行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> ClassLabel</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">4</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show_random_elements(raw_datasets[<span class="string">&quot;train&quot;</span>])</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
text
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
MD 194D is the designation for an unnamed 0 @.@ 02 @-@ mile ( 0 @.@ 032 km ) connector between MD 194 and MD 853E , the old alignment that parallels the northbound direction of the modern highway south of Angell Road . 
</td>
</tr>
<tr>
<th>
1
</th>
<td>
My sense , as though of hemlock I had drunk , 
</td>
</tr>
<tr>
<th>
2
</th>
<td>
</td>
</tr>
<tr>
<th>
3
</th>
<td>
A mimed stage show , Thunderbirds : F.A.B. , has toured internationally and popularised a staccato style of movement known colloquially as the &quot; Thunderbirds walk &quot; . The production has periodically been revived as Thunderbirds : F.A.B. – The Next Generation . 
</td>
</tr>
</tbody>
</table>
<h2 id="因果语言模型causal-language-modelingclm">因果语言模型（Causal Language Modeling，CLM）</h2>
<h3 id="数据预处理">数据预处理</h3>
<p>在将数据喂入模型之前，我们需要对数据进行预处理。</p>
<p>仍然是两个数据预处理的基本流程：</p>
<ol type="1">
<li>分词；</li>
<li>转化成对应任务输入模型的格式；</li>
</ol>
<p><code>Tokenizer</code>用于上面两步数据预处理工作：<code>Tokenizer</code>首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。</p>
<h4 id="初始化tokenizer">初始化Tokenizer</h4>
<p><a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer">之前的博客</a>已经介绍了一些Tokenizer的内容，并做了Tokenizer分词的示例，这里不再重复。<code>use_fast=True</code>指定使用fast版本的tokenizer。我们使用已经训练好的<a target="_blank" rel="noopener" href="https://huggingface.co/distilgpt2"><code>distilgpt2</code></a> 模型checkpoint来做该任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">model_checkpoint = <span class="string">&quot;distilgpt2&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="转化成对应任务输入模型的格式">转化成对应任务输入模型的格式</h4>
<p>对于因果语言模型(CLM)，我们首先获取到数据集中的所有文本并分词，之后将它们连接起来。最后，在特定序列长度的例子中拆分它们，将各个拆分部分作为模型输入。</p>
<p>通过这种方式，模型将接收如下的连续文本块，<code>[BOS_TOKEN]</code>用于分割拼接了来自不同内容的文本。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入类型1：文本1</span><br><span class="line">输入类型2：文本1结尾 [BOS_TOKEN] 文本2开头</span><br></pre></td></tr></table></figure>
<h5 id="调用分词器对所有的文本分词">调用分词器对所有的文本分词</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>])</span><br></pre></td></tr></table></figure>
<p><strong>使用map函数</strong>对数据集<strong>datasets里面三个样本集合的所有样本进行预处理，</strong>将函数<code>tokenize_function</code>应用到（map)所有样本上。使用<code>batch=True</code>和<code>4</code>个进程来加速预处理。这是为了充分利用前面加载fast_tokenizer的优势，它将使用多线程并发地处理批中的文本。之后我们并不需要<code>text</code>列，所以将其舍弃（<code>remove_columns</code>）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>, num_proc=<span class="number">4</span>, remove_columns=[<span class="string">&quot;text&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>我们现在查看数据集的一个元素，训练集中<code>text</code>已经被模型所需的<code>input_ids</code>所取代:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">1</span>]</span><br><span class="line"><span class="comment">#&#123;&#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment"># &#x27;input_ids&#x27;: [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="拼接文本按定长拆分文本">拼接文本&amp;按定长拆分文本</h5>
<p>我们需要将所有文本连接在一起，然后将结果分割成特定<code>block_size</code>的小块。<code>block_size</code>设置为预训练模型时所使用的最大长度。</p>
<p>编写预处理函数来对文本进行组合和拆分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># block_size = tokenizer.model_max_length</span></span><br><span class="line">block_size = <span class="number">128</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="comment"># 拼接所有文本</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">sum</span>(examples[k], []) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># 我们将余数对应的部分去掉。但如果模型支持的话，可以添加padding，您可以根据需要定制此部件。</span></span><br><span class="line">    total_length = (total_length // block_size) * block_size</span><br><span class="line">    <span class="comment"># 通过max_len进行分割。</span></span><br><span class="line">    result = &#123;</span><br><span class="line">        k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">        <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">    &#125;</span><br><span class="line">    result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy()</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p><strong>注意：因为我们做的是因果语言模型，其预测label就是其输入的input_id，所以我们复制了标签的输入。</strong></p>
<p>我们将再次使用<code>map</code>方法，<code>batched=True</code>表示允许通过返回不同数量的样本来改变数据集中的样本数量，这样可以从一批示例中创建新的示例。</p>
<p>注意，在默认情况下，<code>map</code>方法将发送一批1,000个示例，由预处理函数处理。<strong>可以通过传递不同batch_size来调整。也可以使用<code>num_proc</code>来加速预处理。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lm_datasets = tokenized_datasets.<span class="built_in">map</span>(</span><br><span class="line">    group_texts,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=<span class="number">1000</span>,</span><br><span class="line">    num_proc=<span class="number">4</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>检查一下数据集是否发生了变化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer.decode(lm_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">1</span>][<span class="string">&quot;input_ids&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>现在样本包含了<code>block_size</code>大小的连续字符块，可能跨越了几个原始文本。</p>
<p>' game and follows the &quot; Nameless &quot;, a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit &quot; Calamaty Raven &quot;. The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Oz'</p>
<h3 id="微调预训练模型">微调预训练模型</h3>
<p>数据已经准备好了，我们需要下载并加载预训练模型，然后微调预训练模型。</p>
<h4 id="加载预训练模型">加载预训练模型</h4>
<p>做<strong>因果语言模型任务，那么需要一个能解决这个任务的模型类。我们使用<code>AutoModelForCausalLM</code> 这个类</strong>。</p>
<p>和之前几篇博客提到的加载方式相同不再赘述。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
<h4 id="设定训练参数">设定训练参数</h4>
<p>为了能够得到一个<strong><code>Trainer</code>训练工具</strong>，我们还需要<strong>训练的设定/参数 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments"><code>TrainingArguments</code></a>。这个训练设定包含了能够定义训练过程的所有属性</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;test-clm&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="定义评估方法">定义评估方法</h4>
<p>完成该任务不需要特别定义评估指标处理函数，模型将直接计算困惑度perplexity作为评估指标。</p>
<h4 id="开始训练">开始训练</h4>
<p>将数据/模型/参数传入<code>Trainer</code>即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=lm_datasets[<span class="string">&quot;train&quot;</span>][:<span class="number">1000</span>],</span><br><span class="line">    eval_dataset=lm_datasets[<span class="string">&quot;validation&quot;</span>][:<span class="number">1000</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>调用<code>train</code>方法开始训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<h3 id="评估模型">评估模型</h3>
<p>一旦训练完成，我们就可以评估模型，得到它在验证集上的perplexity，如下所示:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">eval_results = trainer.evaluate()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Perplexity: <span class="subst">&#123;math.exp(eval_results[<span class="string">&#x27;eval_loss&#x27;</span>]):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="掩码语言模型mask-language-modelingmlm">掩码语言模型（Mask Language Modeling，MLM）</h2>
<p>掩码语言模型相比因果语言模型好训练得多，因为只需要对mask的token(比如只占总数的15%)进行预测，同时可以访问其余的token。对于模型来说，这是一项更容易的任务。</p>
<h3 id="数据预处理-1">数据预处理</h3>
<p>和前面的步骤相同：</p>
<ol type="1">
<li>分词；</li>
<li>转化成对应任务输入模型的格式；</li>
</ol>
<h4 id="初始化tokenizer-1">初始化Tokenizer</h4>
<p>我们使用已经训练好的<a target="_blank" rel="noopener" href="https://huggingface.co/distilroberta-base"><code>distilroberta-base</code></a>模型checkpoint来做该任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">model_checkpoint = <span class="string">&quot;distilroberta-base&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="转化成对应任务输入模型的格式-1">转化成对应任务输入模型的格式</h4>
<p>对于<strong>掩码语言模型(MLM)</strong>，我们首先获取到数据集中的所有文本并分词，之后将它们连接起来，接着在特定序列长度的例子中拆分它们。与因果语言模型不同的是，我们<strong>在拆分后还需要随机&quot;MASK&quot;一些字符(使用&quot;[MASK]&quot;进行替换)以及调整标签为只包含在&quot;[MASK]&quot;位置处的标签(因为我们不需要预测没有被&quot;MASK&quot;的字符)</strong>，最后将各个经掩码的拆分部分作为模型输入。</p>
<h5 id="调用分词器对所有的文本分词-1">调用分词器对所有的文本分词</h5>
<p>应用一个和前面相同的分词器函数，只需要更新分词器来使用刚刚选择的checkpoint：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>同样<strong>使用map函数</strong>对数据集<strong>datasets里面三个样本集合的所有样本进行预处理，</strong>将函数<code>tokenize_function</code>应用到（map)所有样本上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>, num_proc=<span class="number">4</span>, remove_columns=[<span class="string">&quot;text&quot;</span>])</span><br></pre></td></tr></table></figure>
<h5 id="拼接文本按定长拆分文本-1">拼接文本&amp;按定长拆分文本</h5>
<p>我们需要将所有文本连接在一起，然后将结果分割成特定<code>block_size</code>的小块。<code>block_size</code>设置为预训练模型时所使用的最大长度。</p>
<p>编写预处理函数来对文本进行组合和拆分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># block_size = tokenizer.model_max_length</span></span><br><span class="line">block_size = <span class="number">128</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_texts</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="comment"># 拼接所有文本</span></span><br><span class="line">    concatenated_examples = &#123;k: <span class="built_in">sum</span>(examples[k], []) <span class="keyword">for</span> k <span class="keyword">in</span> examples.keys()&#125;</span><br><span class="line">    total_length = <span class="built_in">len</span>(concatenated_examples[<span class="built_in">list</span>(examples.keys())[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># 我们将余数对应的部分去掉。但如果模型支持的话，可以添加padding，您可以根据需要定制此部件。</span></span><br><span class="line">    total_length = (total_length // block_size) * block_size</span><br><span class="line">    <span class="comment"># 通过max_len进行分割。</span></span><br><span class="line">    result = &#123;</span><br><span class="line">        k: [t[i : i + block_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, total_length, block_size)]</span><br><span class="line">        <span class="keyword">for</span> k, t <span class="keyword">in</span> concatenated_examples.items()</span><br><span class="line">    &#125;</span><br><span class="line">    result[<span class="string">&quot;labels&quot;</span>] = result[<span class="string">&quot;input_ids&quot;</span>].copy()</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p><strong>注意：我们这里仍然复制了标签的输入作为label，因为掩码语言模型的本质还是预测原文，而掩码在data collator中通过添加特别的参数进行处理，下文会着重说明</strong>。</p>
<p>我们将再次使用<code>map</code>方法，<code>batched=True</code>表示允许通过返回不同数量的样本来改变数据集中的样本数量，这样可以从一批示例中创建新的示例。</p>
<p>注意，在默认情况下，<code>map</code>方法将发送一批1,000个示例，由预处理函数处理。<strong>可以通过传递不同batch_size来调整。也可以使用<code>num_proc</code>来加速预处理。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lm_datasets = tokenized_datasets.<span class="built_in">map</span>(</span><br><span class="line">    group_texts,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=<span class="number">1000</span>,</span><br><span class="line">    num_proc=<span class="number">4</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="微调预训练模型-1">微调预训练模型</h3>
<p>数据已经准备好了，我们需要下载并加载预训练模型，然后微调预训练模型。</p>
<h4 id="加载预训练模型-1">加载预训练模型</h4>
<p>做<strong>掩码语言模型任务，那么需要一个能解决这个任务的模型类。我们使用<code>AutoModelForMaskedLM</code> 这个类</strong>。</p>
<p>和之前几篇博客提到的加载方式相同不再赘述。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForMaskedLM</span><br><span class="line">model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
<h4 id="设定训练参数-1">设定训练参数</h4>
<p>为了能够得到一个<strong><code>Trainer</code>训练工具</strong>，我们还需要<strong>训练的设定/参数 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments"><code>TrainingArguments</code></a>。这个训练设定包含了能够定义训练过程的所有属性</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;test-clm&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="数据收集器data-collator">数据收集器data collator</h3>
<p>data_collator是一个函数，负责获取样本并将它们批处理成张量。data_collator负责获取样本并将它们批处理成张量。掩码语言模型任务需要使用一个特殊的data_collator，用于随机&quot;MASK&quot;句子中的token。</p>
<p>注意：我们可以将MASK作为预处理步骤(<code>tokenizer</code>)进行处理，但<code>tokenizer</code>在每个阶段字符总是以相同的方式被掩盖。而通过在<code>data_collator</code>中执行这一步，可以确保<strong>每次生成数据时都以新的方式完成掩码（随机）。</strong></p>
<p>为了实现随机mask，<strong><code>Transformers</code>为掩码语言模型提供了一个特殊的<code>DataCollatorForLanguageModeling</code>。</strong>可以通过<code>mlm_probability</code>调整掩码的概率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForLanguageModeling</span><br><span class="line">data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="number">0.15</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义评估方法-1">定义评估方法</h4>
<p>完成该任务不需要特别定义评估指标处理函数，模型将直接计算困惑度perplexity作为评估指标。</p>
<h4 id="开始训练-1">开始训练</h4>
<p>将数据/模型/参数传入<code>Trainer</code>即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=lm_datasets[<span class="string">&quot;train&quot;</span>][:<span class="number">1000</span>],</span><br><span class="line">    eval_dataset=lm_datasets[<span class="string">&quot;validation&quot;</span>][:<span class="number">100</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>调用<code>train</code>方法开始训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<h3 id="评估模型-1">评估模型</h3>
<p>训练完成后就可以评估模型，得到它在验证集上的perplexity，如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">eval_results = trainer.evaluate()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Perplexity: <span class="subst">&#123;math.exp(eval_results[<span class="string">&#x27;eval_loss&#x27;</span>]):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/篇章4-使用Transformers解决NLP任务/4.5-生成任务-语言模型.md">4.5-生成任务-语言模型.md</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/transformers">transformers官方文档</a></p>
<p><a href="https://ifwind.github.io/2021/08/26/BERT实战——（1）文本分类/#加载自己的数据或来自网络的数据">BERT实战——（1）文本分类 | 冬于的博客 (ifwind.github.io)</a></p>
<p><a href="https://ifwind.github.io/2021/08/20/BERT相关——（1）语言模型/">BERT相关——（1）语言模型</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">冬于</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ifwind.github.io" target="_blank">冬于的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/"><img class="prev-cover" src="/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%81%E9%9D%A2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">李宏毅深度学习2021春p5-9：神经网络训练技巧</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/"><img class="next-cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">BERT实战——（6）生成任务-摘要生成</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/20/BERT相关——（2）Contextualized_Word_Embedding和ELMO模型/" title="BERT相关——（2）Contextualized Word Embedding和ELMO模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（2）Contextualized Word Embedding和ELMO模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（3）BERT模型/" title="BERT相关——（3）BERT模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（3）BERT模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（1）语言模型/" title="BERT相关——（1）语言模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（1）语言模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（4）GPT模型/" title="BERT相关——（4）GPT-2模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（4）GPT-2模型</div></div></a></div><div><a href="/2021/08/22/BERT相关——（5）Pre-train Model/" title="BERT相关——（5）Pre-train Model"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-22</div><div class="title">BERT相关——（5）Pre-train Model</div></div></a></div><div><a href="/2021/08/24/BERT相关——（7）把BERT应用到下游任务/" title="BERT相关——（7）将BERT应用到下游任务"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-24</div><div class="title">BERT相关——（7）将BERT应用到下游任务</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bert%E5%AE%9E%E6%88%987%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">BERT实战——（7）生成任务-语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.1.</span> <span class="toc-text">任务介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87"><span class="toc-number">1.1.2.</span> <span class="toc-text">前期准备</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.2.</span> <span class="toc-text">数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.1.</span> <span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">加载数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%A0%E6%9E%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8Bcausal-language-modelingclm"><span class="toc-number">1.3.</span> <span class="toc-text">因果语言模型（Causal Language Modeling，CLM）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.1.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96tokenizer"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">初始化Tokenizer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AC%E5%8C%96%E6%88%90%E5%AF%B9%E5%BA%94%E4%BB%BB%E5%8A%A1%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">转化成对应任务输入模型的格式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B0%83%E7%94%A8%E5%88%86%E8%AF%8D%E5%99%A8%E5%AF%B9%E6%89%80%E6%9C%89%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E8%AF%8D"><span class="toc-number">1.3.1.2.1.</span> <span class="toc-text">调用分词器对所有的文本分词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5%E6%96%87%E6%9C%AC%E6%8C%89%E5%AE%9A%E9%95%BF%E6%8B%86%E5%88%86%E6%96%87%E6%9C%AC"><span class="toc-number">1.3.1.2.2.</span> <span class="toc-text">拼接文本&amp;按定长拆分文本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.</span> <span class="toc-text">微调预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">加载预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">设定训练参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">定义评估方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">开始训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.3.</span> <span class="toc-text">评估模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8Bmask-language-modelingmlm"><span class="toc-number">1.4.</span> <span class="toc-text">掩码语言模型（Mask Language Modeling，MLM）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="toc-number">1.4.1.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96tokenizer-1"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">初始化Tokenizer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AC%E5%8C%96%E6%88%90%E5%AF%B9%E5%BA%94%E4%BB%BB%E5%8A%A1%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%BC%E5%BC%8F-1"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">转化成对应任务输入模型的格式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B0%83%E7%94%A8%E5%88%86%E8%AF%8D%E5%99%A8%E5%AF%B9%E6%89%80%E6%9C%89%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E8%AF%8D-1"><span class="toc-number">1.4.1.2.1.</span> <span class="toc-text">调用分词器对所有的文本分词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5%E6%96%87%E6%9C%AC%E6%8C%89%E5%AE%9A%E9%95%BF%E6%8B%86%E5%88%86%E6%96%87%E6%9C%AC-1"><span class="toc-number">1.4.1.2.2.</span> <span class="toc-text">拼接文本&amp;按定长拆分文本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">1.4.2.</span> <span class="toc-text">微调预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">加载预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0-1"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">设定训练参数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%99%A8data-collator"><span class="toc-number">1.4.3.</span> <span class="toc-text">数据收集器data collator</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95-1"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">定义评估方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83-1"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">开始训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">1.4.4.</span> <span class="toc-text">评估模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.5.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 冬于</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
      appKey: 'YKdl4HKX9W6jLSdPlypgEtDM',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: 'https://e4lpmfet.lc-cn-n1-shared.com',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.17.0/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://e4lpmfet.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
        "X-LC-Key": 'YKdl4HKX9W6jLSdPlypgEtDM',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>