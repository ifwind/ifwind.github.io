<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>BERT相关——（6）BERT代码分析 | 冬于的博客</title><meta name="keywords" content="深度学习,BERT,NLP"><meta name="author" content="冬于"><meta name="copyright" content="冬于"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="BERT相关——（6）BERT代码分析 引言 上一篇介绍了如何利用HuggingFace的transformers从头开始预训练BERT模型，所使用的AutoModelForMaskedLM函数可以实例化为transformers library中现有的masked language model中的模型类之一。这一篇将分析transformers中实现BERT模型相关的源码，以便我们可以设计自">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT相关——（6）BERT代码分析">
<meta property="og:url" content="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="冬于的博客">
<meta property="og:description" content="BERT相关——（6）BERT代码分析 引言 上一篇介绍了如何利用HuggingFace的transformers从头开始预训练BERT模型，所使用的AutoModelForMaskedLM函数可以实例化为transformers library中现有的masked language model中的模型类之一。这一篇将分析transformers中实现BERT模型相关的源码，以便我们可以设计自">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ifwind.github.io/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png">
<meta property="article:published_time" content="2021-08-24T07:42:06.000Z">
<meta property="article:modified_time" content="2021-09-04T12:31:12.997Z">
<meta property="article:author" content="冬于">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="BERT">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ifwind.github.io/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LVK1P2D38K","apiKey":"8cbbb0bcbb5c7448f68b4fae01d4ccd5","indexName":"DongYu","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BERT相关——（6）BERT代码分析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-09-04 20:31:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/iconfont.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">冬于的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BERT相关——（6）BERT代码分析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-24T07:42:06.000Z" title="发表于 2021-08-24 15:42:06">2021-08-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-04T12:31:12.997Z" title="更新于 2021-09-04 20:31:12">2021-09-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8,096</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>41分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="BERT相关——（6）BERT代码分析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="bert相关6bert代码分析">BERT相关——（6）BERT代码分析</h1>
<h2 id="引言">引言</h2>
<p>上一篇介绍了如何利用HuggingFace的transformers从头开始预训练BERT模型，所使用的<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/auto.html?highlight=automodelformaskedlm#transformers.AutoModelForMaskedLM"><code>AutoModelForMaskedLM</code>函数</a>可以实例化为transformers library中现有的masked language model中的模型类之一。这一篇将分析transformers中实现BERT模型相关的源码，以便我们可以设计自己的模型。</p>
<p>回顾一下从头开始预训练BERT模型的几个步骤，分别对应了各个模块的源码：</p>
<ol type="1">
<li><p>利用Tokenizer对语料分词——BertTokenizer；</p></li>
<li><p>重新配置模型——BertModel（其中又包括了各个子模块，如下图所示）；</p>
<blockquote>
<p><img src="/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/BertModel结构.png"></p>
</blockquote></li>
<li><p>编写满足训练任务的处理代码：每个句子进行掩膜并组成句子对的正负样本集合以完成BERT训练的两个任务——数据预处理相关的类；</p></li>
<li><p>数据输入模型进行训练——数据加载入模型相关类如：Dataset、DataLoader、DataCollator。</p></li>
</ol>
<p><strong>这一篇主要分析前两步的代码。</strong></p>
<h2 id="tokenizer对语料分词berttokenizer">Tokenizer对语料分词——BertTokenizer</h2>
<p>BertTokenizer代码核心部分在中<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py">models/bert/tokenization_bert.py</a>。</p>
<p>我们先来理清一下需要BertTokenizer干什么事情，再来看源码是怎么实现的，考虑了什么。</p>
<h3 id="需求分析">需求分析</h3>
<p>BertTokenizer的目的是对<strong>语料分词</strong>，据此衍生出以下几个问题，也对应了该类需要满足的几个功能：</p>
<ol type="1">
<li>输入语料库（多个句子组成）或一个句子：加载全部训练数据、对训练数据进行处理、对单个句子进行处理（包括输入、输出）；</li>
<li>要能对句子进行拆分：分词功能；
<ul>
<li>拆分尺度到哪一步呢？中英文的拆分策略差异？：字（char）、subword（介于 char 和 word之间，比如英文的词根 ）和词（word）等；</li>
<li>subword怎么拼接回word？：subword 拼接回word；</li>
<li>拆分结果要生成索引或者是id才能方便输入模型：根据训练集构建字典、根据id查词、根据词查id；</li>
<li>训练集中未出现的词怎么处理？：未出现词的映射（根据词查id时）、[UNK]字符</li>
</ul></li>
<li>一些特殊字符&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;；</li>
<li>模型保存、加载：BertTokenizer模型的保存、加载；</li>
<li>与BERT模型输入相关的：添加[CLS]等特殊字符、构建NSP任务的训练集、特殊字符的掩码。</li>
</ol>
<h3 id="源码">源码</h3>
<p>现在让我们来看一下具体的源码实现中是怎么实现上述需求的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Tokenization classes for Bert.&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ...tokenization_utils <span class="keyword">import</span> PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace</span><br><span class="line"><span class="keyword">from</span> ...utils <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logger = logging.get_logger(__name__)</span><br><span class="line"></span><br><span class="line">VOCAB_FILES_NAMES = &#123;<span class="string">&quot;vocab_file&quot;</span>: <span class="string">&quot;vocab.txt&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#预训练模型的字典</span></span><br><span class="line">PRETRAINED_VOCAB_FILES_MAP = &#123;</span><br><span class="line">    <span class="string">&quot;vocab_file&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;bert-base-uncased&quot;</span>: <span class="string">&quot;https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt&quot;</span>,</span><br><span class="line">       <span class="comment">#……</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#预训练模型的位置编码大小</span></span><br><span class="line">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = &#123;</span><br><span class="line">    <span class="string">&quot;bert-base-uncased&quot;</span>: <span class="number">512</span>,</span><br><span class="line">     <span class="comment">#……</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#预训练模型的config</span></span><br><span class="line">PRETRAINED_INIT_CONFIGURATION = &#123;</span><br><span class="line">    <span class="string">&quot;bert-base-uncased&quot;</span>: &#123;<span class="string">&quot;do_lower_case&quot;</span>: <span class="literal">True</span>&#125;,</span><br><span class="line">	 <span class="comment">#……</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载字典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_vocab</span>(<span class="params">vocab_file</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Loads a vocabulary file into a dictionary.&quot;&quot;&quot;</span></span><br><span class="line">    vocab = collections.OrderedDict()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(vocab_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">        tokens = reader.readlines()</span><br><span class="line">    <span class="keyword">for</span> index, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">        token = token.rstrip(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        vocab[token] = index</span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line"><span class="comment">#空格分割</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">whitespace_tokenize</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Runs basic whitespace cleaning and splitting on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">    text = text.strip()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> text:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    tokens = text.split()</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertTokenizer</span>(<span class="params">PreTrainedTokenizer</span>):</span></span><br><span class="line">    vocab_files_names = VOCAB_FILES_NAMES</span><br><span class="line">    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP</span><br><span class="line">    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION</span><br><span class="line">    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        vocab_file,</span></span></span><br><span class="line"><span class="params"><span class="function">        do_lower_case=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        do_basic_tokenize=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        never_split=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        unk_token=<span class="string">&quot;[UNK]&quot;</span>, <span class="comment">#特殊字符</span></span></span></span><br><span class="line"><span class="params"><span class="function">        sep_token=<span class="string">&quot;[SEP]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        pad_token=<span class="string">&quot;[PAD]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        cls_token=<span class="string">&quot;[CLS]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        mask_token=<span class="string">&quot;[MASK]&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenize_chinese_chars=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        strip_accents=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        **kwargs</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(</span><br><span class="line">            do_lower_case=do_lower_case,</span><br><span class="line">            do_basic_tokenize=do_basic_tokenize,</span><br><span class="line">            never_split=never_split,</span><br><span class="line">            unk_token=unk_token,</span><br><span class="line">            sep_token=sep_token,</span><br><span class="line">            pad_token=pad_token,</span><br><span class="line">            cls_token=cls_token,</span><br><span class="line">            mask_token=mask_token,</span><br><span class="line">            tokenize_chinese_chars=tokenize_chinese_chars,</span><br><span class="line">            strip_accents=strip_accents,</span><br><span class="line">            **kwargs,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(vocab_file):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Can&#x27;t find a vocabulary file at path &#x27;<span class="subst">&#123;vocab_file&#125;</span>&#x27;. To load the vocabulary from a Google pretrained &quot;</span></span><br><span class="line">                <span class="string">&quot;model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`&quot;</span></span><br><span class="line">            )</span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        self.ids_to_tokens = collections.OrderedDict([(ids, tok) <span class="keyword">for</span> tok, ids <span class="keyword">in</span> self.vocab.items()])</span><br><span class="line">        self.do_basic_tokenize = do_basic_tokenize</span><br><span class="line">        <span class="keyword">if</span> do_basic_tokenize:</span><br><span class="line">            self.basic_tokenizer = BasicTokenizer(</span><br><span class="line">                do_lower_case=do_lower_case,</span><br><span class="line">                never_split=never_split,</span><br><span class="line">                tokenize_chinese_chars=tokenize_chinese_chars,</span><br><span class="line">                strip_accents=strip_accents,</span><br><span class="line">            )</span><br><span class="line">        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)<span class="comment">#在分词的基础上将词划分为子词subword</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_lower_case</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.basic_tokenizer.do_lower_case</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vocab_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(self.vocab, **self.added_tokens_encoder)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">if</span> self.do_basic_tokenize:</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):</span><br><span class="line"></span><br><span class="line">                <span class="comment"># If the token is part of the never_split set</span></span><br><span class="line">                <span class="keyword">if</span> token <span class="keyword">in</span> self.basic_tokenizer.never_split:</span><br><span class="line">                    split_tokens.append(token)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    split_tokens += self.wordpiece_tokenizer.tokenize(token)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            split_tokens = self.wordpiece_tokenizer.tokenize(text)</span><br><span class="line">        <span class="keyword">return</span> split_tokens</span><br><span class="line">	<span class="comment">#根据token查id</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_token_to_id</span>(<span class="params">self, token</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Converts a token (str) in an id using the vocab.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.vocab.get(token, self.vocab.get(self.unk_token))</span><br><span class="line">	<span class="comment">#根据id查token</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_convert_id_to_token</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Converts an index (integer) in a token (str) using the vocab.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.ids_to_tokens.get(index, self.unk_token)</span><br><span class="line">	<span class="comment">#把subword拼接回word</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_string</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Converts a sequence of tokens (string) in a single string.&quot;&quot;&quot;</span></span><br><span class="line">        out_string = <span class="string">&quot; &quot;</span>.join(tokens).replace(<span class="string">&quot; ##&quot;</span>, <span class="string">&quot;&quot;</span>).strip()</span><br><span class="line">        <span class="keyword">return</span> out_string</span><br><span class="line">	<span class="comment">#添加[CLS]等特殊字符、构建NSP任务的训练集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_inputs_with_special_tokens</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, token_ids_0: <span class="type">List</span>[<span class="built_in">int</span>], token_ids_1: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span></span><br><span class="line"><span class="string">        adding special tokens. A BERT sequence has the following format:</span></span><br><span class="line"><span class="string">        - single sequence: ``[CLS] X [SEP]``</span></span><br><span class="line"><span class="string">        - pair of sequences: ``[CLS] A [SEP] B [SEP]``</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token_ids_0 (:obj:`List[int]`):</span></span><br><span class="line"><span class="string">                List of IDs to which the special tokens will be added.</span></span><br><span class="line"><span class="string">            token_ids_1 (:obj:`List[int]`, `optional`):</span></span><br><span class="line"><span class="string">                Optional second list of IDs for sequence pairs.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            :obj:`List[int]`: List of `input IDs &lt;../glossary.html#input-ids&gt;`__ with the appropriate special tokens.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [self.cls_token_id] + token_ids_0 + [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        <span class="keyword">return</span> cls + token_ids_0 + sep + token_ids_1 + sep</span><br><span class="line">	<span class="comment">#特殊字符的掩码，包括单个句子的开头[CLS]和结尾[SEP]（[CLS] X [SEP]），和句子对的特殊字符（[CLS] A [SEP] B [SEP]）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_special_tokens_mask</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, token_ids_0: <span class="type">List</span>[<span class="built_in">int</span>], token_ids_1: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span>, already_has_special_tokens: <span class="built_in">bool</span> = <span class="literal">False</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding</span></span><br><span class="line"><span class="string">        special tokens using the tokenizer ``prepare_for_model`` method.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token_ids_0 (:obj:`List[int]`):</span></span><br><span class="line"><span class="string">                List of IDs.</span></span><br><span class="line"><span class="string">            token_ids_1 (:obj:`List[int]`, `optional`):</span></span><br><span class="line"><span class="string">                Optional second list of IDs for sequence pairs.</span></span><br><span class="line"><span class="string">            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span></span><br><span class="line"><span class="string">                Whether or not the token list is already formatted with special tokens for the model.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> already_has_special_tokens:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">super</span>().get_special_tokens_mask(</span><br><span class="line">                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * <span class="built_in">len</span>(token_ids_0)) + [<span class="number">1</span>] + ([<span class="number">0</span>] * <span class="built_in">len</span>(token_ids_1)) + [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">1</span>] + ([<span class="number">0</span>] * <span class="built_in">len</span>(token_ids_0)) + [<span class="number">1</span>]</span><br><span class="line">	<span class="comment">#句子对的掩码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_token_type_ids_from_sequences</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, token_ids_0: <span class="type">List</span>[<span class="built_in">int</span>], token_ids_1: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence</span></span><br><span class="line"><span class="string">        pair mask has the following format:</span></span><br><span class="line"><span class="string">        ::</span></span><br><span class="line"><span class="string">            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="string">            | first sequence    | second sequence |</span></span><br><span class="line"><span class="string">        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            token_ids_0 (:obj:`List[int]`):</span></span><br><span class="line"><span class="string">                List of IDs.</span></span><br><span class="line"><span class="string">            token_ids_1 (:obj:`List[int]`, `optional`):</span></span><br><span class="line"><span class="string">                Optional second list of IDs for sequence pairs.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            :obj:`List[int]`: List of `token type IDs &lt;../glossary.html#token-type-ids&gt;`_ according to the given</span></span><br><span class="line"><span class="string">            sequence(s).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sep = [self.sep_token_id]</span><br><span class="line">        cls = [self.cls_token_id]</span><br><span class="line">        <span class="keyword">if</span> token_ids_1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(cls + token_ids_0 + sep) * [<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(cls + token_ids_0 + sep) * [<span class="number">0</span>] + <span class="built_in">len</span>(token_ids_1 + sep) * [<span class="number">1</span>]</span><br><span class="line">	<span class="comment">#保存字典，token</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_vocabulary</span>(<span class="params">self, save_directory: <span class="built_in">str</span>, filename_prefix: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">str</span>]:</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> os.path.isdir(save_directory):</span><br><span class="line">            vocab_file = os.path.join(</span><br><span class="line">                save_directory, (filename_prefix + <span class="string">&quot;-&quot;</span> <span class="keyword">if</span> filename_prefix <span class="keyword">else</span> <span class="string">&quot;&quot;</span>) + VOCAB_FILES_NAMES[<span class="string">&quot;vocab_file&quot;</span>]</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            vocab_file = (filename_prefix + <span class="string">&quot;-&quot;</span> <span class="keyword">if</span> filename_prefix <span class="keyword">else</span> <span class="string">&quot;&quot;</span>) + save_directory</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(vocab_file, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> writer:</span><br><span class="line">            <span class="keyword">for</span> token, token_index <span class="keyword">in</span> <span class="built_in">sorted</span>(self.vocab.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>]):</span><br><span class="line">                <span class="keyword">if</span> index != token_index:</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">f&quot;Saving vocabulary to <span class="subst">&#123;vocab_file&#125;</span>: vocabulary indices are not consecutive.&quot;</span></span><br><span class="line">                        <span class="string">&quot; Please check that the vocabulary is not corrupted!&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                    index = token_index</span><br><span class="line">                writer.write(token + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> (vocab_file,)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, do_lower_case=<span class="literal">True</span>, never_split=<span class="literal">None</span>, tokenize_chinese_chars=<span class="literal">True</span>, strip_accents=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            never_split = []</span><br><span class="line">        self.do_lower_case = do_lower_case</span><br><span class="line">        self.never_split = <span class="built_in">set</span>(never_split)</span><br><span class="line">        self.tokenize_chinese_chars = tokenize_chinese_chars</span><br><span class="line">        self.strip_accents = strip_accents</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text, never_split=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Basic Tokenization of a piece of text. Split on &quot;white spaces&quot; only, for sub-word tokenization, see</span></span><br><span class="line"><span class="string">        WordPieceTokenizer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            **never_split**: (`optional`) list of str</span></span><br><span class="line"><span class="string">                Kept for backward compatibility purposes. Now implemented directly at the base class level (see</span></span><br><span class="line"><span class="string">                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># union() returns a new set by concatenating the two sets.</span></span><br><span class="line">        never_split = self.never_split.union(<span class="built_in">set</span>(never_split)) <span class="keyword">if</span> never_split <span class="keyword">else</span> self.never_split</span><br><span class="line">        text = self._clean_text(text)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span></span><br><span class="line">        <span class="comment"># models. This is also applied to the English models now, but it doesn&#x27;t</span></span><br><span class="line">        <span class="comment"># matter since the English models were not trained on any Chinese data</span></span><br><span class="line">        <span class="comment"># and generally don&#x27;t have any Chinese data in them (there are Chinese</span></span><br><span class="line">        <span class="comment"># characters in the vocabulary because Wikipedia does have some Chinese</span></span><br><span class="line">        <span class="comment"># words in the English Wikipedia.).</span></span><br><span class="line">        <span class="keyword">if</span> self.tokenize_chinese_chars:</span><br><span class="line">            text = self._tokenize_chinese_chars(text) <span class="comment">#处理中文</span></span><br><span class="line">        orig_tokens = whitespace_tokenize(text)<span class="comment">#按空格分隔返回tokens</span></span><br><span class="line">        split_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> never_split:</span><br><span class="line">                <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">                    token = token.lower()</span><br><span class="line">                    <span class="keyword">if</span> self.strip_accents <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">False</span>:</span><br><span class="line">                        token = self._run_strip_accents(token)</span><br><span class="line">                <span class="keyword">elif</span> self.strip_accents:</span><br><span class="line">                    token = self._run_strip_accents(token)</span><br><span class="line">            split_tokens.extend(self._run_split_on_punc(token, never_split))</span><br><span class="line"></span><br><span class="line">        output_tokens = whitespace_tokenize(<span class="string">&quot; &quot;</span>.join(split_tokens))</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line">	<span class="comment">#从文本中去除重音符号</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">        text = unicodedata.normalize(<span class="string">&quot;NFD&quot;</span>, text)</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cat = unicodedata.category(char)</span><br><span class="line">            <span class="keyword">if</span> cat == <span class="string">&quot;Mn&quot;</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line">	<span class="comment">#分割标点符号</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span>(<span class="params">self, text, never_split=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> never_split <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> text <span class="keyword">in</span> never_split:</span><br><span class="line">            <span class="keyword">return</span> [text]</span><br><span class="line">        chars = <span class="built_in">list</span>(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="literal">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="literal">False</span></span><br><span class="line">                output[-<span class="number">1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;&quot;</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line">	<span class="comment">#中文句子中每个中文词间都加上空格</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize_chinese_chars</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = <span class="built_in">ord</span>(char)</span><br><span class="line">            <span class="keyword">if</span> self._is_chinese_char(cp):</span><br><span class="line">                output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">                output.append(char)</span><br><span class="line">                output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line">	<span class="comment">#根据编码判断词是否为中文词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span>(<span class="params">self, cp</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># This defines a &quot;chinese character&quot; as anything in the CJK Unicode block:</span></span><br><span class="line">        <span class="comment">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span></span><br><span class="line">        <span class="comment"># despite its name. The modern Korean Hangul alphabet is a different block,</span></span><br><span class="line">        <span class="comment"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span></span><br><span class="line">        <span class="comment"># space-separated words, so they are not treated specially and handled</span></span><br><span class="line">        <span class="comment"># like the all of the other languages.</span></span><br><span class="line">        <span class="keyword">if</span> (</span><br><span class="line">            (cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>)  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>)</span><br><span class="line">            <span class="keyword">or</span> (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)  <span class="comment">#</span></span><br><span class="line">        ):  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">	<span class="comment">#去除一些无效字符</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_clean_text</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cp = <span class="built_in">ord</span>(char)</span><br><span class="line">            <span class="keyword">if</span> cp == <span class="number">0</span> <span class="keyword">or</span> cp == <span class="number">0xFFFD</span> <span class="keyword">or</span> _is_control(char):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> _is_whitespace(char):</span><br><span class="line">                output.append(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(output)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在分词的基础上将词划分为子词subword</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Runs WordPiece tokenization.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab, unk_token, max_input_chars_per_word=<span class="number">100</span></span>):</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.unk_token = unk_token</span><br><span class="line">        self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform</span></span><br><span class="line"><span class="string">        tokenization using the given vocabulary.</span></span><br><span class="line"><span class="string">        For example, :obj:`input = &quot;unaffable&quot;` wil return as output :obj:`[&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]`.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">            already been passed through `BasicTokenizer`.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        output_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">            chars = <span class="built_in">list</span>(token)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">                output_tokens.append(self.unk_token) <span class="comment">#处理没有出现过的词</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            is_bad = <span class="literal">False</span></span><br><span class="line">            start = <span class="number">0</span></span><br><span class="line">            sub_tokens = []</span><br><span class="line">            <span class="keyword">while</span> start &lt; <span class="built_in">len</span>(chars):</span><br><span class="line">                end = <span class="built_in">len</span>(chars)</span><br><span class="line">                cur_substr = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">while</span> start &lt; end:</span><br><span class="line">                    substr = <span class="string">&quot;&quot;</span>.join(chars[start:end])</span><br><span class="line">                    <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">                        substr = <span class="string">&quot;##&quot;</span> + substr</span><br><span class="line">                    <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab: <span class="comment">#找到字符串中存在vocab中的substr</span></span><br><span class="line">                        cur_substr = substr</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    end -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    is_bad = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sub_tokens.append(cur_substr)</span><br><span class="line">                start = end</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> is_bad:</span><br><span class="line">                output_tokens.append(self.unk_token) <span class="comment">#处理没有出现过的词</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output_tokens.extend(sub_tokens)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<h3 id="源码分析">源码分析</h3>
<p><code>BertTokenizer</code> 是基于<code>BasicTokenizer</code>和<code>WordPieceTokenizer</code>的分词器：</p>
<ul>
<li><strong>BasicTokenizer按标点、空格等分割句子，并处理是否统一小写，以及清理非法字符。</strong>其中：
<ul>
<li>对于中文字符，通过预处理（加空格）来按字分割；</li>
<li>同时可以通过never_split指定对某些词不进行分割；</li>
<li>这一步是可选的（默认执行）<code>do_basic_tokenize=True</code>。</li>
</ul></li>
<li><strong>WordPieceTokenizer在词的基础上，进一步将词分解为子词（subword）。</strong>
<ul>
<li>找到字符串中存在vocab中的subword将word划分为subword；</li>
<li><strong>subword 介于 char 和 word 之间</strong>，既在一定程度保留了词的含义，又能够照顾到英文中单复数、时态导致的词表爆炸和未登录词的 OOV（Out-Of-Vocabulary）问题，将词根与时态词缀等分割出来，从而减小词表，也降低了训练难度；</li>
<li>例如，<strong>tokenizer 这个词就可以拆解为“token”和“##izer”两部分，注意后面一个词的“##”表示接在前一个词后面。</strong></li>
</ul></li>
</ul>
<p><strong>BertTokenizer 有以下常用方法：</strong></p>
<ul>
<li>from_pretrained：从包含词表文件（vocab.txt）的目录中初始化一个分词器；</li>
<li>tokenize：将文本（词或者句子）分解为子词列表；</li>
<li>convert_tokens_to_ids：将子词列表转化为子词对应下标的列表；</li>
<li>convert_ids_to_tokens ：与上一个相反；</li>
<li>convert_tokens_to_string：将 subword 列表按“##”拼接回词或者句子；</li>
<li>encode：对于单个句子输入，分解词并加入特殊词形成“[CLS], x, [SEP]”的结构并转换为词表对应下标的列表；对于两个句子输入（多个句子只取前两个），分解词并加入特殊词形成“[CLS], x1, [SEP], x2, [SEP]”的结构并转换为下标列表；</li>
<li>decode：可以将 encode 方法的输出变为完整句子。</li>
</ul>
<h2 id="bertmodel">BertModel</h2>
<p>和 BERT 模型有关的代码主要写在<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py"><code>/models/bert/modeling_bert.py</code></a>中，包含 BERT 模型的基本结构和基于它的微调模型等。这篇中我们主要先看Bert模型的本体-BertModel。</p>
<h3 id="需求分析-1">需求分析</h3>
<p>让我们结合下面这张图来分析一下BERT模型的功能需求。</p>
<p><img src="/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/BERT.png" style="zoom:80%;"></p>
<p>总的来说需要以下几个部分：</p>
<ol type="1">
<li>输入：
<ul>
<li>根据输入的id获取各个词的嵌入：token embedding；</li>
<li>用于NSP任务的segment embedding；</li>
<li>生成位置嵌入position embedding；</li>
</ul></li>
<li>encoder：
<ul>
<li>多头注意力机制模块；</li>
<li>残差模块+layer norm；</li>
<li>全连接层；</li>
</ul></li>
<li>输出：
<ul>
<li>池化层，用于训练任务。</li>
</ul></li>
<li>整体的模型、模型保存、加载。</li>
</ol>
<p>因为每个部分代码都很长，所以分块分析源码。</p>
<p>先从整体模型来看。</p>
<h3 id="bertmodel-1">BertModel</h3>
<h4 id="源码-1">源码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@add_start_docstrings(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    <span class="string">&quot;The bare Bert Model transformer outputting raw hidden-states without any specific head on top.&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    BERT_START_DOCSTRING,</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span>(<span class="params">BertPreTrainedModel</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of</span></span><br><span class="line"><span class="string">    cross-attention is added between the self-attention layers, following the architecture described in `Attention is</span></span><br><span class="line"><span class="string">    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,</span></span><br><span class="line"><span class="string">    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</span></span><br><span class="line"><span class="string">    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration</span></span><br><span class="line"><span class="string">    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`</span></span><br><span class="line"><span class="string">    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an</span></span><br><span class="line"><span class="string">    input to the forward pass.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config, add_pooling_layer=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line">        self.config = config</span><br><span class="line">		<span class="comment"># 输入的嵌入</span></span><br><span class="line">        self.embeddings = BertEmbeddings(config)</span><br><span class="line">        <span class="comment"># encoder</span></span><br><span class="line">        self.encoder = BertEncoder(config)</span><br><span class="line">		<span class="comment"># 输出，可选add_pooling_layer</span></span><br><span class="line">        self.pooler = BertPooler(config) <span class="keyword">if</span> add_pooling_layer <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_input_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embeddings.word_embeddings</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_input_embeddings</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        self.embeddings.word_embeddings = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prune_heads</span>(<span class="params">self, heads_to_prune</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Prunes heads of the model. heads_to_prune: dict of &#123;layer_num: list of heads to prune in this layer&#125; See base</span></span><br><span class="line"><span class="string">        class PreTrainedModel</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer, heads <span class="keyword">in</span> heads_to_prune.items():</span><br><span class="line">            self.encoder.layer[layer].attention.prune_heads(heads)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @add_start_docstrings_to_model_forward(<span class="params">BERT_INPUTS_DOCSTRING.<span class="built_in">format</span>(<span class="params"><span class="string">&quot;batch_size, sequence_length&quot;</span></span>)</span>)</span></span><br><span class="line"><span class="meta">    @add_code_sample_docstrings(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">        tokenizer_class=_TOKENIZER_FOR_DOC,</span></span></span><br><span class="line"><span class="params"><span class="meta">        checkpoint=_CHECKPOINT_FOR_DOC,</span></span></span><br><span class="line"><span class="params"><span class="meta">        output_type=BaseModelOutputWithPoolingAndCrossAttentions,</span></span></span><br><span class="line"><span class="params"><span class="meta">        config_class=_CONFIG_FOR_DOC,</span></span></span><br><span class="line"><span class="params"><span class="meta">    </span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        input_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        position_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        inputs_embeds=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_values=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        use_cache=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        return_dict=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):</span></span><br><span class="line"><span class="string">            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if</span></span><br><span class="line"><span class="string">            the model is configured as a decoder.</span></span><br><span class="line"><span class="string">        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span></span><br><span class="line"><span class="string">            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in</span></span><br><span class="line"><span class="string">            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:</span></span><br><span class="line"><span class="string">            - 1 for tokens that are **not masked**,</span></span><br><span class="line"><span class="string">            - 0 for tokens that are **masked**.</span></span><br><span class="line"><span class="string">        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):</span></span><br><span class="line"><span class="string">            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</span></span><br><span class="line"><span class="string">            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`</span></span><br><span class="line"><span class="string">            (those that don&#x27;t have their past key value states given to this model) of shape :obj:`(batch_size, 1)`</span></span><br><span class="line"><span class="string">            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.</span></span><br><span class="line"><span class="string">        use_cache (:obj:`bool`, `optional`):</span></span><br><span class="line"><span class="string">            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up</span></span><br><span class="line"><span class="string">            decoding (see :obj:`past_key_values`).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output_attentions = output_attentions <span class="keyword">if</span> output_attentions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_attentions</span><br><span class="line">        output_hidden_states = (</span><br><span class="line">            output_hidden_states <span class="keyword">if</span> output_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.output_hidden_states</span><br><span class="line">        )</span><br><span class="line">        return_dict = return_dict <span class="keyword">if</span> return_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_return_dict</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.config.is_decoder:</span><br><span class="line">            use_cache = use_cache <span class="keyword">if</span> use_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.config.use_cache</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_cache = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">            batch_size, seq_length = input_shape</span><br><span class="line">        <span class="keyword">elif</span> inputs_embeds <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:-<span class="number">1</span>]</span><br><span class="line">            batch_size, seq_length = input_shape</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;You have to specify either input_ids or inputs_embeds&quot;</span>)</span><br><span class="line"></span><br><span class="line">        device = input_ids.device <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> inputs_embeds.device</span><br><span class="line"></span><br><span class="line">        <span class="comment"># past_key_values_length</span></span><br><span class="line">        past_key_values_length = past_key_values[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">2</span>] <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(self.embeddings, <span class="string">&quot;token_type_ids&quot;</span>):</span><br><span class="line">                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]</span><br><span class="line">                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)</span><br><span class="line">                token_type_ids = buffered_token_type_ids_expanded</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span></span><br><span class="line">        <span class="comment"># ourselves in which case we just need to make it broadcastable to all heads.</span></span><br><span class="line">        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If a 2D or 3D attention mask is provided for the cross-attention</span></span><br><span class="line">        <span class="comment"># we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]</span></span><br><span class="line">        <span class="keyword">if</span> self.config.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()</span><br><span class="line">            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)</span><br><span class="line">            <span class="keyword">if</span> encoder_attention_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)</span><br><span class="line">            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_extended_attention_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Prepare head mask if needed</span></span><br><span class="line">        <span class="comment"># 1.0 in head_mask indicate we keep the head</span></span><br><span class="line">        <span class="comment"># attention_probs has shape bsz x n_heads x N x N</span></span><br><span class="line">        <span class="comment"># input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span></span><br><span class="line">        <span class="comment"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]</span></span><br><span class="line">        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)</span><br><span class="line">		<span class="comment">#生成嵌入</span></span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            position_ids=position_ids,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            inputs_embeds=inputs_embeds,</span><br><span class="line">            past_key_values_length=past_key_values_length,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#经过encoder</span></span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            attention_mask=extended_attention_mask, <span class="comment">#注意encoder中的attention_mask是extended_attention_mask</span></span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">            past_key_values=past_key_values,</span><br><span class="line">            use_cache=use_cache,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=output_hidden_states,</span><br><span class="line">            return_dict=return_dict,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 取last_hidden_state（最后一层输出）</span></span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 取第一个符号[CLS]作为整个句子的表示（self-attention下，句子中每个词都包括了整个句子的信息）</span></span><br><span class="line">        pooled_output = self.pooler(sequence_output) <span class="keyword">if</span> self.pooler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> (sequence_output, pooled_output) + encoder_outputs[<span class="number">1</span>:]<span class="comment">#encoder_outputs[1:]表示encoder其他的输出，如past_key_values、hidden_states等</span></span><br><span class="line">		</span><br><span class="line">        <span class="comment">#包装输出，类似字典包括输出对应的键/值</span></span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPoolingAndCrossAttentions(</span><br><span class="line">            last_hidden_state=sequence_output,</span><br><span class="line">            pooler_output=pooled_output,</span><br><span class="line">            past_key_values=encoder_outputs.past_key_values,</span><br><span class="line">            hidden_states=encoder_outputs.hidden_states,</span><br><span class="line">            attentions=encoder_outputs.attentions,</span><br><span class="line">            cross_attentions=encoder_outputs.cross_attentions,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h4 id="源码分析-1">源码分析</h4>
<p>在初始化部分可以发现源码将BertModel划分成下图红框的三个模块（BertEmbedding、BertEncoder、BertPooler）：</p>
<blockquote>
<p><img src="/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/BertModel结构1.png"></p>
</blockquote>
<p>BertModel前向传播中利用到了以下参数：</p>
<blockquote>
<ul>
<li>input_ids：经过 tokenizer 分词后的 subword 对应的下标列表；</li>
<li>attention_mask：在 self-attention 过程中，这一块 mask 用于标记 subword 所处句子和 padding 的区别，将 padding 部分填充为 0；</li>
<li>token_type_ids：标记 subword 当前所处句子（第一句/第二句/ padding）；</li>
<li>position_ids：标记当前词所在句子的位置下标；</li>
<li>head_mask：用于将某些层的某些注意力计算无效化；</li>
<li>inputs_embeds：如果提供了，那就不需要input_ids，跨过 embedding lookup 过程直接作为 Embedding 进入 Encoder 计算；</li>
<li>encoder_hidden_states：这一部分在 BertModel 配置为 decoder 时起作用，将执行 cross-attention 而不是 self-attention；</li>
<li>encoder_attention_mask：同上，在 cross-attention 中用于标记 encoder 端输入的 padding；</li>
<li><strong>past_key_values：这个参数貌似是把预先计算好的 K-V 乘积传入，以降低 cross-attention 的开销（因为原本这部分是重复计算）；</strong></li>
<li>use_cache：将保存上一个参数并传回，加速 decoding；</li>
<li>output_attentions：是否返回中间每层的 attention 输出；</li>
<li>output_hidden_states：是否返回中间每层的输出；</li>
<li>return_dict：是否按键值对的形式（ModelOutput 类，也可以当作 tuple 用）返回输出，默认为真。</li>
</ul>
</blockquote>
<p><strong>注意，这里的 <code>head_mask</code> 对部分<code>attention_score</code>无效化，和下文提到的注意力头剪枝（<code>_prune_heads</code>）不同，仅仅把<code>attention_score</code>给乘以这一矩阵。</strong></p>
<p>此外，BertModel 还有以下的方法：</p>
<ul>
<li>get_input_embeddings：提取 embedding 中的 word_embeddings 即词向量部分；</li>
<li>set_input_embeddings：为 embedding 中的 word_embeddings 赋值；</li>
<li>_prune_heads：提供了将<strong>注意力头剪枝</strong>的函数，输入为<code>&#123;layer_num: list of heads to prune in this layer&#125;</code>的字典，可以将指定层的某些注意力头剪枝，直接<strong>对权重矩阵剪枝</strong>。</li>
</ul>
<p><strong>剪枝是一个复杂的操作，需要将保留的注意力头部分的 Wq、Kq、Vq 和拼接后全连接部分的权重拷贝到一个新的较小的权重矩阵（注意先禁止 grad 再拷贝），并实时记录被剪掉的头以防下标出错。具体参考BertAttention部分的prune_heads方法，下文会标注出来。</strong></p>
<p>此外，还需要注意，<strong>encoder中的attention_mask是extended_attention_mask而不是BertModel前向传播输入的attention_mask</strong>。</p>
<p>接下来我们分块分析上面划分的三个子模块。</p>
<h3 id="输入-bertembeddings">输入-BertEmbeddings</h3>
<p>输入的嵌入包含三个部分求和得到：</p>
<blockquote>
<p><img src="/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/bert-embedding.png" style="zoom:80%;"></p>
</blockquote>
<h4 id="源码-2">源码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        <span class="comment"># 层正则化</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.position_embedding_type = <span class="built_in">getattr</span>(config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span>) <span class="comment"># 位置编码</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="string">&quot;1.6.0&quot;</span>):</span><br><span class="line">            self.register_buffer(</span><br><span class="line">                <span class="string">&quot;token_type_ids&quot;</span>,</span><br><span class="line">                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),</span><br><span class="line">                persistent=<span class="literal">False</span>,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span>, past_key_values_length=<span class="number">0</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs</span></span><br><span class="line">        <span class="comment"># when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves</span></span><br><span class="line">        <span class="comment"># issue #5664</span></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;token_type_ids&quot;</span>):</span><br><span class="line">                buffered_token_type_ids = self.token_type_ids[:, :seq_length]</span><br><span class="line">                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[<span class="number">0</span>], seq_length)</span><br><span class="line">                token_type_ids = buffered_token_type_ids_expanded</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + token_type_embeddings</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;absolute&quot;</span>:</span><br><span class="line">            position_embeddings = self.position_embeddings(position_ids) <span class="comment">#使用绝对位置编码[1,2,3,4....]</span></span><br><span class="line">            embeddings += position_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="源码分析-2">源码分析</h4>
<ol type="1">
<li>word_embeddings，上文中 <strong>subword</strong> 对应的嵌入。</li>
<li>token_type_embeddings，用于表示当前词所在的句子，辅助区别句子与 padding、句子对间的差异</li>
<li>position_embeddings，句子中每个词的位置嵌入，用于区别词的顺序。和 transformer 论文中的设计不同，这一块是训练出来的，而不是通过 Sinusoidal 函数计算得到的固定嵌入。一般认为这种实现不利于拓展性（难以直接迁移到更长的句子中）。</li>
</ol>
<p>三个 embedding <strong>不带权重相加</strong>，并<strong>通过一层 LayerNorm+dropout 后输出</strong>，其大小为(batch_size, sequence_length, hidden_size)。【为什么选择LayerNorm可参考：<a href="https://ifwind.github.io/2021/08/17/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89Normalization%E6%96%B9%E5%BC%8F/">Transformer相关——normalization机制</a>】</p>
<h3 id="bertencoder">BertEncoder</h3>
<p>为了方便理解，先把BertEncoder的层次结构图放上来，进一步拆分逐个分析。</p>
<p><img src="/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/BertEncoder结构.png"></p>
<h4 id="bertencoder-1">BertEncoder</h4>
<h5 id="源码-3">源码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        <span class="comment">#多个BertLayer构成</span></span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_values=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        use_cache=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_hidden_states=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        return_dict=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        all_hidden_states = () <span class="keyword">if</span> output_hidden_states <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        all_self_attentions = () <span class="keyword">if</span> output_attentions <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        all_cross_attentions = () <span class="keyword">if</span> output_attentions <span class="keyword">and</span> self.config.add_cross_attention <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        next_decoder_cache = () <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment">#embedding逐层经过BertLayer</span></span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layer):</span><br><span class="line">            <span class="keyword">if</span> output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line">			</span><br><span class="line">            layer_head_mask = head_mask[i] <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            past_key_value = past_key_values[i] <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">getattr</span>(self.config, <span class="string">&quot;gradient_checkpointing&quot;</span>, <span class="literal">False</span>) <span class="keyword">and</span> self.training:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> use_cache:</span><br><span class="line">                    logger.warning(</span><br><span class="line">                        <span class="string">&quot;`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting &quot;</span></span><br><span class="line">                        <span class="string">&quot;`use_cache=False`...&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                    use_cache = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">                <span class="function"><span class="keyword">def</span> <span class="title">create_custom_forward</span>(<span class="params">module</span>):</span></span><br><span class="line">                    <span class="function"><span class="keyword">def</span> <span class="title">custom_forward</span>(<span class="params">*inputs</span>):</span></span><br><span class="line">                        <span class="keyword">return</span> module(*inputs, past_key_value, output_attentions)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> custom_forward</span><br><span class="line">				<span class="comment">#利用 gradient checkpointing 技术以降低训练时的显存占用。梯度检查点，通过减少保存的计算图节点压缩模型占用空间，但是在计算梯度的时候需要重新计算没有存储的值，参考论文《Training Deep Nets with Sublinear Memory Cost》。</span></span><br><span class="line">                layer_outputs = torch.utils.checkpoint.checkpoint(</span><br><span class="line">                    create_custom_forward(layer_module),</span><br><span class="line">                    hidden_states,</span><br><span class="line">                    attention_mask,</span><br><span class="line">                    layer_head_mask,<span class="comment">#取了第i层的head_mask</span></span><br><span class="line">                    encoder_hidden_states,</span><br><span class="line">                    encoder_attention_mask,</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layer_outputs = layer_module(</span><br><span class="line">                    hidden_states,</span><br><span class="line">                    attention_mask,</span><br><span class="line">                    layer_head_mask,</span><br><span class="line">                    encoder_hidden_states,</span><br><span class="line">                    encoder_attention_mask,</span><br><span class="line">                    past_key_value,</span><br><span class="line">                    output_attentions,</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> use_cache:</span><br><span class="line">                next_decoder_cache += (layer_outputs[-<span class="number">1</span>],)</span><br><span class="line">            <span class="keyword">if</span> output_attentions:</span><br><span class="line">                all_self_attentions = all_self_attentions + (layer_outputs[<span class="number">1</span>],)</span><br><span class="line">                <span class="keyword">if</span> self.config.add_cross_attention:</span><br><span class="line">                    all_cross_attentions = all_cross_attentions + (layer_outputs[<span class="number">2</span>],)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">tuple</span>(</span><br><span class="line">                v</span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> [</span><br><span class="line">                    hidden_states,</span><br><span class="line">                    next_decoder_cache,</span><br><span class="line">                    all_hidden_states,</span><br><span class="line">                    all_self_attentions,</span><br><span class="line">                    all_cross_attentions,</span><br><span class="line">                ]</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPastAndCrossAttentions(</span><br><span class="line">            last_hidden_state=hidden_states,</span><br><span class="line">            past_key_values=next_decoder_cache,</span><br><span class="line">            hidden_states=all_hidden_states,</span><br><span class="line">            attentions=all_self_attentions,</span><br><span class="line">            cross_attentions=all_cross_attentions,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h5 id="源码分析-3">源码分析</h5>
<p>BertEncoder由多个BertLayer组成。</p>
<p>特别地，BertEncoder在前向传播时<strong>利用了 gradient checkpointing 技术以降低训练时的显存占用。</strong></p>
<blockquote>
<p><strong>gradient checkpointing 即梯度检查点，通过减少保存的计算图节点压缩模型占用空间，但是在计算梯度的时候需要重新计算没有存储的值，参考论文《Training Deep Nets with Sublinear Memory Cost》，过程如下示意图</strong> <img src="/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/3-1-gradient-checkpointing.gif" style="zoom:80%;"> 图：</p>
<p>在 BertEncoder 中，gradient checkpoint 是通过 <code>torch.utils.checkpoint.checkpoint</code> 实现的，可以参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43002433/article/details/105322846">网络训练高效内存管理——torch.utils.checkpoint的使用</a></p>
</blockquote>
<h4 id="bertlayer">BertLayer</h4>
<h5 id="源码-4">源码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.chunk_size_feed_forward = config.chunk_size_feed_forward</span><br><span class="line">        self.seq_len_dim = <span class="number">1</span></span><br><span class="line">        self.attention = BertAttention(config)  <span class="comment">#attention层</span></span><br><span class="line">        self.is_decoder = config.is_decoder		<span class="comment">#是否配置为decoder而不是encoder</span></span><br><span class="line">        self.add_cross_attention = config.add_cross_attention</span><br><span class="line">        <span class="keyword">if</span> self.add_cross_attention:</span><br><span class="line">            <span class="keyword">assert</span> self.is_decoder, <span class="string">f&quot;<span class="subst">&#123;self&#125;</span> should be used as a decoder model if cross attention is added&quot;</span></span><br><span class="line">            self.crossattention = BertAttention(config)</span><br><span class="line">        self.intermediate = BertIntermediate(config) <span class="comment">#全连接+激活</span></span><br><span class="line">        self.output = BertOutput(config) <span class="comment">#输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="comment"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span></span><br><span class="line">        self_attn_past_key_value = past_key_value[:<span class="number">2</span>] <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self_attention_outputs = self.attention(</span><br><span class="line">            hidden_states,</span><br><span class="line">            attention_mask,</span><br><span class="line">            head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            past_key_value=self_attn_past_key_value,</span><br><span class="line">        )</span><br><span class="line">        attention_output = self_attention_outputs[<span class="number">0</span>] <span class="comment">#获得context_layer 即 attention 矩阵与 value 矩阵的乘积</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if decoder, the last output is tuple of self-attn cache</span></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            outputs = self_attention_outputs[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            present_key_value = self_attention_outputs[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = self_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add self attentions if we output attention weights，attention_probs</span></span><br><span class="line"></span><br><span class="line">        cross_attn_present_key_value = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">hasattr</span>(</span><br><span class="line">                self, <span class="string">&quot;crossattention&quot;</span></span><br><span class="line">            ), <span class="string">f&quot;If `encoder_hidden_states` are passed, <span class="subst">&#123;self&#125;</span> has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple</span></span><br><span class="line">            cross_attn_past_key_value = past_key_value[-<span class="number">2</span>:] <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            cross_attention_outputs = self.crossattention(</span><br><span class="line">                attention_output,</span><br><span class="line">                attention_mask,</span><br><span class="line">                head_mask,</span><br><span class="line">                encoder_hidden_states,</span><br><span class="line">                encoder_attention_mask,</span><br><span class="line">                cross_attn_past_key_value,</span><br><span class="line">                output_attentions,</span><br><span class="line">            )</span><br><span class="line">            attention_output = cross_attention_outputs[<span class="number">0</span>]</span><br><span class="line">            outputs = outputs + cross_attention_outputs[<span class="number">1</span>:-<span class="number">1</span>]  <span class="comment"># add cross attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># add cross-attn cache to positions 3,4 of present_key_value tuple</span></span><br><span class="line">            cross_attn_present_key_value = cross_attention_outputs[-<span class="number">1</span>]</span><br><span class="line">            present_key_value = present_key_value + cross_attn_present_key_value</span><br><span class="line">		<span class="comment">#apply_chunking_to_forward函数将 input_tensors 分块成更小的输入张量部分然后执行forward_fn ，输入参数列表为：forward_fn,chunk_size,chunk_dim,input_tensors</span></span><br><span class="line">        <span class="comment">#在这里执行了self.feed_forward_chunk函数，全连接+激活+输出层</span></span><br><span class="line">        layer_output = apply_chunking_to_forward(</span><br><span class="line">            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output</span><br><span class="line">        )</span><br><span class="line">        outputs = (layer_output,) + outputs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if decoder, return the attn key/values as the last output</span></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            outputs = outputs + (present_key_value,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line">	<span class="comment">#全连接+激活、输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_forward_chunk</span>(<span class="params">self, attention_output</span>):</span></span><br><span class="line">        intermediate_output = self.intermediate(attention_output)</span><br><span class="line">        layer_output = self.output(intermediate_output, attention_output)</span><br><span class="line">        <span class="keyword">return</span> layer_output</span><br></pre></td></tr></table></figure>
<h5 id="源码分析-4">源码分析</h5>
<p>BertLayer被拆分为：BertAttention（attention机制）、BertIntermediate（全连接+激活）、BertOutput（输出层）。</p>
<p>注意这里用<code>apply_chunking_to_forward</code>函数将 input_tensors 分块成更小的输入张量部分然后执行<code>feed_forward_chunk</code>函数，<code>apply_chunking_to_forward</code>函数输入参数列表为：<code>forward_fn,chunk_size,chunk_dim,input_tensors</code>。</p>
<h4 id="bertattention">BertAttention</h4>
<h5 id="源码-5">源码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self = BertSelfAttention(config) <span class="comment">#自注意力机制</span></span><br><span class="line">        self.output = BertSelfOutput(config)  <span class="comment">#输出层</span></span><br><span class="line">        self.pruned_heads = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prune_heads</span>(<span class="params">self, heads</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(heads) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment">#找到准备剪枝的头和索引</span></span><br><span class="line">        heads, index = find_pruneable_heads_and_indices(</span><br><span class="line">            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads</span><br><span class="line">        )</span><br><span class="line">		<span class="comment">#直接剪枝权重</span></span><br><span class="line">        <span class="comment"># Prune linear layers</span></span><br><span class="line">        self.self.query = prune_linear_layer(self.self.query, index)</span><br><span class="line">        self.self.key = prune_linear_layer(self.self.key, index)</span><br><span class="line">        self.self.value = prune_linear_layer(self.self.value, index)</span><br><span class="line">        self.output.dense = prune_linear_layer(self.output.dense, index, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update hyper params and store pruned heads</span></span><br><span class="line">        self.self.num_attention_heads = self.self.num_attention_heads - <span class="built_in">len</span>(heads)</span><br><span class="line">        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads</span><br><span class="line">        self.pruned_heads = self.pruned_heads.union(heads)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="comment">#输出（context_layer, attention_probs）</span></span><br><span class="line">        self_outputs = self.self(</span><br><span class="line">            hidden_states,</span><br><span class="line">            attention_mask,</span><br><span class="line">            head_mask,</span><br><span class="line">            encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask,</span><br><span class="line">            past_key_value,</span><br><span class="line">            output_attentions,</span><br><span class="line">        )</span><br><span class="line">        attention_output = self.output(self_outputs[<span class="number">0</span>], hidden_states)</span><br><span class="line">        outputs = (attention_output,) + self_outputs[<span class="number">1</span>:]  <span class="comment"># add attentions if we output them</span></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h5 id="源码分析-5">源码分析</h5>
<p><code>find_pruneable_heads_and_indices</code>定位需要剪掉的 head，以及需要保留的维度下标 index：<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/0512bfe79e91b5817fc0fe30b0b901dc7f1acde2/src/transformers/modeling_utils.py#L87">find_pruneable_heads_and_indices源码</a></p>
<p><code>prune_linear_layer</code>这里是直接剪枝权重，负责将 <span class="math inline">\(W^k/W^q/W^v\)</span> 权重矩阵（连同 bias）中<strong>按照 index 保留没有被剪枝的维度后转移到新的矩阵</strong>。具体请查看<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/0512bfe79e91b5817fc0fe30b0b901dc7f1acde2/src/transformers/modeling_utils.py#L2034">prune_linear_layer源码</a>。</p>
<h4 id="bertselfattention">BertSelfAttention</h4>
<p>这个类才真正是做了attention操作，关于Encoder中的attention机制可以参考之前的博客：<a href="https://ifwind.github.io/2021/08/16/Transformer相关——（3）Attention机制/">Transformer相关——（3）Attention机制</a></p>
<h5 id="源码-6">源码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;The hidden size (<span class="subst">&#123;config.hidden_size&#125;</span>) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">f&quot;heads (<span class="subst">&#123;config.num_attention_heads&#125;</span>)&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line">		<span class="comment">#线性变换</span></span><br><span class="line">        <span class="comment">#这里 `key_layer/value_layer/query_laye`r 的形状为：(batch_size, num_attention_heads, sequence_length, attention_head_size)；</span></span><br><span class="line">        <span class="comment"># 一开始hidden_size 和 all_head_size 是一样的，做了剪枝操作（prune head）后all_head_size 变小。</span></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line">        self.position_embedding_type = <span class="built_in">getattr</span>(config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">            self.max_position_embeddings = config.max_position_embeddings</span><br><span class="line">            self.distance_embedding = nn.Embedding(<span class="number">2</span> * config.max_position_embeddings - <span class="number">1</span>, self.attention_head_size)</span><br><span class="line"></span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        new_x_shape = x.size()[:-<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        head_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_attention_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        past_key_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        output_attentions=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        mixed_query_layer = self.query(hidden_states)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 这个部分是 cross-attention部分，在bert中只用到了encoder的self-attention，这部分可以跳过。</span></span><br><span class="line">        <span class="comment"># If this is instantiated as a cross-attention module, the keys</span></span><br><span class="line">        <span class="comment"># and values come from an encoder; the attention mask needs to be</span></span><br><span class="line">        <span class="comment"># such that the encoder&#x27;s padding tokens are not attended to.</span></span><br><span class="line">        is_cross_attention = encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_cross_attention <span class="keyword">and</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># reuse k,v, cross_attentions</span></span><br><span class="line">            key_layer = past_key_value[<span class="number">0</span>]</span><br><span class="line">            value_layer = past_key_value[<span class="number">1</span>]</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">elif</span> is_cross_attention:</span><br><span class="line">            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))</span><br><span class="line">            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">elif</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            key_layer = self.transpose_for_scores(self.key(hidden_states))</span><br><span class="line">            value_layer = self.transpose_for_scores(self.value(hidden_states))</span><br><span class="line">            key_layer = torch.cat([past_key_value[<span class="number">0</span>], key_layer], dim=<span class="number">2</span>)</span><br><span class="line">            value_layer = torch.cat([past_key_value[<span class="number">1</span>], value_layer], dim=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#这里的 `transpose_for_scores` 用来把 `hidden_size` 拆成多个头输出的形状，并且将中间两维转置以进行矩阵相乘；</span></span><br><span class="line">            key_layer = self.transpose_for_scores(self.key(hidden_states))</span><br><span class="line">            value_layer = self.transpose_for_scores(self.value(hidden_states))</span><br><span class="line"></span><br><span class="line">        query_layer = self.transpose_for_scores(mixed_query_layer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            <span class="comment"># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span></span><br><span class="line">            <span class="comment"># Further calls to cross_attention layer can then reuse all cross-attention</span></span><br><span class="line">            <span class="comment"># key/value_states (first &quot;if&quot; case)</span></span><br><span class="line">            <span class="comment"># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span></span><br><span class="line">            <span class="comment"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span></span><br><span class="line">            <span class="comment"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span></span><br><span class="line">            <span class="comment"># if encoder bi-directional self-attention `past_key_value` is always `None`</span></span><br><span class="line">            past_key_value = (key_layer, value_layer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">        <span class="comment"># 点积相关性</span></span><br><span class="line">        <span class="comment"># (Q^Tk)</span></span><br><span class="line">        <span class="comment"># 这里 `attention_scores` 的形状为：(batch_size, num_attention_heads, sequence_length, sequence_length)，符合多个头单独计算获得的 attention map 形状。</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line">		<span class="comment">#对于不同的positional_embedding_type，有三种操作：</span></span><br><span class="line"></span><br><span class="line">		<span class="comment">#- absolute：默认值，这部分就不用处理；</span></span><br><span class="line">		<span class="comment">#- relative_key：对 key_layer 作处理，将其与这里的positional_embedding和 key 矩阵相乘作为 key 相关的位置编码；</span></span><br><span class="line">		<span class="comment">#- relative_key_query：对 key 和 value 都进行相乘以作为位置编码。</span></span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">            seq_length = hidden_states.size()[<span class="number">1</span>]</span><br><span class="line">            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">            distance = position_ids_l - position_ids_r</span><br><span class="line">            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - <span class="number">1</span>)</span><br><span class="line">            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span>:</span><br><span class="line">                relative_position_scores = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">                attention_scores = attention_scores + relative_position_scores</span><br><span class="line">            <span class="keyword">elif</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">                relative_position_scores_query = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding) <span class="comment">#torch.einsum是爱因斯坦求和约定</span></span><br><span class="line">                relative_position_scores_key = torch.einsum(<span class="string">&quot;bhrd,lrd-&gt;bhlr&quot;</span>, key_layer, positional_embedding)</span><br><span class="line">                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key</span><br><span class="line">		<span class="comment"># 对点积结果进行缩放</span></span><br><span class="line">        <span class="comment"># (Q^Tk)/\sqrt(d)</span></span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">        <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span></span><br><span class="line">            <span class="comment"># 注意这里的attention_mask，不需要mask的地方为0（不改变值），需要mask的地方为一个很小的数（相加以后经过softmax趋近于0）</span></span><br><span class="line">            attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">        attention_probs = nn.Softmax(dim=-<span class="number">1</span>)(attention_scores)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">        <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># head_mask又出现了，如果不设置默认是全 1，在这里就不会起作用；</span></span><br><span class="line">        <span class="comment"># Mask heads if we want to</span></span><br><span class="line">        <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_probs = attention_probs * head_mask</span><br><span class="line">		<span class="comment"># (Q^Tk)\sqrt(d)V</span></span><br><span class="line">        <span class="comment"># 形状为(batch_size, num_attention_heads, sequence_length, attention_head_size) </span></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:-<span class="number">2</span>] + (self.all_head_size,)</span><br><span class="line">       </span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line">		<span class="comment">#此时context_layer形状为(batch_size, sequence_length, hidden_size)</span></span><br><span class="line">        outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            outputs = outputs + (past_key_value,)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h5 id="源码分析-6">源码分析</h5>
<p>核心就是attention机制，有几个细节在这里总结一下：</p>
<ol type="1">
<li><p><code>hidden_size</code> 和 <code>all_head_size</code> ：一开始hidden_size 和 all_head_size 是一样的，做了剪枝操作（prune head）后all_head_size 变小。</p></li>
<li><p><strong>hidden_size 必须是 num_attention_heads 的整数倍</strong>：原因可查看<a href="https://ifwind.github.io/2021/08/16/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%883%EF%BC%89Attention%E6%9C%BA%E5%88%B6/">之前的博客</a>【定位词”<code>TIPS</code>“】</p></li>
<li><p><code>transpose_for_scores</code> 用来把 <code>hidden_size</code> 拆成多个头输出的形状，并且将中间两维转置以进行矩阵相乘；</p></li>
<li><p>关于<code>torch.einsum</code>爱因斯坦求和约定，可参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44954540">einsum满足你一切需要：深度学习中的爱因斯坦求和约定</a></p></li>
<li><p>对于不同的positional_embedding_type，有三种操作：</p>
<ul>
<li>absolute：默认值，不作处理；</li>
<li><strong>relative_key：对 key_layer 作处理，将positional_embedding和 key 矩阵相乘作为 key 相关的位置编码；</strong></li>
<li><strong>relative_key_query：对 key 和 value 都与positional_embedding进行相乘以作为各自的位置编码。</strong></li>
</ul></li>
<li><p><code>attention_scores = attention_scores + attention_mask</code>：这里的attention_mask，不需要mask的地方为0（不改变值），需要mask的地方为一个很小的数（相加以后经过softmax趋近于0）。前面特别提到<strong>encoder中的attention_mask是extended_attention_mask而不是BertModel前向传播输入的attention_mask</strong>。这个<code>extended_attention_mask</code>由<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/7fcee113c163a95d1b125ef35dc49a0a1aa13a50/src/transformers/modeling_utils.py"><code>get_extended_attention_mask</code>函数</a>产生。</p></li>
<li><p>又注意到，这个<code>get_extended_attention_mask</code>在调用时，用的是<code>self.get_extended_attention_mask</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)</span><br></pre></td></tr></table></figure>
<p>也就是说它是BertModel内部的函数，但在上面的源码中我们并没有发现该函数的实现。</p>
<p>事实上，这是因为BertModel是BertPreTrainedModel的子类，而BertPreTrainedModel是PreTrainedModel的子类，PreTrainedModel又是ModuleUtilsMixin的子类，该函数就是在 ModuleUtilsMixin中实现的。</p></li>
<li><p>代码还为attention_probs做了dropout，这是原始Transformer论文中的实现，虽然源码中也提到了比较unusual。</p></li>
</ol>
<h4 id="bertselfoutput">BertSelfOutput</h4>
<p>全连接+LayerNorm+dropout。</p>
<h5 id="源码-7">源码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h4 id="bertintermediate">BertIntermediate</h4>
<p>全连接+<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/f3558bbcfdfff25abe0137d00f8b4f88fb58eed3/src/transformers/activations.py">ACT2FN激活函数</a>。</p>
<h5 id="源码-8">源码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertIntermediate</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]<span class="comment">#ACT2FN激活函数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h4 id="bertoutput">BertOutput</h4>
<p>和上面的BertSelfOuput一毛一样：全连接+LayerNorm+dropout。</p>
<h5 id="源码-9">源码</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertSelfOutput</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states, input_tensor</span>):</span></span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>到这里BertEncoder终于结束了，接下来是输出部分。</p>
<h3 id="bertpooler">BertPooler</h3>
<h4 id="源码-10">源码</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, hidden_states</span>):</span></span><br><span class="line">        <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token.</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></table></figure>
<h4 id="源码分析-7">源码分析</h4>
<p>只是简单地取出了句子的第一个token，即<code>[CLS]</code>对应的向量，然后过一个全连接层和一个激活函数后输出：（这一部分是可选的，根据<code>add_pooling_layer</code>选择，因为pooling有很多不同的操作）。</p>
<h2 id="参考文献">参考文献</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT/3.1-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AABERT.md">3.1-如何实现一个BERT.md</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013510838/article/details/106908787/">NLP预训练模型2 -- BERT详解和源码分析</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">冬于</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/">https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89BERT%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ifwind.github.io" target="_blank">冬于的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E6%8A%8ABERT%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1/"><img class="prev-cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">BERT相关——（7）将BERT应用到下游任务</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/22/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%885%EF%BC%89Pre-train%20Model/"><img class="next-cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">BERT相关——（5）Pre-train Model</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/20/BERT相关——（2）Contextualized_Word_Embedding和ELMO模型/" title="BERT相关——（2）Contextualized Word Embedding和ELMO模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（2）Contextualized Word Embedding和ELMO模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（3）BERT模型/" title="BERT相关——（3）BERT模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（3）BERT模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（1）语言模型/" title="BERT相关——（1）语言模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（1）语言模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（4）GPT模型/" title="BERT相关——（4）GPT-2模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（4）GPT-2模型</div></div></a></div><div><a href="/2021/08/22/BERT相关——（5）Pre-train Model/" title="BERT相关——（5）Pre-train Model"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-22</div><div class="title">BERT相关——（5）Pre-train Model</div></div></a></div><div><a href="/2021/08/24/BERT相关——（7）把BERT应用到下游任务/" title="BERT相关——（7）将BERT应用到下游任务"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-24</div><div class="title">BERT相关——（7）将BERT应用到下游任务</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bert%E7%9B%B8%E5%85%B36bert%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90"><span class="toc-number">1.</span> <span class="toc-text">BERT相关——（6）BERT代码分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tokenizer%E5%AF%B9%E8%AF%AD%E6%96%99%E5%88%86%E8%AF%8Dberttokenizer"><span class="toc-number">1.2.</span> <span class="toc-text">Tokenizer对语料分词——BertTokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-number">1.2.1.</span> <span class="toc-text">需求分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BA%90%E7%A0%81"><span class="toc-number">1.2.2.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="toc-number">1.2.3.</span> <span class="toc-text">源码分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bertmodel"><span class="toc-number">1.3.</span> <span class="toc-text">BertModel</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90-1"><span class="toc-number">1.3.1.</span> <span class="toc-text">需求分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bertmodel-1"><span class="toc-number">1.3.2.</span> <span class="toc-text">BertModel</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-1"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">源码分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5-bertembeddings"><span class="toc-number">1.3.3.</span> <span class="toc-text">输入-BertEmbeddings</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-2"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-2"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">源码分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bertencoder"><span class="toc-number">1.3.4.</span> <span class="toc-text">BertEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#bertencoder-1"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">BertEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-3"><span class="toc-number">1.3.4.1.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3"><span class="toc-number">1.3.4.1.2.</span> <span class="toc-text">源码分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bertlayer"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">BertLayer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-4"><span class="toc-number">1.3.4.2.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4"><span class="toc-number">1.3.4.2.2.</span> <span class="toc-text">源码分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bertattention"><span class="toc-number">1.3.4.3.</span> <span class="toc-text">BertAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-5"><span class="toc-number">1.3.4.3.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-5"><span class="toc-number">1.3.4.3.2.</span> <span class="toc-text">源码分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bertselfattention"><span class="toc-number">1.3.4.4.</span> <span class="toc-text">BertSelfAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-6"><span class="toc-number">1.3.4.4.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-6"><span class="toc-number">1.3.4.4.2.</span> <span class="toc-text">源码分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bertselfoutput"><span class="toc-number">1.3.4.5.</span> <span class="toc-text">BertSelfOutput</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-7"><span class="toc-number">1.3.4.5.1.</span> <span class="toc-text">源码</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bertintermediate"><span class="toc-number">1.3.4.6.</span> <span class="toc-text">BertIntermediate</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-8"><span class="toc-number">1.3.4.6.1.</span> <span class="toc-text">源码</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bertoutput"><span class="toc-number">1.3.4.7.</span> <span class="toc-text">BertOutput</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-9"><span class="toc-number">1.3.4.7.1.</span> <span class="toc-text">源码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bertpooler"><span class="toc-number">1.3.5.</span> <span class="toc-text">BertPooler</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BA%90%E7%A0%81-10"><span class="toc-number">1.3.5.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-7"><span class="toc-number">1.3.5.2.</span> <span class="toc-text">源码分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.4.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 冬于</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
      appKey: 'YKdl4HKX9W6jLSdPlypgEtDM',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: 'https://e4lpmfet.lc-cn-n1-shared.com',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.17.0/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://e4lpmfet.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
        "X-LC-Key": 'YKdl4HKX9W6jLSdPlypgEtDM',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>