<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­” | å†¬äºçš„åšå®¢</title><meta name="keywords" content="æ·±åº¦å­¦ä¹ ,NLP,BERT"><meta name="author" content="å†¬äº"><meta name="copyright" content="å†¬äº"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­” å¼•è¨€ æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Transformersä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³é—®ç­”ä»»åŠ¡ä¸­çš„æœºå™¨é—®ç­”ä»»åŠ¡ã€‚ ä»»åŠ¡ä»‹ç» æ³¨æ„æˆ‘ä»¬è¿™é‡Œä¸»è¦è§£å†³çš„æ˜¯æŠ½å–å¼é—®ç­”ä»»åŠ¡ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€æ®µæ–‡æœ¬ï¼Œä»è¿™æ®µæ–‡æœ¬ä¸­æ‰¾å‡ºèƒ½å›ç­”è¯¥é—®é¢˜çš„æ–‡æœ¬ç‰‡æ®µï¼ˆspanï¼‰ã€‚æŠ½å–å¼é—®ç­”ä»»åŠ¡æ˜¯ä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆï¼Œå¹¶ä¸æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚ æ¯”å¦‚ï¼š è¾“å…¥ï¼š	é—®é¢˜ï¼šæˆ‘å®¶åœ¨å“ªé‡Œï¼Ÿ	æ–‡æœ¬ï¼šæˆ‘çš„å®¶åœ¨ä¸œåŒ—ã€‚è¾“">
<meta property="og:type" content="article">
<meta property="og:title" content="BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”">
<meta property="og:url" content="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/index.html">
<meta property="og:site_name" content="å†¬äºçš„åšå®¢">
<meta property="og:description" content="BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­” å¼•è¨€ æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Transformersä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³é—®ç­”ä»»åŠ¡ä¸­çš„æœºå™¨é—®ç­”ä»»åŠ¡ã€‚ ä»»åŠ¡ä»‹ç» æ³¨æ„æˆ‘ä»¬è¿™é‡Œä¸»è¦è§£å†³çš„æ˜¯æŠ½å–å¼é—®ç­”ä»»åŠ¡ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€æ®µæ–‡æœ¬ï¼Œä»è¿™æ®µæ–‡æœ¬ä¸­æ‰¾å‡ºèƒ½å›ç­”è¯¥é—®é¢˜çš„æ–‡æœ¬ç‰‡æ®µï¼ˆspanï¼‰ã€‚æŠ½å–å¼é—®ç­”ä»»åŠ¡æ˜¯ä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆï¼Œå¹¶ä¸æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚ æ¯”å¦‚ï¼š è¾“å…¥ï¼š	é—®é¢˜ï¼šæˆ‘å®¶åœ¨å“ªé‡Œï¼Ÿ	æ–‡æœ¬ï¼šæˆ‘çš„å®¶åœ¨ä¸œåŒ—ã€‚è¾“">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg">
<meta property="article:published_time" content="2021-08-30T06:23:57.000Z">
<meta property="article:modified_time" content="2021-08-30T12:07:33.482Z">
<meta property="article:author" content="å†¬äº">
<meta property="article:tag" content="æ·±åº¦å­¦ä¹ ">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LVK1P2D38K","apiKey":"8cbbb0bcbb5c7448f68b4fae01d4ccd5","indexName":"DongYu","hits":{"per_page":6},"languages":{"input_placeholder":"æœç´¢æ–‡ç« ","hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}","hits_stats":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼Œç”¨æ—¶ ${time} æ¯«ç§’"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'å¤©',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ¢ä¸ºç¹ä½“","cht_to_chs":"ä½ å·²åˆ‡æ¢ä¸ºç®€ä½“","day_to_night":"ä½ å·²åˆ‡æ¢ä¸ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ¢ä¸ºæµ…è‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-08-30 20:07:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">åŠ è½½ä¸­...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">66</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">50</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> åˆ†ç±»</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> ç±»åˆ«</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> éŸ³ä¹</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> ç…§ç‰‡</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> ç•™è¨€æ¿</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> å…³äº</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">å†¬äºçš„åšå®¢</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> åˆ†ç±»</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> ç±»åˆ«</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> éŸ³ä¹</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> ç…§ç‰‡</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> ç•™è¨€æ¿</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹é“¾</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2021-08-30T06:23:57.000Z" title="å‘è¡¨äº 2021-08-30 14:23:57">2021-08-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2021-08-30T12:07:33.482Z" title="æ›´æ–°äº 2021-08-30 20:07:33">2021-08-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">æ·±åº¦å­¦ä¹ </a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">7,536</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>35åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="bertå®æˆ˜4é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”">BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”</h1>
<h2 id="å¼•è¨€">å¼•è¨€</h2>
<p>æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">ğŸ¤— Transformers</a>ä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³<strong>é—®ç­”ä»»åŠ¡ä¸­çš„æœºå™¨é—®ç­”ä»»åŠ¡</strong>ã€‚</p>
<h3 id="ä»»åŠ¡ä»‹ç»">ä»»åŠ¡ä»‹ç»</h3>
<p>æ³¨æ„æˆ‘ä»¬è¿™é‡Œä¸»è¦è§£å†³çš„æ˜¯<strong>æŠ½å–å¼é—®ç­”ä»»åŠ¡</strong>ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€æ®µæ–‡æœ¬ï¼Œä»è¿™æ®µæ–‡æœ¬ä¸­æ‰¾å‡ºèƒ½å›ç­”è¯¥é—®é¢˜çš„æ–‡æœ¬ç‰‡æ®µï¼ˆspanï¼‰ã€‚<strong>æŠ½å–å¼é—®ç­”ä»»åŠ¡æ˜¯ä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆï¼Œå¹¶ä¸æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚</strong></p>
<p>æ¯”å¦‚ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">è¾“å…¥ï¼š</span><br><span class="line">	é—®é¢˜ï¼šæˆ‘å®¶åœ¨å“ªé‡Œï¼Ÿ</span><br><span class="line">	æ–‡æœ¬ï¼šæˆ‘çš„å®¶åœ¨ä¸œåŒ—ã€‚</span><br><span class="line">è¾“å‡ºï¼šä¸œåŒ—</span><br></pre></td></tr></table></figure>
<p>è¿™åœ¨<a href="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E6%8A%8ABERT%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1/#copy%20from%20input.html">ä¹‹å‰çš„åšå®¢ã€å®šä½è¯ï¼šcopy from inputã€‘</a>ä¸­æœ‰è¯¦ç»†ä»‹ç»ï¼Œè¿™é‡Œç®€å•å¤ä¹ ä¸€ä¸‹ï¼Œå°±æ˜¯éœ€è¦å¯¹æ¯ä¸ªtokenè¿›è¡Œåˆ†ç±»ï¼Œçœ‹tokenæ˜¯ä¸æ˜¯ç­”æ¡ˆæ–‡æœ¬ç‰‡æ®µçš„startæˆ–æ˜¯endã€‚</p>
<blockquote>
<p><img src="/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/æŠ½å–å¼é—®ç­”.png" style="zoom:80%;"><img src="/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/æŠ½å–å¼é—®ç­”2.png" style="zoom:80%;"></p>
</blockquote>
<p>ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š</p>
<ol type="1">
<li>æ•°æ®åŠ è½½</li>
<li>æ•°æ®é¢„å¤„ç†</li>
<li>å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„<code>Trainer</code>æ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›</li>
<li>æ¨¡å‹è¯„ä¼°ã€‚</li>
</ol>
<h3 id="å‰æœŸå‡†å¤‡">å‰æœŸå‡†å¤‡</h3>
<p>å®‰è£…ä»¥ä¸‹åº“ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install datasets transformers</span><br><span class="line"><span class="comment">#transformers==4.9.2</span></span><br><span class="line"><span class="comment">#datasets==1.11.0</span></span><br></pre></td></tr></table></figure>
<h2 id="æ•°æ®åŠ è½½">æ•°æ®åŠ è½½</h2>
<h3 id="æ•°æ®é›†ä»‹ç»">æ•°æ®é›†ä»‹ç»</h3>
<p>æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†æ˜¯<a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">SQUAD 2</a>ï¼ŒStanford Question Answering Dataset (SQuAD) æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—å·¥å¯¹ä¸€ç»„ç»´åŸºç™¾ç§‘æ–‡ç« æå‡ºçš„é—®é¢˜ç»„æˆï¼Œæ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ä»ç›¸åº”çš„é˜…è¯»æ–‡ç« ä¸­èŠ‚é€‰å‡ºæ¥çš„ï¼Œæˆ–è€…è¿™ä¸ªé—®é¢˜å¯èƒ½æ˜¯æ— æ³•å›ç­”çš„ã€‚</p>
<h3 id="åŠ è½½æ•°æ®">åŠ è½½æ•°æ®</h3>
<p>è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># squad_v2ç­‰äºTrueæˆ–è€…Falseåˆ†åˆ«ä»£è¡¨ä½¿ç”¨SQUAD v1 æˆ–è€… SQUAD v2ã€‚</span></span><br><span class="line"><span class="comment"># å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯å…¶ä»–æ•°æ®é›†ï¼Œé‚£ä¹ˆTrueä»£è¡¨çš„æ˜¯ï¼šæ¨¡å‹å¯ä»¥å›ç­”â€œä¸å¯å›ç­”â€é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯éƒ¨åˆ†é—®é¢˜ä¸ç»™å‡ºç­”æ¡ˆï¼Œè€ŒFalseåˆ™ä»£è¡¨æ‰€æœ‰é—®é¢˜å¿…é¡»å›ç­”ã€‚</span></span><br><span class="line">squad_v2 = <span class="literal">False</span></span><br><span class="line"><span class="comment"># ä¸‹è½½æ•°æ®ï¼ˆç¡®ä¿æœ‰ç½‘ç»œï¼‰</span></span><br><span class="line">datasets = load_dataset(<span class="string">&quot;squad_v2&quot;</span> <span class="keyword">if</span> squad_v2 <span class="keyword">else</span> <span class="string">&quot;squad&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>å¦‚æœä½ ä½¿ç”¨çš„æ˜¯è‡ªå·±çš„æ•°æ®ï¼Œå‚è€ƒ<a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE">ç¬¬ä¸€ç¯‡å®æˆ˜åšå®¢ã€å®šä½è¯ï¼šåŠ è½½æ•°æ®ã€‘</a>åŠ è½½è‡ªå·±çš„æ•°æ®ã€‚</p>
<p>ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚æ— è®ºæ˜¯è®­ç»ƒé›†ã€éªŒè¯é›†è¿˜æ˜¯æµ‹è¯•é›†ï¼Œå¯¹äºæ¯ä¸€ä¸ªé—®ç­”æ•°æ®æ ·æœ¬éƒ½ä¼šæœ‰â€œcontext&quot;, &quot;question&quot;å’Œâ€œanswersâ€ä¸‰ä¸ªkeyã€‚</p>
<ol type="1">
<li>answersä»£è¡¨ç­”æ¡ˆï¼Œanswersé™¤äº†ç»™å‡ºäº†æ–‡æœ¬ç‰‡æ®µé‡Œçš„ç­”æ¡ˆæ–‡æœ¬ä¹‹å¤–ï¼Œ<strong>è¿˜ç»™å‡ºäº†è¯¥answeræ‰€åœ¨ä½ç½®ï¼ˆä»¥characterå¼€å§‹è®¡ç®—ï¼Œä¸‹é¢çš„ä¾‹å­æ˜¯ç¬¬515ä½</strong>ï¼‰ã€‚</li>
<li>contextä»£è¡¨æ–‡æœ¬ç‰‡æ®µï¼›</li>
<li>questionä»£è¡¨é—®é¢˜ã€‚</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&#123;&#x27;answers&#x27;: &#123;&#x27;answer_start&#x27;: [515], &#x27;text&#x27;: [&#x27;Saint Bernadette Soubirous&#x27;]&#125;,</span></span><br><span class="line"><span class="comment"># &#x27;context&#x27;: &#x27;Architecturally, the school has a Catholic character. Atop the Main Building\&#x27;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;id&#x27;: &#x27;5733be284776f41900661182&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;question&#x27;: &#x27;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;title&#x27;: &#x27;University_of_Notre_Dame&#x27;&#125;</span></span><br></pre></td></tr></table></figure>
<p>ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> ClassLabel, <span class="type">Sequence</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(typ, <span class="type">Sequence</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(typ.feature, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> x: [typ.feature.names[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show_random_elements(datasets[<span class="string">&quot;train&quot;</span>], num_examples=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
answers
</th>
<th>
context
</th>
<th>
id
</th>
<th>
question
</th>
<th>
title
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
{'answer_start': [185], 'text': ['diesel fuel']}
</td>
<td>
In Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.
</td>
<td>
571b074c9499d21900609be3
</td>
<td>
Besides crude oil, what does the Suncor Energy plant produce?
</td>
<td>
Asphalt
</td>
</tr>
</tbody>
</table>
<h2 id="æ•°æ®é¢„å¤„ç†">æ•°æ®é¢„å¤„ç†</h2>
<p>åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚</p>
<p>ä»ç„¶æ˜¯ä¸¤ä¸ªæ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹ï¼š</p>
<ol type="1">
<li>åˆ†è¯ï¼›</li>
<li>è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼ï¼›</li>
</ol>
<p><code>Tokenizer</code>ç”¨äºä¸Šé¢ä¸¤æ­¥æ•°æ®é¢„å¤„ç†å·¥ä½œï¼š<code>Tokenizer</code>é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚</p>
<h3 id="åˆå§‹åŒ–tokenizer">åˆå§‹åŒ–Tokenizer</h3>
<p><a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer">ä¹‹å‰çš„åšå®¢</a>å·²ç»ä»‹ç»äº†ä¸€äº›Tokenizerçš„å†…å®¹ï¼Œå¹¶åšäº†Tokenizeråˆ†è¯çš„ç¤ºä¾‹ï¼Œè¿™é‡Œä¸å†é‡å¤ã€‚<code>use_fast=True</code>æŒ‡å®šä½¿ç”¨fastç‰ˆæœ¬çš„tokenizerã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼">è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼</h3>
<p>æœºå™¨é—®ç­”é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸å°†questionå’Œcontextæ‹¼æ¥ä¹‹åä½œä¸ºè¾“å…¥ï¼Œç„¶åè®©æ¨¡å‹ä»contexté‡Œå¯»æ‰¾ç­”æ¡ˆã€‚<strong>å¯¹äºcontextä¸­æ— ç­”æ¡ˆçš„æƒ…å†µï¼Œæˆ‘ä»¬ç›´æ¥å°†æ ‡æ³¨çš„ç­”æ¡ˆèµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®æ”¾ç½®åœ¨CLSçš„ä¸‹æ ‡å¤„ã€‚</strong></p>
<p>æˆ‘ä»¬å°†questionä½œä¸ºtokenizerçš„å¥å­1ï¼Œcontextä½œä¸ºtokenizerçš„å¥å­2ï¼Œtokenizerä¼šå°†ä»–ä»¬æ‹¼æ¥èµ·æ¥å¹¶åŠ å…¥ç‰¹æ®Šå­—ç¬¦ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example = datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line">tokenized_example=tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>])</span><br><span class="line">tokenized_example[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line"><span class="comment">#[101,2000,3183,2106,1996,6261,2984,9382,3711,1999,8517,1999,10223,26371,2605,1029,102,6549,2135,1010,1996,2082,2038,1037,3234,2839,1012,10234,1996,2364,2311,1005,1055,2751,8514,2003,1037,3585,6231,1997,1996,6261,2984,1012,3202,1999,2392,1997,1996,2364,2311,1998,5307,2009,1010,2003,1037,6967,6231,1997,4828,2007,2608,2039,14995,6924,2007,1996,5722,1000,2310,3490,2618,4748,2033,18168,5267,1000,1012,2279,2000,1996,2364,2311,2003,1996,13546,1997,1996,6730,2540,1012,3202,2369,1996,13546,2003,1996,24665,23052,1010,1037,14042,2173,1997,7083,1998,9185,1012,2009,2003,1037,15059,1997,1996,24665,23052,2012,10223,26371,1010,2605,2073,1996,6261,2984,22353,2135,2596,2000,3002,16595,9648,4674,2061,12083,9711,2271,1999,8517,1012,2012,1996,2203,1997,1996,2364,3298,1006,1998,1999,1037,3622,2240,2008,8539,2083,1017,11342,1998,1996,2751,8514,1007,1010,2003,1037,3722,1010,2715,2962,6231,1997,2984,1012,102]</span></span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬<strong>ä½¿ç”¨<code>sequence_ids</code>æ–¹æ³•æ¥è·å–maskåŒºåˆ†questionå’Œcontext</strong>ã€‚ <code>None</code>å¯¹åº”äº†special tokensï¼Œç„¶å0æˆ–è€…1åˆ†è¡¨ä»£è¡¨ç¬¬1ä¸ªæ–‡æœ¬å’Œç¬¬2ä¸ªæ–‡æœ¬ï¼Œç”±äºæˆ‘ä»¬questionç¬¬1ä¸ªä¼ å…¥ï¼Œcontextç¬¬2ä¸ªä¼ å…¥ï¼Œæ‰€ä»¥åˆ†åˆ«å¯¹åº”questionå’Œcontextã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sequence_ids = tokenized_example.sequence_ids()</span><br><span class="line"><span class="built_in">print</span>(sequence_ids)</span><br><span class="line"><span class="comment">#[None,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,None,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,None]</span></span><br></pre></td></tr></table></figure>
<p>ç°åœ¨éœ€è¦ç‰¹åˆ«æ€è€ƒä¸€ä¸ªé—®é¢˜ï¼š<strong>å½“é‡åˆ°è¶…é•¿contextæ—¶ï¼ˆè¶…è¿‡äº†æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§é•¿åº¦ï¼‰ä¼šä¸ä¼šå¯¹æ¨¡å‹é€ æˆå½±å“å‘¢ï¼Ÿ</strong></p>
<p><strong>ä¸€èˆ¬æ¥è¯´é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æœ‰æœ€å¤§é•¿åº¦è¦æ±‚ï¼Œç„¶åé€šå¸¸å°†è¶…é•¿è¾“å…¥è¿›è¡Œæˆªæ–­</strong>ã€‚ä½†æ˜¯ï¼Œ<strong>å¦‚æœæˆ‘ä»¬å°†é—®ç­”æ•°æ®ä¸‰å…ƒç»„&lt;question, context, answer&gt;ä¸­è¶…é•¿contextæˆªæ–­ï¼Œé‚£ä¹ˆå¯èƒ½ä¸¢æ‰ç­”æ¡ˆ</strong>ï¼ˆå› ä¸ºæ˜¯ä»contextä¸­æŠ½å–å‡ºä¸€ä¸ªå°ç‰‡æ®µä½œä¸ºç­”æ¡ˆï¼‰ã€‚</p>
<h4 id="é‚£ä¹ˆé¢„è®­ç»ƒæœºå™¨é—®ç­”æ¨¡å‹æ˜¯å¦‚ä½•å¤„ç†è¶…é•¿æ–‡æœ¬çš„å‘¢"><strong>é‚£ä¹ˆé¢„è®­ç»ƒæœºå™¨é—®ç­”æ¨¡å‹æ˜¯å¦‚ä½•å¤„ç†è¶…é•¿æ–‡æœ¬çš„å‘¢ï¼Ÿ</strong></h4>
<p>æˆ‘ä»¬é¦–å…ˆæ‰¾åˆ°ä¸€ä¸ªè¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦çš„ä¾‹å­ï¼Œç„¶ååˆ†æå¤„ç†ä¸Šè¿°é—®é¢˜çš„æœºåˆ¶ã€‚</p>
<p>forå¾ªç¯éå†æ•°æ®é›†ï¼Œå¯»æ‰¾ä¸€ä¸ªè¶…é•¿æ ·æœ¬ï¼Œæˆ‘ä»¬å‰é¢é€‰æ‹©çš„æ¨¡å‹æ‰€è¦æ±‚çš„æœ€å¤§è¾“å…¥æ˜¯384ï¼ˆç»å¸¸ä½¿ç”¨çš„è¿˜æœ‰512ï¼‰ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(datasets[<span class="string">&quot;train&quot;</span>]):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>])[<span class="string">&quot;input_ids&quot;</span>]) &gt; <span class="number">384</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">example = datasets[<span class="string">&quot;train&quot;</span>][i]</span><br></pre></td></tr></table></figure>
<p>å¦‚æœä¸æˆªæ–­çš„è¯ï¼Œé‚£ä¹ˆè¾“å…¥çš„é•¿åº¦æ˜¯396ï¼Œå¦‚æœæˆ‘ä»¬æˆªæ–­æˆæœ€å¤§é•¿åº¦384ï¼Œå°†ä¼šä¸¢å¤±è¶…é•¿éƒ¨åˆ†çš„ä¿¡æ¯</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>])[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line"><span class="comment">#396</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>], max_length=max_length, truncation=<span class="string">&quot;only_second&quot;</span>)[<span class="string">&quot;input_ids&quot;</span>]) <span class="comment">#truncation=&quot;only_second&quot;è¡¨ç¤ºåªå¯¹å¥å­2è¿›è¡Œæˆªæ–­</span></span><br><span class="line"><span class="comment">#384</span></span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬æŠŠ<strong>è¶…é•¿çš„è¾“å…¥åˆ‡ç‰‡ä¸ºå¤šä¸ªè¾ƒçŸ­çš„è¾“</strong>å…¥ï¼Œæ¯ä¸ªè¾“å…¥éƒ½è¦æ»¡è¶³æ¨¡å‹æœ€å¤§é•¿åº¦è¾“å…¥è¦æ±‚ã€‚ç”±äºç­”æ¡ˆå¯èƒ½å­˜åœ¨ä¸åˆ‡ç‰‡çš„åœ°æ–¹ï¼Œå› æ­¤<strong>å…è®¸ç›¸é‚»åˆ‡ç‰‡ä¹‹é—´æœ‰äº¤é›†ï¼Œtokenizerä¸­é€šè¿‡<code>doc_stride</code>å‚æ•°æ§åˆ¶</strong>ã€‚</p>
<p>é¢„è®­ç»ƒæ¨¡å‹çš„tokenizeråŒ…è£…äº†æ–¹æ³•å¸®åŠ©æˆ‘ä»¬å®Œæˆä¸Šè¿°æ­¥éª¤ï¼Œåªéœ€è¦è®¾å®šä¸€äº›å‚æ•°å³å¯ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_length = <span class="number">384</span> <span class="comment"># è¾“å…¥featureçš„æœ€å¤§é•¿åº¦ï¼Œquestionå’Œcontextæ‹¼æ¥ä¹‹å</span></span><br><span class="line">doc_stride = <span class="number">128</span> <span class="comment"># 2ä¸ªåˆ‡ç‰‡ä¹‹é—´çš„é‡åˆtokenæ•°é‡ã€‚</span></span><br></pre></td></tr></table></figure>
<p>æ³¨æ„ï¼š<strong>ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åªå¯¹contextè¿›è¡Œåˆ‡ç‰‡ï¼Œä¸ä¼šå¯¹é—®é¢˜è¿›è¡Œåˆ‡ç‰‡ï¼Œç”±äºcontextæ˜¯æ‹¼æ¥åœ¨questionåé¢çš„ï¼Œå¯¹åº”ç€ç¬¬2ä¸ªæ–‡æœ¬ï¼Œæ‰€ä»¥ä½¿ç”¨<code>only_second</code>æ§åˆ¶</strong>ã€‚<strong>tokenizerä½¿ç”¨<code>doc_stride</code>æ§åˆ¶åˆ‡ç‰‡ä¹‹é—´çš„é‡åˆé•¿åº¦ã€‚</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_example = tokenizer(</span><br><span class="line">    example[<span class="string">&quot;question&quot;</span>],</span><br><span class="line">    example[<span class="string">&quot;context&quot;</span>],</span><br><span class="line">    max_length=max_length,</span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    stride=doc_stride</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>ç”±äºå¯¹è¶…é•¿è¾“å…¥è¿›è¡Œäº†åˆ‡ç‰‡ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¤šä¸ªè¾“å…¥ï¼Œè¿™äº›è¾“å…¥input_idså¯¹åº”çš„é•¿åº¦æ˜¯</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokenized_example[<span class="string">&quot;input_ids&quot;</span>]]</span><br><span class="line"><span class="comment">#[384, 157]</span></span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¯ä»¥å°†é¢„å¤„ç†åçš„token IDsï¼Œinput_idsè¿˜åŸä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ–¹ä¾¿æ£€æŸ¥åˆ‡ç‰‡ç»“æœã€‚å¯ä»¥å‘ç°<strong>tokenizerè‡ªåŠ¨å¸®æˆ‘ä»¬ä¸ºç¬¬äºŒä¸ªåˆ‡ç‰‡çš„contextæ‹¼æ¥äº†questionæ–‡æœ¬</strong>ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokenized_example[<span class="string">&quot;input_ids&quot;</span>][:<span class="number">2</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;åˆ‡ç‰‡: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(tokenizer.decode(x))</span><br><span class="line"><span class="comment">#åˆ‡ç‰‡: 0</span></span><br><span class="line"><span class="comment">#[CLS] how many wins does the notre dame men&#x27;s basketball team have? [SEP] the men&#x27;s basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla&#x27;s record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla&#x27;s 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey&#x27;s fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]</span></span><br><span class="line"><span class="comment">#åˆ‡ç‰‡: 1</span></span><br><span class="line"><span class="comment">#[CLS] how many wins does the notre dame men&#x27;s basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey&#x27;s fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]</span></span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬çŸ¥é“æœºå™¨é—®ç­”æ¨¡å‹å°†ä½¿ç”¨ç­”æ¡ˆçš„ä½ç½®ï¼ˆç­”æ¡ˆçš„èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®ï¼Œstartå’Œendï¼‰ä½œä¸ºè®­ç»ƒæ ‡ç­¾ï¼ˆè€Œä¸æ˜¯ç­”æ¡ˆçš„token IDSï¼‰ã€‚é‚£ä¹ˆ<strong>ç”±äºè¿›è¡Œäº†åˆ‡ç‰‡ï¼Œä¸€ä¸ªæ–°çš„é—®é¢˜å‡ºç°äº†ï¼šç­”æ¡ˆæ‰€åœ¨çš„ä½ç½®è¢«æ”¹å˜äº†ï¼Œå› æ­¤éœ€è¦é‡æ–°å¯»æ‰¾ç­”æ¡ˆæ‰€åœ¨ä½ç½®ï¼ˆç›¸å¯¹äºæ¯ä¸€ç‰‡contextå¼€å¤´çš„ç›¸å¯¹ä½ç½®</strong>ï¼‰ã€‚</p>
<p>æ‰€ä»¥<strong>åˆ‡ç‰‡éœ€è¦å’ŒåŸå§‹è¾“å…¥æœ‰ä¸€ä¸ªå¯¹åº”å…³ç³»ï¼Œæ¯ä¸ªtokenåœ¨åˆ‡ç‰‡åcontextçš„ä½ç½®å’ŒåŸå§‹è¶…é•¿contexté‡Œä½ç½®çš„å¯¹åº”å…³ç³»</strong>ã€‚</p>
<h4 id="è·å–åˆ‡ç‰‡å‰åçš„ä½ç½®å¯¹åº”å…³ç³»">è·å–åˆ‡ç‰‡å‰åçš„ä½ç½®å¯¹åº”å…³ç³»</h4>
<p><strong>åœ¨tokenizeré‡Œå¯ä»¥ä½¿ç”¨<code>return_offsets_mapping=True</code>å‚æ•°å¾—åˆ°è¿™ä¸ªå¯¹åº”å…³ç³»çš„map</strong>ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_example = tokenizer(</span><br><span class="line">    example[<span class="string">&quot;question&quot;</span>],</span><br><span class="line">    example[<span class="string">&quot;context&quot;</span>],</span><br><span class="line">    max_length=max_length,</span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">    stride=doc_stride</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬æ‰“å°tokenized_exampleåˆ‡ç‰‡1ï¼ˆä¹Ÿå°±æ˜¯ç¬¬äºŒä¸ªåˆ‡ç‰‡ï¼Œå‰30ä¸ªtokensï¼‰åœ¨åŸå§‹contextç‰‡é‡Œçš„ä½ç½®ã€‚æ³¨æ„ç‰¹æ®Štokenæ˜¯ï¼ˆå¦‚<code>[CLS]</code>è®¾å®šä¸º(0, 0)ï¼‰ï¼Œæ˜¯å› ä¸ºè¿™ä¸ªtokenä¸å±äºqeustionæˆ–è€…answerçš„ä¸€éƒ¨åˆ†ã€‚ç¬¬2ä¸ªtokenå¯¹åº”çš„èµ·å§‹å’Œç»“æŸä½ç½®æ˜¯0å’Œ3ã€‚å¯ä»¥å‘ç°<strong>åˆ‡ç‰‡1contextéƒ¨åˆ†ç¬¬2ä¸ªtokenæ²¡æœ‰ä»(0,N)æ ‡è®°ï¼Œè€Œæ˜¯ä»è®°å½•äº†å…¶åœ¨åŸæœ¬è¶…é•¿æ–‡æœ¬ä¸­çš„ä½ç½®ã€‚</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ‰“å°åˆ‡ç‰‡å‰åä½ç½®ä¸‹æ ‡çš„å¯¹åº”å…³ç³»</span></span><br><span class="line"><span class="built_in">print</span>(tokenized_example[<span class="string">&quot;offset_mapping&quot;</span>][<span class="number">1</span>][:<span class="number">30</span>])</span><br><span class="line"><span class="comment">#[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172)]</span></span><br></pre></td></tr></table></figure>
<p>å› æ­¤æˆ‘ä»¬å¯ä»¥æ ¹æ®åˆ‡ç‰‡åçš„<code>token id</code>è½¬åŒ–å¯¹åº”çš„<code>token</code>ï¼Œç„¶åä½¿ç”¨<code>offset_mapping</code>å‚æ•°æ˜ å°„å›åˆ‡ç‰‡å‰çš„tokenä½ç½®ï¼Œæ‰¾åˆ°åŸå§‹ä½ç½®çš„tokensã€‚ç”±äºquestionæ‹¼æ¥åœ¨contextå‰é¢ï¼Œæ‰€ä»¥ç›´æ¥ä»questioné‡Œæ ¹æ®ä¸‹æ ‡æ‰¾ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">first_token_id = tokenized_example[<span class="string">&quot;input_ids&quot;</span>][<span class="number">0</span>][<span class="number">1</span>] <span class="comment">#2129</span></span><br><span class="line">offsets = tokenized_example[<span class="string">&quot;offset_mapping&quot;</span>][<span class="number">0</span>][<span class="number">1</span>] <span class="comment">#(0, 3)</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.convert_ids_to_tokens([first_token_id])[<span class="number">0</span>], example[<span class="string">&quot;question&quot;</span>][offsets[<span class="number">0</span>]:offsets[<span class="number">1</span>]])</span><br><span class="line"><span class="comment">#how How</span></span><br></pre></td></tr></table></figure>
<p>å› æ­¤æˆ‘ä»¬å¾—åˆ°æ›´æ–°ç­”æ¡ˆç›¸å¯¹åˆ‡ç‰‡contextä½ç½®çš„æµç¨‹ï¼š</p>
<ol type="1">
<li>æˆ‘ä»¬é¦–å…ˆæ‰¾åˆ°contextåœ¨å¥å­1å’Œå¥å­2æ‹¼æ¥åçš„å¥å­ä¸­çš„èµ·å§‹ä½ç½®token_start_indexã€ç»ˆæ­¢ä½ç½®token_end_indexï¼›</li>
<li>ç„¶ååˆ¤æ–­ç­”æ¡ˆæ˜¯å¦åœ¨æ–‡æœ¬åŒºé—´å¤–éƒ¨ï¼š
<ol type="1">
<li>è‹¥åœ¨ï¼Œåˆ™æ›´æ–°ç­”æ¡ˆçš„ä½ç½®ï¼›</li>
<li>è‹¥ä¸åœ¨ï¼Œåˆ™è®©ç­”æ¡ˆæ ‡æ³¨åœ¨CLS tokenä½ç½®ã€‚</li>
</ol></li>
</ol>
<p>æœ€ç»ˆæˆ‘ä»¬å¯ä»¥æ›´æ–°æ ‡æ³¨çš„ç­”æ¡ˆåœ¨é¢„å¤„ç†ä¹‹åçš„featuresé‡Œçš„ä½ç½®ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">answers = example[<span class="string">&quot;answers&quot;</span>] <span class="comment">#ç­”æ¡ˆ&#123;&#x27;answer_start&#x27;: [30], &#x27;text&#x27;: [&#x27;over 1,600&#x27;]&#125;</span></span><br><span class="line">start_char = answers[<span class="string">&quot;answer_start&quot;</span>][<span class="number">0</span>] <span class="comment">#ç­”æ¡ˆèµ·å§‹ä½ç½®30</span></span><br><span class="line">end_char = start_char + <span class="built_in">len</span>(answers[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])<span class="comment">#ç­”æ¡ˆç»ˆæ­¢ä½ç½®40</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ‰¾åˆ°å½“å‰æ–‡æœ¬çš„Start token index.</span></span><br><span class="line">token_start_index = <span class="number">0</span> <span class="comment">#å¾—åˆ°contextåœ¨å¥å­1å’Œå¥å­2æ‹¼æ¥åçš„å¥å­ä¸­çš„èµ·å§‹ä½ç½®</span></span><br><span class="line"><span class="keyword">while</span> sequence_ids[token_start_index] != <span class="number">1</span>: <span class="comment">#sequence_idsåŒºåˆ†questionå’Œcontext</span></span><br><span class="line">    token_start_index += <span class="number">1</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># æ‰¾åˆ°å½“å‰æ–‡æœ¬çš„End token index.</span></span><br><span class="line">token_end_index = <span class="built_in">len</span>(tokenized_example[<span class="string">&quot;input_ids&quot;</span>][<span class="number">0</span>]) - <span class="number">1</span><span class="comment">#å¾—åˆ°contextåœ¨å¥å­1å’Œå¥å­2æ‹¼æ¥åçš„å¥å­ä¸­çš„ç»ˆæ­¢ä½ç½®ï¼Œå¯èƒ½è¿˜è¦å»æ‰ä¸€äº›padding</span></span><br><span class="line"><span class="keyword">while</span> sequence_ids[token_end_index] != <span class="number">1</span>:</span><br><span class="line">    token_end_index -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ£€æµ‹ç­”æ¡ˆæ˜¯å¦åœ¨æ–‡æœ¬åŒºé—´çš„å¤–éƒ¨ï¼Œè¿™ç§æƒ…å†µä¸‹æ„å‘³ç€è¯¥æ ·æœ¬çš„æ•°æ®æ ‡æ³¨åœ¨CLS tokenä½ç½®ã€‚</span></span><br><span class="line">offsets = tokenized_example[<span class="string">&quot;offset_mapping&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="keyword">if</span> (offsets[token_start_index][<span class="number">0</span>] &lt;= start_char <span class="keyword">and</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char): <span class="comment">#ç­”æ¡ˆåœ¨æ–‡æœ¬å†…</span></span><br><span class="line">    <span class="comment"># å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°answeræ‰€åœ¨ä½ç½®çš„ä¸¤ä¾§.</span></span><br><span class="line">    <span class="comment"># æ³¨æ„ï¼šç­”æ¡ˆåœ¨æœ€æœ«å°¾çš„è¾¹ç•Œæ¡ä»¶.</span></span><br><span class="line">    <span class="keyword">while</span> token_start_index &lt; <span class="built_in">len</span>(offsets) <span class="keyword">and</span> offsets[token_start_index][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">        token_start_index += <span class="number">1</span> <span class="comment">#ä¹‹å‰çš„token_start_indexåœ¨contextçš„ç¬¬ä¸€ä¸ªtokenä½ç½®</span></span><br><span class="line">    start_position = token_start_index - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">        token_end_index -= <span class="number">1</span><span class="comment">#ä¹‹å‰çš„token_end_indexåœ¨contextçš„æœ€åä¸€ä¸ªtokenä½ç½®</span></span><br><span class="line">    end_position = token_end_index + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;start_position: &#123;&#125;, end_position: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(start_position, end_position))</span><br><span class="line"><span class="keyword">else</span>: <span class="comment">#ç­”æ¡ˆåœ¨æ–‡æœ¬å¤–</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The answer is not in this feature.&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>start_position: 23, end_position: 26</p>
<p>æˆ‘ä»¬å¯¹ç­”æ¡ˆçš„ä½ç½®è¿›è¡ŒéªŒè¯ï¼šä½¿ç”¨ç­”æ¡ˆæ‰€åœ¨ä½ç½®ä¸‹æ ‡ï¼Œå–åˆ°å¯¹åº”çš„token IDï¼Œç„¶åè½¬åŒ–ä¸ºæ–‡æœ¬ï¼Œç„¶åå’ŒåŸå§‹ç­”æ¡ˆè¿›è¡Œä½†å¯¹æ¯”ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tokenizer.decode(tokenized_example[<span class="string">&quot;input_ids&quot;</span>][<span class="number">0</span>][start_position: end_position+<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(answers[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>over 1, 600 over 1,600</p>
<p>æ­¤å¤–ï¼Œè¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼š<strong>æœ‰æ—¶å€™questionæ‹¼æ¥contextï¼Œè€Œæœ‰æ—¶å€™æ˜¯contextæ‹¼æ¥questionï¼Œä¸åŒçš„æ¨¡å‹æœ‰ä¸åŒçš„è¦æ±‚ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä½¿ç”¨<code>padding_side</code>å‚æ•°æ¥æŒ‡å®šã€‚</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pad_on_right = tokenizer.padding_side == <span class="string">&quot;right&quot;</span> <span class="comment">#contextåœ¨å³è¾¹</span></span><br></pre></td></tr></table></figure>
<p>ç°åœ¨ï¼ŒæŠŠæ‰€æœ‰æ­¥éª¤åˆå¹¶åˆ°ä¸€èµ·ã€‚å¦‚æœ<code>allow_impossible_answers</code>è¿™ä¸ªå‚æ•°æ˜¯<code>False</code>çš„è¯ï¼Œæ— ç­”æ¡ˆçš„æ ·æœ¬éƒ½ä¼šè¢«æ‰”æ‰ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_train_features</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="comment"># æ—¢è¦å¯¹examplesè¿›è¡Œtruncationï¼ˆæˆªæ–­ï¼‰å’Œpaddingï¼ˆè¡¥å…¨ï¼‰è¿˜è¦è¿˜è¦ä¿ç•™æ‰€æœ‰ä¿¡æ¯ï¼Œæ‰€ä»¥è¦ç”¨çš„åˆ‡ç‰‡çš„æ–¹æ³•ã€‚</span></span><br><span class="line">    <span class="comment"># æ¯ä¸€ä¸ªè¶…é•¿æ–‡æœ¬exampleä¼šè¢«åˆ‡ç‰‡æˆå¤šä¸ªè¾“å…¥ï¼Œç›¸é‚»ä¸¤ä¸ªè¾“å…¥ä¹‹é—´ä¼šæœ‰äº¤é›†ã€‚</span></span><br><span class="line">    tokenized_examples = tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;question&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;context&quot;</span>],</span><br><span class="line">        examples[<span class="string">&quot;context&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;question&quot;</span>],</span><br><span class="line">        truncation=<span class="string">&quot;only_second&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;only_first&quot;</span>,</span><br><span class="line">        max_length=max_length,</span><br><span class="line">        stride=doc_stride,</span><br><span class="line">        return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">        return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æˆ‘ä»¬ä½¿ç”¨overflow_to_sample_mappingå‚æ•°æ¥æ˜ å°„åˆ‡ç‰‡ç‰‡IDåˆ°åŸå§‹IDã€‚</span></span><br><span class="line">    <span class="comment"># æ¯”å¦‚æœ‰2ä¸ªexpamplesè¢«åˆ‡æˆ4ç‰‡ï¼Œé‚£ä¹ˆå¯¹åº”æ˜¯[0, 0, 1, 1]ï¼Œå‰ä¸¤ç‰‡å¯¹åº”åŸæ¥çš„ç¬¬ä¸€ä¸ªexampleã€‚</span></span><br><span class="line">    sample_mapping = tokenized_examples.pop(<span class="string">&quot;overflow_to_sample_mapping&quot;</span>)</span><br><span class="line">    <span class="comment"># offset_mappingä¹Ÿå¯¹åº”4ç‰‡</span></span><br><span class="line">    <span class="comment"># offset_mappingå‚æ•°å¸®åŠ©æˆ‘ä»¬æ˜ å°„åˆ°åŸå§‹è¾“å…¥ï¼Œç”±äºç­”æ¡ˆæ ‡æ³¨åœ¨åŸå§‹è¾“å…¥ä¸Šï¼Œæ‰€ä»¥æœ‰åŠ©äºæˆ‘ä»¬æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚</span></span><br><span class="line">    offset_mapping = tokenized_examples.pop(<span class="string">&quot;offset_mapping&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># é‡æ–°æ ‡æ³¨æ•°æ®</span></span><br><span class="line">    tokenized_examples[<span class="string">&quot;start_positions&quot;</span>] = []</span><br><span class="line">    tokenized_examples[<span class="string">&quot;end_positions&quot;</span>] = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, offsets <span class="keyword">in</span> <span class="built_in">enumerate</span>(offset_mapping):</span><br><span class="line">        <span class="comment"># å¯¹æ¯ä¸€ç‰‡è¿›è¡Œå¤„ç†</span></span><br><span class="line">        <span class="comment"># å°†æ— ç­”æ¡ˆçš„æ ·æœ¬æ ‡æ³¨åˆ°CLSä¸Š</span></span><br><span class="line">        input_ids = tokenized_examples[<span class="string">&quot;input_ids&quot;</span>][i]</span><br><span class="line">        cls_index = input_ids.index(tokenizer.cls_token_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># åŒºåˆ†questionå’Œcontext</span></span><br><span class="line">        sequence_ids = tokenized_examples.sequence_ids(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># æ‹¿åˆ°åŸå§‹çš„example ä¸‹æ ‡.</span></span><br><span class="line">        sample_index = sample_mapping[i]</span><br><span class="line">        answers = examples[<span class="string">&quot;answers&quot;</span>][sample_index]</span><br><span class="line">        <span class="comment"># å¦‚æœæ²¡æœ‰ç­”æ¡ˆï¼Œåˆ™ä½¿ç”¨CLSæ‰€åœ¨çš„ä½ç½®ä¸ºç­”æ¡ˆ.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(answers[<span class="string">&quot;answer_start&quot;</span>]) == <span class="number">0</span>:</span><br><span class="line">            tokenized_examples[<span class="string">&quot;start_positions&quot;</span>].append(cls_index)</span><br><span class="line">            tokenized_examples[<span class="string">&quot;end_positions&quot;</span>].append(cls_index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ç­”æ¡ˆçš„characterçº§åˆ«Start/endä½ç½®.</span></span><br><span class="line">            start_char = answers[<span class="string">&quot;answer_start&quot;</span>][<span class="number">0</span>]</span><br><span class="line">            end_char = start_char + <span class="built_in">len</span>(answers[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># æ‰¾åˆ°tokençº§åˆ«çš„index start.</span></span><br><span class="line">            token_start_index = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> sequence_ids[token_start_index] != (<span class="number">1</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="number">0</span>):</span><br><span class="line">                token_start_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># æ‰¾åˆ°tokençº§åˆ«çš„index end.</span></span><br><span class="line">            token_end_index = <span class="built_in">len</span>(input_ids) - <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> sequence_ids[token_end_index] != (<span class="number">1</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="number">0</span>):</span><br><span class="line">                token_end_index -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºæ–‡æœ¬é•¿åº¦ï¼Œè¶…å‡ºçš„è¯ä¹Ÿä½¿ç”¨CLS indexä½œä¸ºæ ‡æ³¨.</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (offsets[token_start_index][<span class="number">0</span>] &lt;= start_char <span class="keyword">and</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char):</span><br><span class="line">                tokenized_examples[<span class="string">&quot;start_positions&quot;</span>].append(cls_index)</span><br><span class="line">                tokenized_examples[<span class="string">&quot;end_positions&quot;</span>].append(cls_index)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># å¦‚æœä¸è¶…å‡ºåˆ™æ‰¾åˆ°ç­”æ¡ˆtokençš„startå’Œendä½ç½®ã€‚.</span></span><br><span class="line">                <span class="comment"># Note: we could go after the last offset if the answer is the last word (edge case).</span></span><br><span class="line">                <span class="keyword">while</span> token_start_index &lt; <span class="built_in">len</span>(offsets) <span class="keyword">and</span> offsets[token_start_index][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">                    token_start_index += <span class="number">1</span></span><br><span class="line">                tokenized_examples[<span class="string">&quot;start_positions&quot;</span>].append(token_start_index - <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">while</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">                    token_end_index -= <span class="number">1</span></span><br><span class="line">                tokenized_examples[<span class="string">&quot;end_positions&quot;</span>].append(token_end_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenized_examples</span><br></pre></td></tr></table></figure>
<p>ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚</p>
<p>æ¥ä¸‹æ¥<strong>ä½¿ç”¨mapå‡½æ•°</strong>å¯¹æ•°æ®é›†<strong>datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œ</strong>å°†é¢„å¤„ç†å‡½æ•°<code>prepare_train_features</code>åº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚å‚æ•°<code>batched=True</code>å¯ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = datasets.<span class="built_in">map</span>(prepare_train_features, batched=<span class="literal">True</span>, remove_columns=datasets[<span class="string">&quot;train&quot;</span>].column_names)</span><br></pre></td></tr></table></figure>
<h2 id="å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹">å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹</h2>
<p>æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<h3 id="åŠ è½½é¢„è®­ç»ƒæ¨¡å‹">åŠ è½½é¢„è®­ç»ƒæ¨¡å‹</h3>
<p>åš<strong>æœºå™¨é—®ç­”ä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨<code>AutoModelForQuestionAnswering</code> è¿™ä¸ªç±»</strong>ã€‚</p>
<p>å’Œä¹‹å‰å‡ ç¯‡åšå®¢æåˆ°çš„åŠ è½½æ–¹å¼ç›¸åŒä¸å†èµ˜è¿°ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForQuestionAnswering</span><br><span class="line">model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
<h3 id="è®¾å®šè®­ç»ƒå‚æ•°">è®¾å®šè®­ç»ƒå‚æ•°</h3>
<p>ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª<code>Trainer</code>è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦<strong>è®­ç»ƒçš„è®¾å®š/å‚æ•° <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments"><code>TrainingArguments</code></a>ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§</strong>ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">f&quot;test-squad&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>, <span class="comment">#å­¦ä¹ ç‡</span></span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>, <span class="comment"># è®­ç»ƒçš„æ¬¡æ•°</span></span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="æ•°æ®æ”¶é›†å™¨data-collator">æ•°æ®æ”¶é›†å™¨data collator</h3>
<p>æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰<code>Trainer</code>å¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚</p>
<p>æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªdefault_data_collatorå°†é¢„å¤„ç†å¥½çš„æ•°æ®å–‚ç»™æ¨¡å‹ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> default_data_collator</span><br><span class="line">data_collator = default_data_collator</span><br></pre></td></tr></table></figure>
<h3 id="å®šä¹‰è¯„ä¼°æ–¹æ³•">å®šä¹‰è¯„ä¼°æ–¹æ³•</h3>
<p>æ³¨æ„ï¼Œæœ¬æ¬¡è®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬å°†åªä¼šè®¡ç®—lossï¼Œæš‚æ—¶ä¸å®šä¹‰è¯„ä¼°æ–¹æ³•ã€‚</p>
<h2 id="å¼€å§‹è®­ç»ƒ">å¼€å§‹è®­ç»ƒ</h2>
<p>å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥<code>Trainer</code>å³å¯ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>è°ƒç”¨<code>train</code>æ–¹æ³•å¼€å§‹è®­ç»ƒï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line"><span class="comment">#åŠæ—¶ä¿å­˜æ¨¡å‹</span></span><br><span class="line">trainer.save_model(<span class="string">&quot;test-squad-trained&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="æ¨¡å‹è¯„ä¼°">æ¨¡å‹è¯„ä¼°</h2>
<p>æ¨¡å‹çš„è¾“å‡ºæ˜¯answeræ‰€åœ¨start/endä½ç½®çš„logitsã€‚</p>
<p>ç”¨ç¬¬ä¸€ä¸ªbatchæ¥ä¸¾ä¸€ä¸ªä¾‹å­ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> trainer.get_eval_dataloader(): <span class="comment">#äº§ç”Ÿbatchçš„è¿­ä»£å™¨</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">batch = &#123;k: v.to(trainer.args.device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = trainer.model(**batch)</span><br><span class="line">output.keys()</span><br><span class="line"><span class="comment">#odict_keys([&#x27;loss&#x27;, &#x27;start_logits&#x27;, &#x27;end_logits&#x27;])</span></span><br></pre></td></tr></table></figure>
<p>è¿˜è®°å¾—æˆ‘ä»¬åœ¨åˆ†æ<a href="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%888%EF%BC%89BERT-based%20Model%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/">BERT-based Modelæºç </a>æ—¶ï¼Œä¹Ÿå¯ä»¥çœ‹å‡ºBertForQuestionAnsweringçš„è¾“å‡ºåŒ…æ‹¬ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">return</span> QuestionAnsweringModelOutput(</span><br><span class="line">            loss=total_loss,</span><br><span class="line">            start_logits=start_logits,</span><br><span class="line">            end_logits=end_logits,</span><br><span class="line">            hidden_states=outputs.hidden_states,</span><br><span class="line">            attentions=outputs.attentions,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬åœ¨è¾“å‡ºé¢„æµ‹ç»“æœçš„æ—¶å€™ä¸éœ€è¦çœ‹lossï¼Œæ¯ä¸ªfeatureï¼ˆåˆ‡ç‰‡ï¼‰é‡Œçš„æ¯ä¸ªtokenéƒ½ä¼šæœ‰ä¸¤ä¸ªlogitå€¼ï¼ˆåˆ†åˆ«ç»„æˆstart_logitsï¼Œend_logitsï¼‰ï¼Œç›´æ¥æ ¹æ®logitsæ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®å³å¯ã€‚</p>
<h3 id="å¦‚ä½•æ ¹æ®logitsæ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®">å¦‚ä½•æ ¹æ®logitsæ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®ï¼Ÿ</h3>
<h4 id="æ–¹æ³•1">æ–¹æ³•1</h4>
<p>é¢„æµ‹answeræœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯é€‰æ‹©start_logitsé‡Œæœ€å¤§çš„ä¸‹æ ‡ä½œä¸ºanswerèµ·å§‹ä½ç½®ï¼Œend_logitsé‡Œæœ€å¤§ä¸‹æ ‡ä½œä¸ºanswerçš„ç»“æŸä½ç½®ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output.start_logits.shape, output.end_logits.shape</span><br><span class="line"><span class="comment">#(torch.Size([16, 384]), torch.Size([16, 384]))</span></span><br><span class="line"></span><br><span class="line">output.start_logits.argmax(dim=-<span class="number">1</span>), output.end_logits.argmax(dim=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># (tensor([ 46,  57,  78,  43, 118,  15,  72,  35,  15,  34,  73,  41,  80,  91, 156,  35], device=&#x27;cuda:0&#x27;),tensor([ 47,  58,  81,  55, 118, 110,  75,  37, 110,  36,  76,  53,  83,  94, 158,  35], device=&#x27;cuda:0&#x27;))</span></span><br></pre></td></tr></table></figure>
<p>è¯¥ç­–ç•¥å¤§éƒ¨åˆ†æƒ…å†µä¸‹éƒ½æ˜¯ä¸é”™çš„ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥å‘Šè¯‰æˆ‘ä»¬æ‰¾ä¸åˆ°ç­”æ¡ˆï¼šæ¯”å¦‚startçš„ä½ç½®æ¯”endçš„ä½ç½®ä¸‹æ ‡å¤§ï¼Œæˆ–è€…startå’Œendçš„ä½ç½®æŒ‡å‘äº†questionã€‚è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ</p>
<h4 id="æ–¹æ³•2">æ–¹æ³•2</h4>
<p>è¿™ä¸ªæ—¶å€™ï¼Œç®€å•çš„æ–¹æ³•æ˜¯ç»§ç»­éœ€è¦é€‰æ‹©ç¬¬2å¥½çš„é¢„æµ‹ä½œä¸ºç­”æ¡ˆï¼Œå®åœ¨ä¸è¡Œçœ‹ç¬¬3å¥½çš„é¢„æµ‹ï¼Œä»¥æ­¤ç±»æ¨ã€‚</p>
<p>ä½†æ˜¯æ–¹æ³•2ä¸å¤ªå®¹æ˜“æ‰¾åˆ°å¯è¡Œçš„ç­”æ¡ˆï¼Œè¿˜æœ‰æ²¡æœ‰æ›´åˆç†ä¸€äº›çš„æ–¹æ³•å‘¢ï¼Ÿ</p>
<h4 id="æ–¹æ³•3">æ–¹æ³•3</h4>
<p>åˆ†ä¸ºå››ä¸ªæ­¥éª¤ï¼š</p>
<ol type="1">
<li>æˆ‘ä»¬å…ˆæ‰¾åˆ°æœ€å¥½çš„<code>n_best_size</code>ä¸ªï¼ˆè‡ªå®šä¹‰ï¼‰startå’Œendå¯¹åº”çš„å¯èƒ½çš„å¤‡é€‰èµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹ï¼›</li>
<li>ä»ä¸­å…ˆæ„å»ºåˆç†çš„å¤‡é€‰ç­”æ¡ˆï¼Œ<strong>ä¸åˆç†çš„æƒ…å†µ</strong>åŒ…æ‹¬ä»¥ä¸‹å‡ ç§ï¼š
<ol type="1">
<li>start&gt;endçš„å¤‡é€‰èµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹ï¼›</li>
<li>startæˆ–endè¶…è¿‡æœ€å¤§é•¿åº¦ï¼›</li>
<li>startå’Œendä½ç½®å¯¹åº”çš„æ–‡æœ¬åœ¨questioné‡Œé¢è€Œä¸åœ¨contexté‡Œé¢ï¼ˆè¿™é‡ŒåŸ‹äº†ä¸€ä¸ªé›·ï¼Œï¼‰ï¼›</li>
</ol></li>
<li>ç„¶åå°†åˆç†å¤‡é€‰ç­”æ¡ˆçš„startå’Œendçš„logitsç›¸åŠ å¾—åˆ°æ–°çš„æ‰“åˆ†ï¼›</li>
<li>æœ€åæˆ‘ä»¬æ ¹æ®<code>score</code>å¯¹<code>valid_answers</code>è¿›è¡Œæ’åºï¼Œæ‰¾åˆ°æœ€å¥½çš„é‚£ä¸€ä¸ªåšä¸ºç­”æ¡ˆã€‚</li>
</ol>
<p><strong>ä¸ºäº†æ‰¾åˆ°ç¬¬3ç§ä¸åˆç†çš„æƒ…å†µï¼Œæˆ‘ä»¬åœ¨validationçš„featuresé‡Œæ·»åŠ ä»¥ä¸‹ä¸¤ä¸ªä¿¡æ¯</strong>ï¼š</p>
<ol type="1">
<li>äº§ç”Ÿfeatureçš„example ID-<code>overflow_to_sample_mapping</code>ï¼šç”±äºæ¯ä¸ªexampleå¯èƒ½ä¼šäº§ç”Ÿå¤šä¸ªfeatureï¼Œæ‰€ä»¥æ¯ä¸ªfeature/åˆ‡ç‰‡çš„featureéœ€è¦çŸ¥é“ä»–ä»¬å¯¹åº”çš„exampleæ˜¯å“ªä¸€ä¸ªã€‚</li>
<li>offset mapping-<code>offset_mapping</code>ï¼š å°†æ¯ä¸ªåˆ‡ç‰‡tokensçš„ä½ç½®æ˜ å°„å›åŸå§‹æ–‡æœ¬åŸºäºcharacterçš„ä¸‹æ ‡ä½ç½®ï¼ŒæŠŠquestionéƒ¨åˆ†çš„offset_mappingç”¨Noneæ©ç ï¼Œcontextéƒ¨åˆ†ä¿ç•™ä¸å˜ã€‚</li>
</ol>
<p>æˆ‘ä»¬ç°åœ¨<strong>åˆ©ç”¨ä¸€ä¸ª<code>prepare_validation_features</code>å‡½æ•°å¤„ç†validationéªŒè¯é›†</strong>ï¼Œæ·»åŠ ä¸Šé¢ä¸¤ä¸ªä¿¡æ¯ï¼Œè¯¥å‡½æ•°å’Œå¤„ç†è®­ç»ƒçš„æ—¶å€™çš„<code>prepare_train_features</code>ç¨æœ‰ä¸åŒã€‚ç„¶ååˆ©ç”¨å¤„ç†åçš„éªŒè¯é›†è¿›è¡Œè¯„ä¼°ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_validation_features</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="comment"># Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results</span></span><br><span class="line">    <span class="comment"># in one example possible giving several features when a context is long, each of those features having a</span></span><br><span class="line">    <span class="comment"># context that overlaps a bit the context of the previous feature.</span></span><br><span class="line">    tokenized_examples = tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;question&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;context&quot;</span>],</span><br><span class="line">        examples[<span class="string">&quot;context&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;question&quot;</span>],</span><br><span class="line">        truncation=<span class="string">&quot;only_second&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;only_first&quot;</span>,</span><br><span class="line">        max_length=max_length,</span><br><span class="line">        stride=doc_stride,</span><br><span class="line">        return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">        return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since one example might give us several features if it has a long context, we need a map from a feature to</span></span><br><span class="line">    <span class="comment"># its corresponding example. This key gives us just that.</span></span><br><span class="line">    <span class="comment"># æˆ‘ä»¬ä½¿ç”¨overflow_to_sample_mappingå‚æ•°æ¥æ˜ å°„åˆ‡ç‰‡ç‰‡IDåˆ°åŸå§‹IDã€‚</span></span><br><span class="line">    <span class="comment"># æ¯”å¦‚æœ‰2ä¸ªexpamplesè¢«åˆ‡æˆ4ç‰‡ï¼Œé‚£ä¹ˆå¯¹åº”æ˜¯[0, 0, 1, 1]ï¼Œå‰ä¸¤ç‰‡å¯¹åº”åŸæ¥çš„ç¬¬ä¸€ä¸ªexampleã€‚</span></span><br><span class="line">    sample_mapping = tokenized_examples.pop(<span class="string">&quot;overflow_to_sample_mapping&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We keep the example_id that gave us this feature and we will store the offset mappings.</span></span><br><span class="line">    tokenized_examples[<span class="string">&quot;example_id&quot;</span>] = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tokenized_examples[<span class="string">&quot;input_ids&quot;</span>])):</span><br><span class="line">        <span class="comment"># Grab the sequence corresponding to that example (to know what is the context and what is the question).</span></span><br><span class="line">        sequence_ids = tokenized_examples.sequence_ids(i)</span><br><span class="line">        context_index = <span class="number">1</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># One example can give several spans, this is the index of the example containing this span of text.</span></span><br><span class="line">        <span class="comment"># æ‹¿åˆ°åŸå§‹çš„example ä¸‹æ ‡.</span></span><br><span class="line">        sample_index = sample_mapping[i]</span><br><span class="line">        tokenized_examples[<span class="string">&quot;example_id&quot;</span>].append(examples[<span class="string">&quot;id&quot;</span>][sample_index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set to None the offset_mapping that are not part of the context so it&#x27;s easy to determine if a token</span></span><br><span class="line">        <span class="comment"># position is part of the context or not.</span></span><br><span class="line">        <span class="comment"># æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨contextä¸­ï¼Œå¦‚æœä¸åœ¨offset_mappingä¸ºNone</span></span><br><span class="line">        <span class="comment"># å…¶å®å°±æ˜¯æŠŠquestionéƒ¨åˆ†çš„offset_mappingç”¨Noneæ©ç ï¼Œcontextéƒ¨åˆ†ä¿ç•™ä¸å˜</span></span><br><span class="line">        tokenized_examples[<span class="string">&quot;offset_mapping&quot;</span>][i] = [</span><br><span class="line">            (o <span class="keyword">if</span> sequence_ids[k] == context_index <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">for</span> k, o <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokenized_examples[<span class="string">&quot;offset_mapping&quot;</span>][i])</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenized_examples</span><br></pre></td></tr></table></figure>
<p>å’Œä¹‹å‰ä¸€æ ·å°†<code>prepare_validation_features</code>å‡½æ•°åº”ç”¨åˆ°æ¯ä¸ªéªŒè¯é›†åˆçš„æ ·æœ¬ä¸Šã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validation_features = datasets[<span class="string">&quot;validation&quot;</span>].<span class="built_in">map</span>(</span><br><span class="line">    prepare_validation_features,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=datasets[<span class="string">&quot;validation&quot;</span>].column_names</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>ä½¿ç”¨<code>Trainer.predict</code>æ–¹æ³•è·å¾—æ‰€æœ‰é¢„æµ‹ç»“æœï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_predictions = trainer.predict(validation_features)</span><br></pre></td></tr></table></figure>
<p><strong>è¿™ä¸ª <code>Trainer</code> éšè—äº† ä¸€äº›æ¨¡å‹è®­ç»ƒæ—¶å€™æ²¡æœ‰ä½¿ç”¨çš„å±æ€§(è¿™é‡Œæ˜¯ <code>example_id</code>å’Œ<code>offset_mapping</code>ï¼Œåå¤„ç†çš„æ—¶å€™ä¼šç”¨åˆ°)ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠè¿™äº›è®¾ç½®å›æ¥:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validation_features.set_format(<span class="built_in">type</span>=validation_features.<span class="built_in">format</span>[<span class="string">&quot;type&quot;</span>], columns=<span class="built_in">list</span>(validation_features.features.keys()))</span><br></pre></td></tr></table></figure>
<p>ç»è¿‡å‰é¢çš„<code>prepare_validation_features</code>å‡½æ•°å¤„ç†ï¼Œ<strong>å½“ä¸€ä¸ªtokenä½ç½®å¯¹åº”questionéƒ¨åˆ†offset mappingsä¸º<code>None</code>ï¼Œ</strong>æ‰€ä»¥æˆ‘ä»¬æ ¹æ®offset mappingå¯ä»¥åˆ¤æ–­tokenæ˜¯å¦åœ¨contexté‡Œé¢ã€‚</p>
<p>æ›´è¿‘ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬ç”¨<code>max_answer_length</code>æ§åˆ¶å»æ‰ç‰¹åˆ«é•¿çš„ç­”æ¡ˆã€‚</p>
<p>ä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_best_size = <span class="number">20</span></span><br><span class="line">max_answer_length = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">start_logits = output.start_logits[<span class="number">0</span>].cpu().numpy()</span><br><span class="line">end_logits = output.end_logits[<span class="number">0</span>].cpu().numpy()</span><br><span class="line">offset_mapping = validation_features[<span class="number">0</span>][<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"><span class="comment"># The first feature comes from the first example. For the more general case, we will need to be match the example_id to</span></span><br><span class="line"><span class="comment"># an example index</span></span><br><span class="line">context = datasets[<span class="string">&quot;validation&quot;</span>][<span class="number">0</span>][<span class="string">&quot;context&quot;</span>]</span><br><span class="line"><span class="comment"># æ”¶é›†æœ€ä½³çš„startå’Œend logitsçš„ä½ç½®</span></span><br><span class="line"><span class="comment"># Gather the indices the best start/end logits:</span></span><br><span class="line">start_indexes = np.argsort(start_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">end_indexes = np.argsort(end_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">valid_answers = []</span><br><span class="line"><span class="keyword">for</span> start_index <span class="keyword">in</span> start_indexes:</span><br><span class="line">    <span class="keyword">for</span> end_index <span class="keyword">in</span> end_indexes:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Don&#x27;t consider out-of-scope answers, either because the indices are out of bounds or correspond</span></span><br><span class="line">        <span class="comment"># to part of the input_ids that are not in the context.</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="comment">#ç­”æ¡ˆä¸åˆç†</span></span><br><span class="line">            start_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">            <span class="keyword">or</span> end_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">            <span class="keyword">or</span> offset_mapping[start_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">or</span> offset_mapping[end_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">        ):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># Don&#x27;t consider answers with a length that is either &lt; 0 or &gt; max_answer_length.</span></span><br><span class="line">        <span class="keyword">if</span> end_index &lt; start_index <span class="keyword">or</span> end_index - start_index + <span class="number">1</span> &gt; max_answer_length:<span class="comment">#ç­”æ¡ˆä¸åˆç†</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> start_index &lt;= end_index: <span class="comment"># We need to refine that test to check the answer is inside the context # å¦‚æœstartå°äºendï¼Œé‚£ä¹ˆæ˜¯åˆç†çš„å¯èƒ½ç­”æ¡ˆ</span></span><br><span class="line">            start_char = offset_mapping[start_index][<span class="number">0</span>]</span><br><span class="line">            end_char = offset_mapping[end_index][<span class="number">1</span>]</span><br><span class="line">            valid_answers.append(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;score&quot;</span>: start_logits[start_index] + end_logits[end_index],</span><br><span class="line">                    <span class="string">&quot;text&quot;</span>: context[start_char: end_char]<span class="comment"># åç»­éœ€è¦æ ¹æ®tokençš„ä¸‹æ ‡å°†ç­”æ¡ˆæ‰¾å‡ºæ¥</span></span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line"><span class="comment">#æœ€åæ ¹æ®`score`å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œæ‰¾åˆ°æœ€å¥½çš„é‚£ä¸€ä¸ª</span></span><br><span class="line">valid_answers = <span class="built_in">sorted</span>(valid_answers, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;score&quot;</span>], reverse=<span class="literal">True</span>)[:n_best_size]</span><br><span class="line">valid_answers</span><br></pre></td></tr></table></figure>
<p>å°†é¢„æµ‹ç­”æ¡ˆå’ŒçœŸå®ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼ŒéªŒè¯ä¸€ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets[<span class="string">&quot;validation&quot;</span>][<span class="number">0</span>][<span class="string">&quot;answers&quot;</span>]</span><br><span class="line"><span class="comment">#&#123;&#x27;answer_start&#x27;: [177, 177, 177],</span></span><br><span class="line"><span class="comment"># &#x27;text&#x27;: [&#x27;Denver Broncos&#x27;, &#x27;Denver Broncos&#x27;, &#x27;Denver Broncos&#x27;]&#125;</span></span><br></pre></td></tr></table></figure>
<p>è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜éœ€è¦æ€è€ƒï¼š</p>
<p><strong>å½“ä¸€ä¸ªexampleè¢«åˆ†æˆå¤šä¸ªåˆ‡ç‰‡è¾“å…¥æ¨¡å‹ï¼Œæ¨¡å‹ä¼šæŠŠè¿™äº›åˆ‡ç‰‡å½“ä½œå¤šä¸ªå•ç‹¬çš„â€œæ ·æœ¬â€è¿›è¡Œè®­ç»ƒï¼Œé‚£æˆ‘ä»¬åœ¨è®¡ç®—æ­£ç¡®ç‡å’Œå¬å›ç‡çš„æ—¶å€™ï¼Œä¸èƒ½ä»¥è¿™å¤šä¸ªåˆ‡ç‰‡ä¸ºå•ä½ç›´æ¥è®¡ç®—ï¼Œè€Œæ˜¯åº”è¯¥å°†å…¶å¯¹åº”çš„ä¸€ä¸ªexampleä¸ºå•ä½è¿›è¡Œè®¡ç®—ã€‚</strong></p>
<p>å¯¹äºä¸Šé¢åœ°ä¾‹å­æ¥è¯´ï¼Œç”±äºç­”æ¡ˆæ­£å¥½åœ¨ç¬¬1ä¸ªfeatureï¼Œè€Œç¬¬1ä¸ªfeatureä¸€å®šæ˜¯æ¥è‡ªäºç¬¬1ä¸ªexampleï¼Œæ‰€ä»¥ç›¸å¯¹å®¹æ˜“ã€‚<strong>å¯¹äºä¸€ä¸ªè¶…é•¿exampleäº§ç”Ÿçš„å…¶ä»–fearuresæ¥è¯´ï¼Œéœ€è¦ä¸€ä¸ªfeatureså’Œexamplesè¿›è¡Œæ˜ å°„çš„mapã€‚å› æ­¤ç”±äºä¸€ä¸ªexampleå¯èƒ½è¢«åˆ‡ç‰‡æˆå¤šä¸ªfeaturesï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰featuresé‡Œçš„ç­”æ¡ˆå…¨éƒ¨æ”¶é›†èµ·æ¥ã€‚</strong></p>
<p>ä»¥ä¸‹çš„ä»£ç å°±å°†exmapleçš„ä¸‹æ ‡å’Œfeaturesçš„ä¸‹æ ‡è¿›è¡Œmapæ˜ å°„ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line">examples = datasets[<span class="string">&quot;validation&quot;</span>]</span><br><span class="line">features = validation_features</span><br><span class="line"></span><br><span class="line">example_id_to_index = &#123;k: i <span class="keyword">for</span> i, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples[<span class="string">&quot;id&quot;</span>])&#125;</span><br><span class="line">features_per_example = collections.defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">    features_per_example[example_id_to_index[feature[<span class="string">&quot;example_id&quot;</span>]]].append(i)</span><br></pre></td></tr></table></figure>
<p>å¯¹äºåå¤„ç†è¿‡ç¨‹åŸºæœ¬ä¸Šå·²ç»å…¨éƒ¨å®Œæˆäº†ã€‚</p>
<p>ä½†æ˜¯è¿™é‡Œè¿˜è¿˜è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼š<strong>å¦‚ä½•è§£å†³æ— ç­”æ¡ˆçš„æƒ…å†µï¼ˆsquad_v2=Trueï¼‰</strong>ã€‚</p>
<p>ä»¥ä¸Šçš„ä»£ç éƒ½åªè€ƒè™‘äº†contexté‡Œé¢çš„asnwersï¼Œæˆ‘ä»¬åŒæ ·éœ€è¦å°†æ— ç­”æ¡ˆçš„é¢„æµ‹å¾—åˆ†è¿›è¡Œæœé›†ï¼ˆæ— ç­”æ¡ˆçš„é¢„æµ‹å¯¹åº”äº†CLS tokençš„startå’Œend logitsï¼‰ã€‚å¦‚æœä¸€ä¸ªexampleæ ·æœ¬æœ‰å¤šä¸ªfeaturesï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿˜éœ€è¦åœ¨å¤šä¸ªfeaturesé‡Œé¢„æµ‹æ˜¯ä¸æ˜¯éƒ½æ— ç­”æ¡ˆã€‚æ‰€ä»¥<strong>æ— ç­”æ¡ˆçš„æœ€ç»ˆå¾—åˆ†æ˜¯æ‰€æœ‰featuresçš„æ— ç­”æ¡ˆå¾—åˆ†æœ€å°çš„é‚£ä¸ªã€‚</strong>ï¼ˆä¸º<strong>ä»€ä¹ˆæ˜¯æœ€å°çš„é‚£ä¸ªå‘¢ï¼Ÿå› ä¸ºç­”æ¡ˆå¦‚æœåªåœ¨ä¸€ä¸ªåˆ‡ç‰‡é‡Œï¼Œå…¶ä»–åˆ‡ç‰‡è‚¯å®šæ˜¯æ²¡æœ‰ç­”æ¡ˆçš„ï¼Œå¦‚æœè¦ç¡®ä¿æ•´ä¸ªexampleæ˜¯æ²¡æœ‰ç­”æ¡ˆçš„è¯ï¼Œç›¸å½“äºæœ€æœ‰å¯èƒ½æœ‰ç­”æ¡ˆçš„åˆ‡ç‰‡é‡Œé¢ä¹Ÿæ²¡æœ‰ç­”æ¡ˆ</strong>ï¼‰<strong>åªè¦æ— ç­”æ¡ˆçš„æœ€ç»ˆå¾—åˆ†é«˜äºå…¶ä»–æ‰€æœ‰ç­”æ¡ˆçš„å¾—åˆ†ï¼Œé‚£ä¹ˆè¯¥é—®é¢˜å°±æ˜¯æ— ç­”æ¡ˆã€‚</strong></p>
<p>æœ€ç»ˆçš„åå¤„ç†å‡½æ•°ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess_qa_predictions</span>(<span class="params">examples, features, raw_predictions, n_best_size = <span class="number">20</span>, max_answer_length = <span class="number">30</span></span>):</span></span><br><span class="line">    all_start_logits, all_end_logits = raw_predictions</span><br><span class="line">    <span class="comment"># Build a map example to its corresponding features.</span></span><br><span class="line">    example_id_to_index = &#123;k: i <span class="keyword">for</span> i, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples[<span class="string">&quot;id&quot;</span>])&#125;</span><br><span class="line">    features_per_example = collections.defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">        features_per_example[example_id_to_index[feature[<span class="string">&quot;example_id&quot;</span>]]].append(i)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The dictionaries we have to fill.</span></span><br><span class="line">    predictions = collections.OrderedDict()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logging.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Post-processing <span class="subst">&#123;<span class="built_in">len</span>(examples)&#125;</span> example predictions split into <span class="subst">&#123;<span class="built_in">len</span>(features)&#125;</span> features.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Let&#x27;s loop over all the examples!</span></span><br><span class="line">    <span class="keyword">for</span> example_index, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(examples)):</span><br><span class="line">        <span class="comment"># Those are the indices of the features associated to the current example.</span></span><br><span class="line">        feature_indices = features_per_example[example_index]</span><br><span class="line"></span><br><span class="line">        min_null_score = <span class="literal">None</span> <span class="comment"># Only used if squad_v2 is True.</span></span><br><span class="line">        valid_answers = []</span><br><span class="line">        </span><br><span class="line">        context = example[<span class="string">&quot;context&quot;</span>]</span><br><span class="line">        <span class="comment"># Looping through all the features associated to the current example.</span></span><br><span class="line">        <span class="keyword">for</span> feature_index <span class="keyword">in</span> feature_indices:</span><br><span class="line">            <span class="comment"># We grab the predictions of the model for this feature.</span></span><br><span class="line">            start_logits = all_start_logits[feature_index]</span><br><span class="line">            end_logits = all_end_logits[feature_index]</span><br><span class="line">            <span class="comment"># This is what will allow us to map some the positions in our logits to span of texts in the original</span></span><br><span class="line">            <span class="comment"># context.</span></span><br><span class="line">            offset_mapping = features[feature_index][<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update minimum null prediction.</span></span><br><span class="line">            cls_index = features[feature_index][<span class="string">&quot;input_ids&quot;</span>].index(tokenizer.cls_token_id)</span><br><span class="line">            feature_null_score = start_logits[cls_index] + end_logits[cls_index]</span><br><span class="line">            <span class="keyword">if</span> min_null_score <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> min_null_score &lt; feature_null_score:</span><br><span class="line">                min_null_score = feature_null_score</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Go through all possibilities for the `n_best_size` greater start and end logits.</span></span><br><span class="line">            start_indexes = np.argsort(start_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">            end_indexes = np.argsort(end_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">            <span class="keyword">for</span> start_index <span class="keyword">in</span> start_indexes:</span><br><span class="line">                <span class="keyword">for</span> end_index <span class="keyword">in</span> end_indexes:</span><br><span class="line">                    <span class="comment"># Don&#x27;t consider out-of-scope answers, either because the indices are out of bounds or correspond</span></span><br><span class="line">                    <span class="comment"># to part of the input_ids that are not in the context.</span></span><br><span class="line">                    <span class="keyword">if</span> (</span><br><span class="line">                        start_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">                        <span class="keyword">or</span> end_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">                        <span class="keyword">or</span> offset_mapping[start_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                        <span class="keyword">or</span> offset_mapping[end_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                    ):</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment"># Don&#x27;t consider answers with a length that is either &lt; 0 or &gt; max_answer_length.</span></span><br><span class="line">                    <span class="keyword">if</span> end_index &lt; start_index <span class="keyword">or</span> end_index - start_index + <span class="number">1</span> &gt; max_answer_length:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                    start_char = offset_mapping[start_index][<span class="number">0</span>]</span><br><span class="line">                    end_char = offset_mapping[end_index][<span class="number">1</span>]</span><br><span class="line">                    valid_answers.append(</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">&quot;score&quot;</span>: start_logits[start_index] + end_logits[end_index],</span><br><span class="line">                            <span class="string">&quot;text&quot;</span>: context[start_char: end_char]</span><br><span class="line">                        &#125;</span><br><span class="line">                    )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(valid_answers) &gt; <span class="number">0</span>:</span><br><span class="line">            best_answer = <span class="built_in">sorted</span>(valid_answers, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;score&quot;</span>], reverse=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid</span></span><br><span class="line">            <span class="comment"># failure.</span></span><br><span class="line">            best_answer = &#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;&quot;</span>, <span class="string">&quot;score&quot;</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Let&#x27;s pick our final answer: the best one or the null answer (only for squad_v2)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> squad_v2:</span><br><span class="line">            predictions[example[<span class="string">&quot;id&quot;</span>]] = best_answer[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            answer = best_answer[<span class="string">&quot;text&quot;</span>] <span class="keyword">if</span> best_answer[<span class="string">&quot;score&quot;</span>] &gt; min_null_score <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">            predictions[example[<span class="string">&quot;id&quot;</span>]] = answer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>
<p>å°†åå¤„ç†å‡½æ•°åº”ç”¨åˆ°åŸå§‹é¢„æµ‹è¾“å‡ºä¸Šï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">final_predictions = postprocess_qa_predictions(datasets[<span class="string">&quot;validation&quot;</span>], validation_features, raw_predictions.predictions)</span><br></pre></td></tr></table></figure>
<p>ç„¶ååŠ è½½è¯„æµ‹æŒ‡æ ‡ï¼š</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line">metric = load_metric(<span class="string">&quot;squad_v2&quot;</span> <span class="keyword">if</span> squad_v2 <span class="keyword">else</span> <span class="string">&quot;squad&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>åŸºäºé¢„æµ‹å’Œæ ‡æ³¨å¯¹è¯„æµ‹æŒ‡æ ‡è¿›è¡Œè®¡ç®—ã€‚ä¸ºäº†åˆç†çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬éœ€è¦å°†é¢„æµ‹å’Œæ ‡æ³¨çš„æ ¼å¼ã€‚å¯¹äºsquad2æ¥è¯´ï¼Œè¯„æµ‹æŒ‡æ ‡è¿˜éœ€è¦<code>no_answer_probability</code>å‚æ•°ï¼ˆç”±äºå·²ç»æ— ç­”æ¡ˆç›´æ¥è®¾ç½®æˆäº†ç©ºå­—ç¬¦ä¸²ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥å°†è¿™ä¸ªå‚æ•°è®¾ç½®ä¸º0.0ï¼‰</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> squad_v2:</span><br><span class="line">    formatted_predictions = [&#123;<span class="string">&quot;id&quot;</span>: k, <span class="string">&quot;prediction_text&quot;</span>: v, <span class="string">&quot;no_answer_probability&quot;</span>: <span class="number">0.0</span>&#125; <span class="keyword">for</span> k, v <span class="keyword">in</span> predictions.items()]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    formatted_predictions = [&#123;<span class="string">&quot;id&quot;</span>: k, <span class="string">&quot;prediction_text&quot;</span>: v&#125; <span class="keyword">for</span> k, v <span class="keyword">in</span> final_predictions.items()]</span><br><span class="line">references = [&#123;<span class="string">&quot;id&quot;</span>: ex[<span class="string">&quot;id&quot;</span>], <span class="string">&quot;answers&quot;</span>: ex[<span class="string">&quot;answers&quot;</span>]&#125; <span class="keyword">for</span> ex <span class="keyword">in</span> datasets[<span class="string">&quot;validation&quot;</span>]]</span><br><span class="line">metric.compute(predictions=formatted_predictions, references=references)</span><br></pre></td></tr></table></figure>
<p>åˆ°è¿™é‡Œæ‰ç»“æŸå•¦ï¼</p>
<h2 id="å‚è€ƒæ–‡çŒ®">å‚è€ƒæ–‡çŒ®</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.4-é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”.md">4.4-é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”.md</a></p>
<p><a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">BERTå®æˆ˜â€”â€”ï¼ˆ1ï¼‰æ–‡æœ¬åˆ†ç±»</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/transformers">transformerså®˜æ–¹æ–‡æ¡£</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="mailto:undefined">å†¬äº</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/">https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="https://ifwind.github.io" target="_blank">å†¬äºçš„åšå®¢</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">æ·±åº¦å­¦ä¹ </a><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a></div><div class="post_share"><div class="social-share" data-image="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/31/Transformer-BERT-%E5%AE%9E%E6%88%98/"><img class="prev-cover" src="/2021/08/31/Transformer-BERT-%E5%AE%9E%E6%88%98/BERT.jfif" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">Transformer/BERT/å®æˆ˜</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%883%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E5%A4%9A%E9%80%89%E9%97%AE%E7%AD%94/"><img class="next-cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">BERTå®æˆ˜â€”â€”ï¼ˆ3ï¼‰é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2021/08/31/BERTå®æˆ˜â€”â€”ï¼ˆ5ï¼‰ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘/" title="BERTå®æˆ˜â€”â€”ï¼ˆ5ï¼‰ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘"><img class="cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-31</div><div class="title">BERTå®æˆ˜â€”â€”ï¼ˆ5ï¼‰ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘</div></div></a></div><div><a href="/2021/08/31/BERTå®æˆ˜â€”â€”ï¼ˆ6ï¼‰ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ/" title="BERTå®æˆ˜â€”â€”ï¼ˆ6ï¼‰ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ"><img class="cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-31</div><div class="title">BERTå®æˆ˜â€”â€”ï¼ˆ6ï¼‰ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ</div></div></a></div><div><a href="/2021/08/20/BERTç›¸å…³â€”â€”ï¼ˆ1ï¼‰è¯­è¨€æ¨¡å‹/" title="BERTç›¸å…³â€”â€”ï¼ˆ1ï¼‰è¯­è¨€æ¨¡å‹"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERTç›¸å…³â€”â€”ï¼ˆ1ï¼‰è¯­è¨€æ¨¡å‹</div></div></a></div><div><a href="/2021/08/20/BERTç›¸å…³â€”â€”ï¼ˆ2ï¼‰Contextualized_Word_Embeddingå’ŒELMOæ¨¡å‹/" title="BERTç›¸å…³â€”â€”ï¼ˆ2ï¼‰Contextualized Word Embeddingå’ŒELMOæ¨¡å‹"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERTç›¸å…³â€”â€”ï¼ˆ2ï¼‰Contextualized Word Embeddingå’ŒELMOæ¨¡å‹</div></div></a></div><div><a href="/2021/08/20/BERTç›¸å…³â€”â€”ï¼ˆ3ï¼‰BERTæ¨¡å‹/" title="BERTç›¸å…³â€”â€”ï¼ˆ3ï¼‰BERTæ¨¡å‹"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERTç›¸å…³â€”â€”ï¼ˆ3ï¼‰BERTæ¨¡å‹</div></div></a></div><div><a href="/2021/08/20/BERTç›¸å…³â€”â€”ï¼ˆ4ï¼‰GPTæ¨¡å‹/" title="BERTç›¸å…³â€”â€”ï¼ˆ4ï¼‰GPT-2æ¨¡å‹"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERTç›¸å…³â€”â€”ï¼ˆ4ï¼‰GPT-2æ¨¡å‹</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> è¯„è®º</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bert%E5%AE%9E%E6%88%984%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94"><span class="toc-number">1.</span> <span class="toc-text">BERTå®æˆ˜â€”â€”ï¼ˆ4ï¼‰é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">å¼•è¨€</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.1.</span> <span class="toc-text">ä»»åŠ¡ä»‹ç»</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87"><span class="toc-number">1.1.2.</span> <span class="toc-text">å‰æœŸå‡†å¤‡</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.2.</span> <span class="toc-text">æ•°æ®åŠ è½½</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.1.</span> <span class="toc-text">æ•°æ®é›†ä»‹ç»</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">åŠ è½½æ•°æ®</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">æ•°æ®é¢„å¤„ç†</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96tokenizer"><span class="toc-number">1.3.1.</span> <span class="toc-text">åˆå§‹åŒ–Tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E5%8C%96%E6%88%90%E5%AF%B9%E5%BA%94%E4%BB%BB%E5%8A%A1%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.3.2.</span> <span class="toc-text">è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%82%A3%E4%B9%88%E9%A2%84%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E8%B6%85%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%91%A2"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">é‚£ä¹ˆé¢„è®­ç»ƒæœºå™¨é—®ç­”æ¨¡å‹æ˜¯å¦‚ä½•å¤„ç†è¶…é•¿æ–‡æœ¬çš„å‘¢ï¼Ÿ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%88%87%E7%89%87%E5%89%8D%E5%90%8E%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">è·å–åˆ‡ç‰‡å‰åçš„ä½ç½®å¯¹åº”å…³ç³»</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">åŠ è½½é¢„è®­ç»ƒæ¨¡å‹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">è®¾å®šè®­ç»ƒå‚æ•°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%99%A8data-collator"><span class="toc-number">1.4.3.</span> <span class="toc-text">æ•°æ®æ”¶é›†å™¨data collator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.4.</span> <span class="toc-text">å®šä¹‰è¯„ä¼°æ–¹æ³•</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.5.</span> <span class="toc-text">å¼€å§‹è®­ç»ƒ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">1.6.</span> <span class="toc-text">æ¨¡å‹è¯„ä¼°</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%A0%B9%E6%8D%AElogits%E6%89%BE%E5%88%B0%E7%AD%94%E6%A1%88%E7%9A%84%E4%BD%8D%E7%BD%AE"><span class="toc-number">1.6.1.</span> <span class="toc-text">å¦‚ä½•æ ¹æ®logitsæ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®ï¼Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">æ–¹æ³•1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%952"><span class="toc-number">1.6.1.2.</span> <span class="toc-text">æ–¹æ³•2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%953"><span class="toc-number">1.6.1.3.</span> <span class="toc-text">æ–¹æ³•3</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.7.</span> <span class="toc-text">å‚è€ƒæ–‡çŒ®</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By å†¬äº</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="ç›´è¾¾è¯„è®º"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
      appKey: 'YKdl4HKX9W6jLSdPlypgEtDM',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: 'https://e4lpmfet.lc-cn-n1-shared.com',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.17.0/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[å›¾ç‰‡]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[é“¾æ¥]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[ä»£ç ]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += 'æ²¡æœ‰è¯„è®º'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://e4lpmfet.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
        "X-LC-Key": 'YKdl4HKX9W6jLSdPlypgEtDM',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "æ— æ³•è·å–è¯„è®ºï¼Œè¯·ç¡®è®¤ç›¸å…³é…ç½®æ˜¯å¦æ­£ç¡®"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>