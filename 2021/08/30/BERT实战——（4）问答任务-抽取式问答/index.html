<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>BERT实战——（4）问答任务-抽取式问答 | 冬于的博客</title><meta name="keywords" content="深度学习,NLP,BERT"><meta name="author" content="冬于"><meta name="copyright" content="冬于"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="BERT实战——（4）问答任务-抽取式问答 引言 我们将展示如何使用 🤗 Transformers代码库中的模型来解决问答任务中的机器问答任务。 任务介绍 注意我们这里主要解决的是抽取式问答任务：给定一个问题和一段文本，从这段文本中找出能回答该问题的文本片段（span）。抽取式问答任务是从文本中抽取答案，并不是直接生成答案。 比如： 输入：	问题：我家在哪里？	文本：我的家在东北。输">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT实战——（4）问答任务-抽取式问答">
<meta property="og:url" content="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/index.html">
<meta property="og:site_name" content="冬于的博客">
<meta property="og:description" content="BERT实战——（4）问答任务-抽取式问答 引言 我们将展示如何使用 🤗 Transformers代码库中的模型来解决问答任务中的机器问答任务。 任务介绍 注意我们这里主要解决的是抽取式问答任务：给定一个问题和一段文本，从这段文本中找出能回答该问题的文本片段（span）。抽取式问答任务是从文本中抽取答案，并不是直接生成答案。 比如： 输入：	问题：我家在哪里？	文本：我的家在东北。输">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg">
<meta property="article:published_time" content="2021-08-30T06:23:57.000Z">
<meta property="article:modified_time" content="2021-08-30T12:07:33.482Z">
<meta property="article:author" content="冬于">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LVK1P2D38K","apiKey":"8cbbb0bcbb5c7448f68b4fae01d4ccd5","indexName":"DongYu","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'BERT实战——（4）问答任务-抽取式问答',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-08-30 20:07:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">66</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">50</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">冬于的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">BERT实战——（4）问答任务-抽取式问答</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-30T06:23:57.000Z" title="发表于 2021-08-30 14:23:57">2021-08-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-08-30T12:07:33.482Z" title="更新于 2021-08-30 20:07:33">2021-08-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7,536</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>35分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="BERT实战——（4）问答任务-抽取式问答"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="bert实战4问答任务-抽取式问答">BERT实战——（4）问答任务-抽取式问答</h1>
<h2 id="引言">引言</h2>
<p>我们将展示如何使用 <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">🤗 Transformers</a>代码库中的模型来解决<strong>问答任务中的机器问答任务</strong>。</p>
<h3 id="任务介绍">任务介绍</h3>
<p>注意我们这里主要解决的是<strong>抽取式问答任务</strong>：给定一个问题和一段文本，从这段文本中找出能回答该问题的文本片段（span）。<strong>抽取式问答任务是从文本中抽取答案，并不是直接生成答案。</strong></p>
<p>比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line">	问题：我家在哪里？</span><br><span class="line">	文本：我的家在东北。</span><br><span class="line">输出：东北</span><br></pre></td></tr></table></figure>
<p>这在<a href="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E6%8A%8ABERT%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1/#copy%20from%20input.html">之前的博客【定位词：copy from input】</a>中有详细介绍，这里简单复习一下，就是需要对每个token进行分类，看token是不是答案文本片段的start或是end。</p>
<blockquote>
<p><img src="/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/抽取式问答.png" style="zoom:80%;"><img src="/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/抽取式问答2.png" style="zoom:80%;"></p>
</blockquote>
<p>主要分为以下几个部分：</p>
<ol type="1">
<li>数据加载</li>
<li>数据预处理</li>
<li>微调预训练模型：使用transformer中的<code>Trainer</code>接口对预训练模型进行微调；</li>
<li>模型评估。</li>
</ol>
<h3 id="前期准备">前期准备</h3>
<p>安装以下库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install datasets transformers</span><br><span class="line"><span class="comment">#transformers==4.9.2</span></span><br><span class="line"><span class="comment">#datasets==1.11.0</span></span><br></pre></td></tr></table></figure>
<h2 id="数据加载">数据加载</h2>
<h3 id="数据集介绍">数据集介绍</h3>
<p>我们使用的数据集是<a target="_blank" rel="noopener" href="https://rajpurkar.github.io/SQuAD-explorer/">SQUAD 2</a>，Stanford Question Answering Dataset (SQuAD) 是一个阅读理解数据集，由众工对一组维基百科文章提出的问题组成，每个问题的答案都是从相应的阅读文章中节选出来的，或者这个问题可能是无法回答的。</p>
<h3 id="加载数据">加载数据</h3>
<p>该数据的加载方式在transformers库中进行了封装，我们可以通过以下语句进行数据加载：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># squad_v2等于True或者False分别代表使用SQUAD v1 或者 SQUAD v2。</span></span><br><span class="line"><span class="comment"># 如果您使用的是其他数据集，那么True代表的是：模型可以回答“不可回答”问题，也就是部分问题不给出答案，而False则代表所有问题必须回答。</span></span><br><span class="line">squad_v2 = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 下载数据（确保有网络）</span></span><br><span class="line">datasets = load_dataset(<span class="string">&quot;squad_v2&quot;</span> <span class="keyword">if</span> squad_v2 <span class="keyword">else</span> <span class="string">&quot;squad&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>如果你使用的是自己的数据，参考<a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE">第一篇实战博客【定位词：加载数据】</a>加载自己的数据。</p>
<p>给定一个数据切分的key（train、validation或者test）和下标即可查看数据。无论是训练集、验证集还是测试集，对于每一个问答数据样本都会有“context&quot;, &quot;question&quot;和“answers”三个key。</p>
<ol type="1">
<li>answers代表答案，answers除了给出了文本片段里的答案文本之外，<strong>还给出了该answer所在位置（以character开始计算，下面的例子是第515位</strong>）。</li>
<li>context代表文本片段；</li>
<li>question代表问题。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment">#&#123;&#x27;answers&#x27;: &#123;&#x27;answer_start&#x27;: [515], &#x27;text&#x27;: [&#x27;Saint Bernadette Soubirous&#x27;]&#125;,</span></span><br><span class="line"><span class="comment"># &#x27;context&#x27;: &#x27;Architecturally, the school has a Catholic character. Atop the Main Building\&#x27;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;id&#x27;: &#x27;5733be284776f41900661182&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;question&#x27;: &#x27;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#x27;,</span></span><br><span class="line"><span class="comment"># &#x27;title&#x27;: &#x27;University_of_Notre_Dame&#x27;&#125;</span></span><br></pre></td></tr></table></figure>
<p>下面的函数将从数据集里随机选择几个例子进行展示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> ClassLabel, <span class="type">Sequence</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(typ, <span class="type">Sequence</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(typ.feature, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> x: [typ.feature.names[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">show_random_elements(datasets[<span class="string">&quot;train&quot;</span>], num_examples=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
answers
</th>
<th>
context
</th>
<th>
id
</th>
<th>
question
</th>
<th>
title
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
{'answer_start': [185], 'text': ['diesel fuel']}
</td>
<td>
In Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.
</td>
<td>
571b074c9499d21900609be3
</td>
<td>
Besides crude oil, what does the Suncor Energy plant produce?
</td>
<td>
Asphalt
</td>
</tr>
</tbody>
</table>
<h2 id="数据预处理">数据预处理</h2>
<p>在将数据喂入模型之前，我们需要对数据进行预处理。</p>
<p>仍然是两个数据预处理的基本流程：</p>
<ol type="1">
<li>分词；</li>
<li>转化成对应任务输入模型的格式；</li>
</ol>
<p><code>Tokenizer</code>用于上面两步数据预处理工作：<code>Tokenizer</code>首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。</p>
<h3 id="初始化tokenizer">初始化Tokenizer</h3>
<p><a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer">之前的博客</a>已经介绍了一些Tokenizer的内容，并做了Tokenizer分词的示例，这里不再重复。<code>use_fast=True</code>指定使用fast版本的tokenizer。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="转化成对应任务输入模型的格式">转化成对应任务输入模型的格式</h3>
<p>机器问答预训练模型通常将question和context拼接之后作为输入，然后让模型从context里寻找答案。<strong>对于context中无答案的情况，我们直接将标注的答案起始位置和结束位置放置在CLS的下标处。</strong></p>
<p>我们将question作为tokenizer的句子1，context作为tokenizer的句子2，tokenizer会将他们拼接起来并加入特殊字符作为模型输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example = datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line">tokenized_example=tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>])</span><br><span class="line">tokenized_example[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line"><span class="comment">#[101,2000,3183,2106,1996,6261,2984,9382,3711,1999,8517,1999,10223,26371,2605,1029,102,6549,2135,1010,1996,2082,2038,1037,3234,2839,1012,10234,1996,2364,2311,1005,1055,2751,8514,2003,1037,3585,6231,1997,1996,6261,2984,1012,3202,1999,2392,1997,1996,2364,2311,1998,5307,2009,1010,2003,1037,6967,6231,1997,4828,2007,2608,2039,14995,6924,2007,1996,5722,1000,2310,3490,2618,4748,2033,18168,5267,1000,1012,2279,2000,1996,2364,2311,2003,1996,13546,1997,1996,6730,2540,1012,3202,2369,1996,13546,2003,1996,24665,23052,1010,1037,14042,2173,1997,7083,1998,9185,1012,2009,2003,1037,15059,1997,1996,24665,23052,2012,10223,26371,1010,2605,2073,1996,6261,2984,22353,2135,2596,2000,3002,16595,9648,4674,2061,12083,9711,2271,1999,8517,1012,2012,1996,2203,1997,1996,2364,3298,1006,1998,1999,1037,3622,2240,2008,8539,2083,1017,11342,1998,1996,2751,8514,1007,1010,2003,1037,3722,1010,2715,2962,6231,1997,2984,1012,102]</span></span><br></pre></td></tr></table></figure>
<p>我们<strong>使用<code>sequence_ids</code>方法来获取mask区分question和context</strong>。 <code>None</code>对应了special tokens，然后0或者1分表代表第1个文本和第2个文本，由于我们question第1个传入，context第2个传入，所以分别对应question和context。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sequence_ids = tokenized_example.sequence_ids()</span><br><span class="line"><span class="built_in">print</span>(sequence_ids)</span><br><span class="line"><span class="comment">#[None,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,None,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,None]</span></span><br></pre></td></tr></table></figure>
<p>现在需要特别思考一个问题：<strong>当遇到超长context时（超过了模型能处理的最大长度）会不会对模型造成影响呢？</strong></p>
<p><strong>一般来说预训练模型输入有最大长度要求，然后通常将超长输入进行截断</strong>。但是，<strong>如果我们将问答数据三元组&lt;question, context, answer&gt;中超长context截断，那么可能丢掉答案</strong>（因为是从context中抽取出一个小片段作为答案）。</p>
<h4 id="那么预训练机器问答模型是如何处理超长文本的呢"><strong>那么预训练机器问答模型是如何处理超长文本的呢？</strong></h4>
<p>我们首先找到一个超过模型最大长度的例子，然后分析处理上述问题的机制。</p>
<p>for循环遍历数据集，寻找一个超长样本，我们前面选择的模型所要求的最大输入是384（经常使用的还有512）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(datasets[<span class="string">&quot;train&quot;</span>]):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>])[<span class="string">&quot;input_ids&quot;</span>]) &gt; <span class="number">384</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">example = datasets[<span class="string">&quot;train&quot;</span>][i]</span><br></pre></td></tr></table></figure>
<p>如果不截断的话，那么输入的长度是396，如果我们截断成最大长度384，将会丢失超长部分的信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>])[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line"><span class="comment">#396</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(tokenizer(example[<span class="string">&quot;question&quot;</span>], example[<span class="string">&quot;context&quot;</span>], max_length=max_length, truncation=<span class="string">&quot;only_second&quot;</span>)[<span class="string">&quot;input_ids&quot;</span>]) <span class="comment">#truncation=&quot;only_second&quot;表示只对句子2进行截断</span></span><br><span class="line"><span class="comment">#384</span></span><br></pre></td></tr></table></figure>
<p>我们把<strong>超长的输入切片为多个较短的输</strong>入，每个输入都要满足模型最大长度输入要求。由于答案可能存在与切片的地方，因此<strong>允许相邻切片之间有交集，tokenizer中通过<code>doc_stride</code>参数控制</strong>。</p>
<p>预训练模型的tokenizer包装了方法帮助我们完成上述步骤，只需要设定一些参数即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_length = <span class="number">384</span> <span class="comment"># 输入feature的最大长度，question和context拼接之后</span></span><br><span class="line">doc_stride = <span class="number">128</span> <span class="comment"># 2个切片之间的重合token数量。</span></span><br></pre></td></tr></table></figure>
<p>注意：<strong>一般来说，我们只对context进行切片，不会对问题进行切片，由于context是拼接在question后面的，对应着第2个文本，所以使用<code>only_second</code>控制</strong>。<strong>tokenizer使用<code>doc_stride</code>控制切片之间的重合长度。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_example = tokenizer(</span><br><span class="line">    example[<span class="string">&quot;question&quot;</span>],</span><br><span class="line">    example[<span class="string">&quot;context&quot;</span>],</span><br><span class="line">    max_length=max_length,</span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    stride=doc_stride</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由于对超长输入进行了切片，我们得到了多个输入，这些输入input_ids对应的长度是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokenized_example[<span class="string">&quot;input_ids&quot;</span>]]</span><br><span class="line"><span class="comment">#[384, 157]</span></span><br></pre></td></tr></table></figure>
<p>我们可以将预处理后的token IDs，input_ids还原为文本格式，方便检查切片结果。可以发现<strong>tokenizer自动帮我们为第二个切片的context拼接了question文本</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokenized_example[<span class="string">&quot;input_ids&quot;</span>][:<span class="number">2</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;切片: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(tokenizer.decode(x))</span><br><span class="line"><span class="comment">#切片: 0</span></span><br><span class="line"><span class="comment">#[CLS] how many wins does the notre dame men&#x27;s basketball team have? [SEP] the men&#x27;s basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla&#x27;s record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla&#x27;s 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey&#x27;s fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]</span></span><br><span class="line"><span class="comment">#切片: 1</span></span><br><span class="line"><span class="comment">#[CLS] how many wins does the notre dame men&#x27;s basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey&#x27;s fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]</span></span><br></pre></td></tr></table></figure>
<p>我们知道机器问答模型将使用答案的位置（答案的起始位置和结束位置，start和end）作为训练标签（而不是答案的token IDS）。那么<strong>由于进行了切片，一个新的问题出现了：答案所在的位置被改变了，因此需要重新寻找答案所在位置（相对于每一片context开头的相对位置</strong>）。</p>
<p>所以<strong>切片需要和原始输入有一个对应关系，每个token在切片后context的位置和原始超长context里位置的对应关系</strong>。</p>
<h4 id="获取切片前后的位置对应关系">获取切片前后的位置对应关系</h4>
<p><strong>在tokenizer里可以使用<code>return_offsets_mapping=True</code>参数得到这个对应关系的map</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_example = tokenizer(</span><br><span class="line">    example[<span class="string">&quot;question&quot;</span>],</span><br><span class="line">    example[<span class="string">&quot;context&quot;</span>],</span><br><span class="line">    max_length=max_length,</span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">    stride=doc_stride</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们打印tokenized_example切片1（也就是第二个切片，前30个tokens）在原始context片里的位置。注意特殊token是（如<code>[CLS]</code>设定为(0, 0)），是因为这个token不属于qeustion或者answer的一部分。第2个token对应的起始和结束位置是0和3。可以发现<strong>切片1context部分第2个token没有从(0,N)标记，而是从记录了其在原本超长文本中的位置。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印切片前后位置下标的对应关系</span></span><br><span class="line"><span class="built_in">print</span>(tokenized_example[<span class="string">&quot;offset_mapping&quot;</span>][<span class="number">1</span>][:<span class="number">30</span>])</span><br><span class="line"><span class="comment">#[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172)]</span></span><br></pre></td></tr></table></figure>
<p>因此我们可以根据切片后的<code>token id</code>转化对应的<code>token</code>，然后使用<code>offset_mapping</code>参数映射回切片前的token位置，找到原始位置的tokens。由于question拼接在context前面，所以直接从question里根据下标找。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">first_token_id = tokenized_example[<span class="string">&quot;input_ids&quot;</span>][<span class="number">0</span>][<span class="number">1</span>] <span class="comment">#2129</span></span><br><span class="line">offsets = tokenized_example[<span class="string">&quot;offset_mapping&quot;</span>][<span class="number">0</span>][<span class="number">1</span>] <span class="comment">#(0, 3)</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.convert_ids_to_tokens([first_token_id])[<span class="number">0</span>], example[<span class="string">&quot;question&quot;</span>][offsets[<span class="number">0</span>]:offsets[<span class="number">1</span>]])</span><br><span class="line"><span class="comment">#how How</span></span><br></pre></td></tr></table></figure>
<p>因此我们得到更新答案相对切片context位置的流程：</p>
<ol type="1">
<li>我们首先找到context在句子1和句子2拼接后的句子中的起始位置token_start_index、终止位置token_end_index；</li>
<li>然后判断答案是否在文本区间外部：
<ol type="1">
<li>若在，则更新答案的位置；</li>
<li>若不在，则让答案标注在CLS token位置。</li>
</ol></li>
</ol>
<p>最终我们可以更新标注的答案在预处理之后的features里的位置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">answers = example[<span class="string">&quot;answers&quot;</span>] <span class="comment">#答案&#123;&#x27;answer_start&#x27;: [30], &#x27;text&#x27;: [&#x27;over 1,600&#x27;]&#125;</span></span><br><span class="line">start_char = answers[<span class="string">&quot;answer_start&quot;</span>][<span class="number">0</span>] <span class="comment">#答案起始位置30</span></span><br><span class="line">end_char = start_char + <span class="built_in">len</span>(answers[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])<span class="comment">#答案终止位置40</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到当前文本的Start token index.</span></span><br><span class="line">token_start_index = <span class="number">0</span> <span class="comment">#得到context在句子1和句子2拼接后的句子中的起始位置</span></span><br><span class="line"><span class="keyword">while</span> sequence_ids[token_start_index] != <span class="number">1</span>: <span class="comment">#sequence_ids区分question和context</span></span><br><span class="line">    token_start_index += <span class="number">1</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到当前文本的End token index.</span></span><br><span class="line">token_end_index = <span class="built_in">len</span>(tokenized_example[<span class="string">&quot;input_ids&quot;</span>][<span class="number">0</span>]) - <span class="number">1</span><span class="comment">#得到context在句子1和句子2拼接后的句子中的终止位置，可能还要去掉一些padding</span></span><br><span class="line"><span class="keyword">while</span> sequence_ids[token_end_index] != <span class="number">1</span>:</span><br><span class="line">    token_end_index -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检测答案是否在文本区间的外部，这种情况下意味着该样本的数据标注在CLS token位置。</span></span><br><span class="line">offsets = tokenized_example[<span class="string">&quot;offset_mapping&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="keyword">if</span> (offsets[token_start_index][<span class="number">0</span>] &lt;= start_char <span class="keyword">and</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char): <span class="comment">#答案在文本内</span></span><br><span class="line">    <span class="comment"># 将token_start_index和token_end_index移动到answer所在位置的两侧.</span></span><br><span class="line">    <span class="comment"># 注意：答案在最末尾的边界条件.</span></span><br><span class="line">    <span class="keyword">while</span> token_start_index &lt; <span class="built_in">len</span>(offsets) <span class="keyword">and</span> offsets[token_start_index][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">        token_start_index += <span class="number">1</span> <span class="comment">#之前的token_start_index在context的第一个token位置</span></span><br><span class="line">    start_position = token_start_index - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">        token_end_index -= <span class="number">1</span><span class="comment">#之前的token_end_index在context的最后一个token位置</span></span><br><span class="line">    end_position = token_end_index + <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;start_position: &#123;&#125;, end_position: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(start_position, end_position))</span><br><span class="line"><span class="keyword">else</span>: <span class="comment">#答案在文本外</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The answer is not in this feature.&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>start_position: 23, end_position: 26</p>
<p>我们对答案的位置进行验证：使用答案所在位置下标，取到对应的token ID，然后转化为文本，然后和原始答案进行但对比。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tokenizer.decode(tokenized_example[<span class="string">&quot;input_ids&quot;</span>][<span class="number">0</span>][start_position: end_position+<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(answers[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>over 1, 600 over 1,600</p>
<p>此外，还需要注意的是：<strong>有时候question拼接context，而有时候是context拼接question，不同的模型有不同的要求，因此我们需要使用<code>padding_side</code>参数来指定。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pad_on_right = tokenizer.padding_side == <span class="string">&quot;right&quot;</span> <span class="comment">#context在右边</span></span><br></pre></td></tr></table></figure>
<p>现在，把所有步骤合并到一起。如果<code>allow_impossible_answers</code>这个参数是<code>False</code>的话，无答案的样本都会被扔掉。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_train_features</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="comment"># 既要对examples进行truncation（截断）和padding（补全）还要还要保留所有信息，所以要用的切片的方法。</span></span><br><span class="line">    <span class="comment"># 每一个超长文本example会被切片成多个输入，相邻两个输入之间会有交集。</span></span><br><span class="line">    tokenized_examples = tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;question&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;context&quot;</span>],</span><br><span class="line">        examples[<span class="string">&quot;context&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;question&quot;</span>],</span><br><span class="line">        truncation=<span class="string">&quot;only_second&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;only_first&quot;</span>,</span><br><span class="line">        max_length=max_length,</span><br><span class="line">        stride=doc_stride,</span><br><span class="line">        return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">        return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们使用overflow_to_sample_mapping参数来映射切片片ID到原始ID。</span></span><br><span class="line">    <span class="comment"># 比如有2个expamples被切成4片，那么对应是[0, 0, 1, 1]，前两片对应原来的第一个example。</span></span><br><span class="line">    sample_mapping = tokenized_examples.pop(<span class="string">&quot;overflow_to_sample_mapping&quot;</span>)</span><br><span class="line">    <span class="comment"># offset_mapping也对应4片</span></span><br><span class="line">    <span class="comment"># offset_mapping参数帮助我们映射到原始输入，由于答案标注在原始输入上，所以有助于我们找到答案的起始和结束位置。</span></span><br><span class="line">    offset_mapping = tokenized_examples.pop(<span class="string">&quot;offset_mapping&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重新标注数据</span></span><br><span class="line">    tokenized_examples[<span class="string">&quot;start_positions&quot;</span>] = []</span><br><span class="line">    tokenized_examples[<span class="string">&quot;end_positions&quot;</span>] = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, offsets <span class="keyword">in</span> <span class="built_in">enumerate</span>(offset_mapping):</span><br><span class="line">        <span class="comment"># 对每一片进行处理</span></span><br><span class="line">        <span class="comment"># 将无答案的样本标注到CLS上</span></span><br><span class="line">        input_ids = tokenized_examples[<span class="string">&quot;input_ids&quot;</span>][i]</span><br><span class="line">        cls_index = input_ids.index(tokenizer.cls_token_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 区分question和context</span></span><br><span class="line">        sequence_ids = tokenized_examples.sequence_ids(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拿到原始的example 下标.</span></span><br><span class="line">        sample_index = sample_mapping[i]</span><br><span class="line">        answers = examples[<span class="string">&quot;answers&quot;</span>][sample_index]</span><br><span class="line">        <span class="comment"># 如果没有答案，则使用CLS所在的位置为答案.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(answers[<span class="string">&quot;answer_start&quot;</span>]) == <span class="number">0</span>:</span><br><span class="line">            tokenized_examples[<span class="string">&quot;start_positions&quot;</span>].append(cls_index)</span><br><span class="line">            tokenized_examples[<span class="string">&quot;end_positions&quot;</span>].append(cls_index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 答案的character级别Start/end位置.</span></span><br><span class="line">            start_char = answers[<span class="string">&quot;answer_start&quot;</span>][<span class="number">0</span>]</span><br><span class="line">            end_char = start_char + <span class="built_in">len</span>(answers[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 找到token级别的index start.</span></span><br><span class="line">            token_start_index = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> sequence_ids[token_start_index] != (<span class="number">1</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="number">0</span>):</span><br><span class="line">                token_start_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 找到token级别的index end.</span></span><br><span class="line">            token_end_index = <span class="built_in">len</span>(input_ids) - <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> sequence_ids[token_end_index] != (<span class="number">1</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="number">0</span>):</span><br><span class="line">                token_end_index -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 检测答案是否超出文本长度，超出的话也使用CLS index作为标注.</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (offsets[token_start_index][<span class="number">0</span>] &lt;= start_char <span class="keyword">and</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char):</span><br><span class="line">                tokenized_examples[<span class="string">&quot;start_positions&quot;</span>].append(cls_index)</span><br><span class="line">                tokenized_examples[<span class="string">&quot;end_positions&quot;</span>].append(cls_index)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 如果不超出则找到答案token的start和end位置。.</span></span><br><span class="line">                <span class="comment"># Note: we could go after the last offset if the answer is the last word (edge case).</span></span><br><span class="line">                <span class="keyword">while</span> token_start_index &lt; <span class="built_in">len</span>(offsets) <span class="keyword">and</span> offsets[token_start_index][<span class="number">0</span>] &lt;= start_char:</span><br><span class="line">                    token_start_index += <span class="number">1</span></span><br><span class="line">                tokenized_examples[<span class="string">&quot;start_positions&quot;</span>].append(token_start_index - <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">while</span> offsets[token_end_index][<span class="number">1</span>] &gt;= end_char:</span><br><span class="line">                    token_end_index -= <span class="number">1</span></span><br><span class="line">                tokenized_examples[<span class="string">&quot;end_positions&quot;</span>].append(token_end_index + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenized_examples</span><br></pre></td></tr></table></figure>
<p>以上的预处理函数可以处理一个样本，也可以处理多个样本exapmles。如果是处理多个样本，则返回的是多个样本被预处理之后的结果list。</p>
<p>接下来<strong>使用map函数</strong>对数据集<strong>datasets里面三个样本集合的所有样本进行预处理，</strong>将预处理函数<code>prepare_train_features</code>应用到（map)所有样本上。参数<code>batched=True</code>可以批量对文本进行编码。这是为了充分利用前面加载fast_tokenizer的优势，它将使用多线程并发地处理批中的文本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenized_datasets = datasets.<span class="built_in">map</span>(prepare_train_features, batched=<span class="literal">True</span>, remove_columns=datasets[<span class="string">&quot;train&quot;</span>].column_names)</span><br></pre></td></tr></table></figure>
<h2 id="微调预训练模型">微调预训练模型</h2>
<p>数据已经准备好了，我们需要下载并加载预训练模型，然后微调预训练模型。</p>
<h3 id="加载预训练模型">加载预训练模型</h3>
<p>做<strong>机器问答任务，那么需要一个能解决这个任务的模型类。我们使用<code>AutoModelForQuestionAnswering</code> 这个类</strong>。</p>
<p>和之前几篇博客提到的加载方式相同不再赘述。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForQuestionAnswering</span><br><span class="line">model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
<h3 id="设定训练参数">设定训练参数</h3>
<p>为了能够得到一个<code>Trainer</code>训练工具，我们还需要<strong>训练的设定/参数 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments"><code>TrainingArguments</code></a>。这个训练设定包含了能够定义训练过程的所有属性</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">f&quot;test-squad&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>, <span class="comment">#学习率</span></span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>, <span class="comment"># 训练的次数</span></span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="数据收集器data-collator">数据收集器data collator</h3>
<p>接下来需要告诉<code>Trainer</code>如何从预处理的输入数据中构造batch。我们使用数据收集器data collator，将经预处理的输入分batch再次处理后喂给模型。</p>
<p>我们使用一个default_data_collator将预处理好的数据喂给模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> default_data_collator</span><br><span class="line">data_collator = default_data_collator</span><br></pre></td></tr></table></figure>
<h3 id="定义评估方法">定义评估方法</h3>
<p>注意，本次训练的时候，我们将只会计算loss，暂时不定义评估方法。</p>
<h2 id="开始训练">开始训练</h2>
<p>将数据/模型/参数传入<code>Trainer</code>即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>调用<code>train</code>方法开始训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line"><span class="comment">#及时保存模型</span></span><br><span class="line">trainer.save_model(<span class="string">&quot;test-squad-trained&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型评估">模型评估</h2>
<p>模型的输出是answer所在start/end位置的logits。</p>
<p>用第一个batch来举一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> trainer.get_eval_dataloader(): <span class="comment">#产生batch的迭代器</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">batch = &#123;k: v.to(trainer.args.device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = trainer.model(**batch)</span><br><span class="line">output.keys()</span><br><span class="line"><span class="comment">#odict_keys([&#x27;loss&#x27;, &#x27;start_logits&#x27;, &#x27;end_logits&#x27;])</span></span><br></pre></td></tr></table></figure>
<p>还记得我们在分析<a href="https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%888%EF%BC%89BERT-based%20Model%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/">BERT-based Model源码</a>时，也可以看出BertForQuestionAnswering的输出包括：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">return</span> QuestionAnsweringModelOutput(</span><br><span class="line">            loss=total_loss,</span><br><span class="line">            start_logits=start_logits,</span><br><span class="line">            end_logits=end_logits,</span><br><span class="line">            hidden_states=outputs.hidden_states,</span><br><span class="line">            attentions=outputs.attentions,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>我们在输出预测结果的时候不需要看loss，每个feature（切片）里的每个token都会有两个logit值（分别组成start_logits，end_logits），直接根据logits找到答案的位置即可。</p>
<h3 id="如何根据logits找到答案的位置">如何根据logits找到答案的位置？</h3>
<h4 id="方法1">方法1</h4>
<p>预测answer最简单的方法就是选择start_logits里最大的下标作为answer起始位置，end_logits里最大下标作为answer的结束位置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output.start_logits.shape, output.end_logits.shape</span><br><span class="line"><span class="comment">#(torch.Size([16, 384]), torch.Size([16, 384]))</span></span><br><span class="line"></span><br><span class="line">output.start_logits.argmax(dim=-<span class="number">1</span>), output.end_logits.argmax(dim=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># (tensor([ 46,  57,  78,  43, 118,  15,  72,  35,  15,  34,  73,  41,  80,  91, 156,  35], device=&#x27;cuda:0&#x27;),tensor([ 47,  58,  81,  55, 118, 110,  75,  37, 110,  36,  76,  53,  83,  94, 158,  35], device=&#x27;cuda:0&#x27;))</span></span><br></pre></td></tr></table></figure>
<p>该策略大部分情况下都是不错的。但是，如果我们的输入告诉我们找不到答案：比如start的位置比end的位置下标大，或者start和end的位置指向了question。该怎么办呢？</p>
<h4 id="方法2">方法2</h4>
<p>这个时候，简单的方法是继续需要选择第2好的预测作为答案，实在不行看第3好的预测，以此类推。</p>
<p>但是方法2不太容易找到可行的答案，还有没有更合理一些的方法呢？</p>
<h4 id="方法3">方法3</h4>
<p>分为四个步骤：</p>
<ol type="1">
<li>我们先找到最好的<code>n_best_size</code>个（自定义）start和end对应的可能的备选起始点和终止点；</li>
<li>从中先构建合理的备选答案，<strong>不合理的情况</strong>包括以下几种：
<ol type="1">
<li>start&gt;end的备选起始点和终止点；</li>
<li>start或end超过最大长度；</li>
<li>start和end位置对应的文本在question里面而不在context里面（这里埋了一个雷，）；</li>
</ol></li>
<li>然后将合理备选答案的start和end的logits相加得到新的打分；</li>
<li>最后我们根据<code>score</code>对<code>valid_answers</code>进行排序，找到最好的那一个做为答案。</li>
</ol>
<p><strong>为了找到第3种不合理的情况，我们在validation的features里添加以下两个信息</strong>：</p>
<ol type="1">
<li>产生feature的example ID-<code>overflow_to_sample_mapping</code>：由于每个example可能会产生多个feature，所以每个feature/切片的feature需要知道他们对应的example是哪一个。</li>
<li>offset mapping-<code>offset_mapping</code>： 将每个切片tokens的位置映射回原始文本基于character的下标位置，把question部分的offset_mapping用None掩码，context部分保留不变。</li>
</ol>
<p>我们现在<strong>利用一个<code>prepare_validation_features</code>函数处理validation验证集</strong>，添加上面两个信息，该函数和处理训练的时候的<code>prepare_train_features</code>稍有不同。然后利用处理后的验证集进行评估。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_validation_features</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="comment"># Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results</span></span><br><span class="line">    <span class="comment"># in one example possible giving several features when a context is long, each of those features having a</span></span><br><span class="line">    <span class="comment"># context that overlaps a bit the context of the previous feature.</span></span><br><span class="line">    tokenized_examples = tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;question&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;context&quot;</span>],</span><br><span class="line">        examples[<span class="string">&quot;context&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;question&quot;</span>],</span><br><span class="line">        truncation=<span class="string">&quot;only_second&quot;</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="string">&quot;only_first&quot;</span>,</span><br><span class="line">        max_length=max_length,</span><br><span class="line">        stride=doc_stride,</span><br><span class="line">        return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">        return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">        padding=<span class="string">&quot;max_length&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since one example might give us several features if it has a long context, we need a map from a feature to</span></span><br><span class="line">    <span class="comment"># its corresponding example. This key gives us just that.</span></span><br><span class="line">    <span class="comment"># 我们使用overflow_to_sample_mapping参数来映射切片片ID到原始ID。</span></span><br><span class="line">    <span class="comment"># 比如有2个expamples被切成4片，那么对应是[0, 0, 1, 1]，前两片对应原来的第一个example。</span></span><br><span class="line">    sample_mapping = tokenized_examples.pop(<span class="string">&quot;overflow_to_sample_mapping&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We keep the example_id that gave us this feature and we will store the offset mappings.</span></span><br><span class="line">    tokenized_examples[<span class="string">&quot;example_id&quot;</span>] = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tokenized_examples[<span class="string">&quot;input_ids&quot;</span>])):</span><br><span class="line">        <span class="comment"># Grab the sequence corresponding to that example (to know what is the context and what is the question).</span></span><br><span class="line">        sequence_ids = tokenized_examples.sequence_ids(i)</span><br><span class="line">        context_index = <span class="number">1</span> <span class="keyword">if</span> pad_on_right <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># One example can give several spans, this is the index of the example containing this span of text.</span></span><br><span class="line">        <span class="comment"># 拿到原始的example 下标.</span></span><br><span class="line">        sample_index = sample_mapping[i]</span><br><span class="line">        tokenized_examples[<span class="string">&quot;example_id&quot;</span>].append(examples[<span class="string">&quot;id&quot;</span>][sample_index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set to None the offset_mapping that are not part of the context so it&#x27;s easy to determine if a token</span></span><br><span class="line">        <span class="comment"># position is part of the context or not.</span></span><br><span class="line">        <span class="comment"># 检查答案是否在context中，如果不在offset_mapping为None</span></span><br><span class="line">        <span class="comment"># 其实就是把question部分的offset_mapping用None掩码，context部分保留不变</span></span><br><span class="line">        tokenized_examples[<span class="string">&quot;offset_mapping&quot;</span>][i] = [</span><br><span class="line">            (o <span class="keyword">if</span> sequence_ids[k] == context_index <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">for</span> k, o <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokenized_examples[<span class="string">&quot;offset_mapping&quot;</span>][i])</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tokenized_examples</span><br></pre></td></tr></table></figure>
<p>和之前一样将<code>prepare_validation_features</code>函数应用到每个验证集合的样本上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validation_features = datasets[<span class="string">&quot;validation&quot;</span>].<span class="built_in">map</span>(</span><br><span class="line">    prepare_validation_features,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=datasets[<span class="string">&quot;validation&quot;</span>].column_names</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>使用<code>Trainer.predict</code>方法获得所有预测结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_predictions = trainer.predict(validation_features)</span><br></pre></td></tr></table></figure>
<p><strong>这个 <code>Trainer</code> 隐藏了 一些模型训练时候没有使用的属性(这里是 <code>example_id</code>和<code>offset_mapping</code>，后处理的时候会用到)，所以我们需要把这些设置回来:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validation_features.set_format(<span class="built_in">type</span>=validation_features.<span class="built_in">format</span>[<span class="string">&quot;type&quot;</span>], columns=<span class="built_in">list</span>(validation_features.features.keys()))</span><br></pre></td></tr></table></figure>
<p>经过前面的<code>prepare_validation_features</code>函数处理，<strong>当一个token位置对应question部分offset mappings为<code>None</code>，</strong>所以我们根据offset mapping可以判断token是否在context里面。</p>
<p>更近一步地，我们用<code>max_answer_length</code>控制去掉特别长的答案。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_best_size = <span class="number">20</span></span><br><span class="line">max_answer_length = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">start_logits = output.start_logits[<span class="number">0</span>].cpu().numpy()</span><br><span class="line">end_logits = output.end_logits[<span class="number">0</span>].cpu().numpy()</span><br><span class="line">offset_mapping = validation_features[<span class="number">0</span>][<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"><span class="comment"># The first feature comes from the first example. For the more general case, we will need to be match the example_id to</span></span><br><span class="line"><span class="comment"># an example index</span></span><br><span class="line">context = datasets[<span class="string">&quot;validation&quot;</span>][<span class="number">0</span>][<span class="string">&quot;context&quot;</span>]</span><br><span class="line"><span class="comment"># 收集最佳的start和end logits的位置</span></span><br><span class="line"><span class="comment"># Gather the indices the best start/end logits:</span></span><br><span class="line">start_indexes = np.argsort(start_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">end_indexes = np.argsort(end_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">valid_answers = []</span><br><span class="line"><span class="keyword">for</span> start_index <span class="keyword">in</span> start_indexes:</span><br><span class="line">    <span class="keyword">for</span> end_index <span class="keyword">in</span> end_indexes:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Don&#x27;t consider out-of-scope answers, either because the indices are out of bounds or correspond</span></span><br><span class="line">        <span class="comment"># to part of the input_ids that are not in the context.</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="comment">#答案不合理</span></span><br><span class="line">            start_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">            <span class="keyword">or</span> end_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">            <span class="keyword">or</span> offset_mapping[start_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">or</span> offset_mapping[end_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">        ):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># Don&#x27;t consider answers with a length that is either &lt; 0 or &gt; max_answer_length.</span></span><br><span class="line">        <span class="keyword">if</span> end_index &lt; start_index <span class="keyword">or</span> end_index - start_index + <span class="number">1</span> &gt; max_answer_length:<span class="comment">#答案不合理</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> start_index &lt;= end_index: <span class="comment"># We need to refine that test to check the answer is inside the context # 如果start小于end，那么是合理的可能答案</span></span><br><span class="line">            start_char = offset_mapping[start_index][<span class="number">0</span>]</span><br><span class="line">            end_char = offset_mapping[end_index][<span class="number">1</span>]</span><br><span class="line">            valid_answers.append(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;score&quot;</span>: start_logits[start_index] + end_logits[end_index],</span><br><span class="line">                    <span class="string">&quot;text&quot;</span>: context[start_char: end_char]<span class="comment"># 后续需要根据token的下标将答案找出来</span></span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line"><span class="comment">#最后根据`score`对`valid_answers`进行排序，找到最好的那一个</span></span><br><span class="line">valid_answers = <span class="built_in">sorted</span>(valid_answers, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;score&quot;</span>], reverse=<span class="literal">True</span>)[:n_best_size]</span><br><span class="line">valid_answers</span><br></pre></td></tr></table></figure>
<p>将预测答案和真实答案进行比较，验证一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets[<span class="string">&quot;validation&quot;</span>][<span class="number">0</span>][<span class="string">&quot;answers&quot;</span>]</span><br><span class="line"><span class="comment">#&#123;&#x27;answer_start&#x27;: [177, 177, 177],</span></span><br><span class="line"><span class="comment"># &#x27;text&#x27;: [&#x27;Denver Broncos&#x27;, &#x27;Denver Broncos&#x27;, &#x27;Denver Broncos&#x27;]&#125;</span></span><br></pre></td></tr></table></figure>
<p>这里还有一个问题需要思考：</p>
<p><strong>当一个example被分成多个切片输入模型，模型会把这些切片当作多个单独的“样本”进行训练，那我们在计算正确率和召回率的时候，不能以这多个切片为单位直接计算，而是应该将其对应的一个example为单位进行计算。</strong></p>
<p>对于上面地例子来说，由于答案正好在第1个feature，而第1个feature一定是来自于第1个example，所以相对容易。<strong>对于一个超长example产生的其他fearures来说，需要一个features和examples进行映射的map。因此由于一个example可能被切片成多个features，所以我们需要将所有features里的答案全部收集起来。</strong></p>
<p>以下的代码就将exmaple的下标和features的下标进行map映射。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line">examples = datasets[<span class="string">&quot;validation&quot;</span>]</span><br><span class="line">features = validation_features</span><br><span class="line"></span><br><span class="line">example_id_to_index = &#123;k: i <span class="keyword">for</span> i, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples[<span class="string">&quot;id&quot;</span>])&#125;</span><br><span class="line">features_per_example = collections.defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">    features_per_example[example_id_to_index[feature[<span class="string">&quot;example_id&quot;</span>]]].append(i)</span><br></pre></td></tr></table></figure>
<p>对于后处理过程基本上已经全部完成了。</p>
<p>但是这里还还还有一个问题：<strong>如何解决无答案的情况（squad_v2=True）</strong>。</p>
<p>以上的代码都只考虑了context里面的asnwers，我们同样需要将无答案的预测得分进行搜集（无答案的预测对应了CLS token的start和end logits）。如果一个example样本有多个features，那么我们还需要在多个features里预测是不是都无答案。所以<strong>无答案的最终得分是所有features的无答案得分最小的那个。</strong>（为<strong>什么是最小的那个呢？因为答案如果只在一个切片里，其他切片肯定是没有答案的，如果要确保整个example是没有答案的话，相当于最有可能有答案的切片里面也没有答案</strong>）<strong>只要无答案的最终得分高于其他所有答案的得分，那么该问题就是无答案。</strong></p>
<p>最终的后处理函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess_qa_predictions</span>(<span class="params">examples, features, raw_predictions, n_best_size = <span class="number">20</span>, max_answer_length = <span class="number">30</span></span>):</span></span><br><span class="line">    all_start_logits, all_end_logits = raw_predictions</span><br><span class="line">    <span class="comment"># Build a map example to its corresponding features.</span></span><br><span class="line">    example_id_to_index = &#123;k: i <span class="keyword">for</span> i, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples[<span class="string">&quot;id&quot;</span>])&#125;</span><br><span class="line">    features_per_example = collections.defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    <span class="keyword">for</span> i, feature <span class="keyword">in</span> <span class="built_in">enumerate</span>(features):</span><br><span class="line">        features_per_example[example_id_to_index[feature[<span class="string">&quot;example_id&quot;</span>]]].append(i)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The dictionaries we have to fill.</span></span><br><span class="line">    predictions = collections.OrderedDict()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logging.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Post-processing <span class="subst">&#123;<span class="built_in">len</span>(examples)&#125;</span> example predictions split into <span class="subst">&#123;<span class="built_in">len</span>(features)&#125;</span> features.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Let&#x27;s loop over all the examples!</span></span><br><span class="line">    <span class="keyword">for</span> example_index, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(examples)):</span><br><span class="line">        <span class="comment"># Those are the indices of the features associated to the current example.</span></span><br><span class="line">        feature_indices = features_per_example[example_index]</span><br><span class="line"></span><br><span class="line">        min_null_score = <span class="literal">None</span> <span class="comment"># Only used if squad_v2 is True.</span></span><br><span class="line">        valid_answers = []</span><br><span class="line">        </span><br><span class="line">        context = example[<span class="string">&quot;context&quot;</span>]</span><br><span class="line">        <span class="comment"># Looping through all the features associated to the current example.</span></span><br><span class="line">        <span class="keyword">for</span> feature_index <span class="keyword">in</span> feature_indices:</span><br><span class="line">            <span class="comment"># We grab the predictions of the model for this feature.</span></span><br><span class="line">            start_logits = all_start_logits[feature_index]</span><br><span class="line">            end_logits = all_end_logits[feature_index]</span><br><span class="line">            <span class="comment"># This is what will allow us to map some the positions in our logits to span of texts in the original</span></span><br><span class="line">            <span class="comment"># context.</span></span><br><span class="line">            offset_mapping = features[feature_index][<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update minimum null prediction.</span></span><br><span class="line">            cls_index = features[feature_index][<span class="string">&quot;input_ids&quot;</span>].index(tokenizer.cls_token_id)</span><br><span class="line">            feature_null_score = start_logits[cls_index] + end_logits[cls_index]</span><br><span class="line">            <span class="keyword">if</span> min_null_score <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> min_null_score &lt; feature_null_score:</span><br><span class="line">                min_null_score = feature_null_score</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Go through all possibilities for the `n_best_size` greater start and end logits.</span></span><br><span class="line">            start_indexes = np.argsort(start_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">            end_indexes = np.argsort(end_logits)[-<span class="number">1</span> : -n_best_size - <span class="number">1</span> : -<span class="number">1</span>].tolist()</span><br><span class="line">            <span class="keyword">for</span> start_index <span class="keyword">in</span> start_indexes:</span><br><span class="line">                <span class="keyword">for</span> end_index <span class="keyword">in</span> end_indexes:</span><br><span class="line">                    <span class="comment"># Don&#x27;t consider out-of-scope answers, either because the indices are out of bounds or correspond</span></span><br><span class="line">                    <span class="comment"># to part of the input_ids that are not in the context.</span></span><br><span class="line">                    <span class="keyword">if</span> (</span><br><span class="line">                        start_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">                        <span class="keyword">or</span> end_index &gt;= <span class="built_in">len</span>(offset_mapping)</span><br><span class="line">                        <span class="keyword">or</span> offset_mapping[start_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                        <span class="keyword">or</span> offset_mapping[end_index] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                    ):</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment"># Don&#x27;t consider answers with a length that is either &lt; 0 or &gt; max_answer_length.</span></span><br><span class="line">                    <span class="keyword">if</span> end_index &lt; start_index <span class="keyword">or</span> end_index - start_index + <span class="number">1</span> &gt; max_answer_length:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                    start_char = offset_mapping[start_index][<span class="number">0</span>]</span><br><span class="line">                    end_char = offset_mapping[end_index][<span class="number">1</span>]</span><br><span class="line">                    valid_answers.append(</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">&quot;score&quot;</span>: start_logits[start_index] + end_logits[end_index],</span><br><span class="line">                            <span class="string">&quot;text&quot;</span>: context[start_char: end_char]</span><br><span class="line">                        &#125;</span><br><span class="line">                    )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(valid_answers) &gt; <span class="number">0</span>:</span><br><span class="line">            best_answer = <span class="built_in">sorted</span>(valid_answers, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;score&quot;</span>], reverse=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid</span></span><br><span class="line">            <span class="comment"># failure.</span></span><br><span class="line">            best_answer = &#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;&quot;</span>, <span class="string">&quot;score&quot;</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Let&#x27;s pick our final answer: the best one or the null answer (only for squad_v2)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> squad_v2:</span><br><span class="line">            predictions[example[<span class="string">&quot;id&quot;</span>]] = best_answer[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            answer = best_answer[<span class="string">&quot;text&quot;</span>] <span class="keyword">if</span> best_answer[<span class="string">&quot;score&quot;</span>] &gt; min_null_score <span class="keyword">else</span> <span class="string">&quot;&quot;</span></span><br><span class="line">            predictions[example[<span class="string">&quot;id&quot;</span>]] = answer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>
<p>将后处理函数应用到原始预测输出上：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">final_predictions = postprocess_qa_predictions(datasets[<span class="string">&quot;validation&quot;</span>], validation_features, raw_predictions.predictions)</span><br></pre></td></tr></table></figure>
<p>然后加载评测指标：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_metric</span><br><span class="line">metric = load_metric(<span class="string">&quot;squad_v2&quot;</span> <span class="keyword">if</span> squad_v2 <span class="keyword">else</span> <span class="string">&quot;squad&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>基于预测和标注对评测指标进行计算。为了合理的比较，我们需要将预测和标注的格式。对于squad2来说，评测指标还需要<code>no_answer_probability</code>参数（由于已经无答案直接设置成了空字符串，所以这里直接将这个参数设置为0.0）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> squad_v2:</span><br><span class="line">    formatted_predictions = [&#123;<span class="string">&quot;id&quot;</span>: k, <span class="string">&quot;prediction_text&quot;</span>: v, <span class="string">&quot;no_answer_probability&quot;</span>: <span class="number">0.0</span>&#125; <span class="keyword">for</span> k, v <span class="keyword">in</span> predictions.items()]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    formatted_predictions = [&#123;<span class="string">&quot;id&quot;</span>: k, <span class="string">&quot;prediction_text&quot;</span>: v&#125; <span class="keyword">for</span> k, v <span class="keyword">in</span> final_predictions.items()]</span><br><span class="line">references = [&#123;<span class="string">&quot;id&quot;</span>: ex[<span class="string">&quot;id&quot;</span>], <span class="string">&quot;answers&quot;</span>: ex[<span class="string">&quot;answers&quot;</span>]&#125; <span class="keyword">for</span> ex <span class="keyword">in</span> datasets[<span class="string">&quot;validation&quot;</span>]]</span><br><span class="line">metric.compute(predictions=formatted_predictions, references=references)</span><br></pre></td></tr></table></figure>
<p>到这里才结束啦！</p>
<h2 id="参考文献">参考文献</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/篇章4-使用Transformers解决NLP任务/4.4-问答任务-多选问答.md">4.4-问答任务-多选问答.md</a></p>
<p><a href="https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">BERT实战——（1）文本分类</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/transformers">transformers官方文档</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">冬于</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/">https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ifwind.github.io" target="_blank">冬于的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a></div><div class="post_share"><div class="social-share" data-image="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/31/Transformer-BERT-%E5%AE%9E%E6%88%98/"><img class="prev-cover" src="/2021/08/31/Transformer-BERT-%E5%AE%9E%E6%88%98/BERT.jfif" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Transformer/BERT/实战</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%883%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E5%A4%9A%E9%80%89%E9%97%AE%E7%AD%94/"><img class="next-cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">BERT实战——（3）问答任务-多选问答</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/31/BERT实战——（5）生成任务-机器翻译/" title="BERT实战——（5）生成任务-机器翻译"><img class="cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-31</div><div class="title">BERT实战——（5）生成任务-机器翻译</div></div></a></div><div><a href="/2021/08/31/BERT实战——（6）生成任务-摘要生成/" title="BERT实战——（6）生成任务-摘要生成"><img class="cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-31</div><div class="title">BERT实战——（6）生成任务-摘要生成</div></div></a></div><div><a href="/2021/08/20/BERT相关——（1）语言模型/" title="BERT相关——（1）语言模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（1）语言模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（2）Contextualized_Word_Embedding和ELMO模型/" title="BERT相关——（2）Contextualized Word Embedding和ELMO模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（2）Contextualized Word Embedding和ELMO模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（3）BERT模型/" title="BERT相关——（3）BERT模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（3）BERT模型</div></div></a></div><div><a href="/2021/08/20/BERT相关——（4）GPT模型/" title="BERT相关——（4）GPT-2模型"><img class="cover" src="/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/BERT%E5%AE%B6%E6%97%8F.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-20</div><div class="title">BERT相关——（4）GPT-2模型</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#bert%E5%AE%9E%E6%88%984%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94"><span class="toc-number">1.</span> <span class="toc-text">BERT实战——（4）问答任务-抽取式问答</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.1.</span> <span class="toc-text">任务介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87"><span class="toc-number">1.1.2.</span> <span class="toc-text">前期准备</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.2.</span> <span class="toc-text">数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.1.</span> <span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">加载数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96tokenizer"><span class="toc-number">1.3.1.</span> <span class="toc-text">初始化Tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E5%8C%96%E6%88%90%E5%AF%B9%E5%BA%94%E4%BB%BB%E5%8A%A1%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.3.2.</span> <span class="toc-text">转化成对应任务输入模型的格式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%82%A3%E4%B9%88%E9%A2%84%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E8%B6%85%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%91%A2"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">那么预训练机器问答模型是如何处理超长文本的呢？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%88%87%E7%89%87%E5%89%8D%E5%90%8E%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">获取切片前后的位置对应关系</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">微调预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">加载预训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">设定训练参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E5%99%A8data-collator"><span class="toc-number">1.4.3.</span> <span class="toc-text">数据收集器data collator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.4.</span> <span class="toc-text">定义评估方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.5.</span> <span class="toc-text">开始训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">1.6.</span> <span class="toc-text">模型评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%A0%B9%E6%8D%AElogits%E6%89%BE%E5%88%B0%E7%AD%94%E6%A1%88%E7%9A%84%E4%BD%8D%E7%BD%AE"><span class="toc-number">1.6.1.</span> <span class="toc-text">如何根据logits找到答案的位置？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">方法1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%952"><span class="toc-number">1.6.1.2.</span> <span class="toc-text">方法2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%953"><span class="toc-number">1.6.1.3.</span> <span class="toc-text">方法3</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.7.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 冬于</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
      appKey: 'YKdl4HKX9W6jLSdPlypgEtDM',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: 'https://e4lpmfet.lc-cn-n1-shared.com',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.17.0/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://e4lpmfet.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
        "X-LC-Key": 'YKdl4HKX9W6jLSdPlypgEtDM',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>