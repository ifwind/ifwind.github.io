<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Task4-基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类 | 冬于的博客</title><meta name="keywords" content="深度学习,NLP,文本分类,Word2Vec,TextCNN,BiLSTM,Attention"><meta name="author" content="冬于"><meta name="copyright" content="冬于"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Task4 基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类 完整代码见：NLP-hands-on&#x2F;天池-零基础入门NLP at main · ifwind&#x2F;NLP-hands-on (github.com) 模型架构 模型结构如下图所示，主要包括WordCNNEncoder、SentEncoder、SentAttention和FC模块。">
<meta property="og:type" content="article">
<meta property="og:title" content="Task4-基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类">
<meta property="og:url" content="https://ifwind.github.io/2021/10/13/Task4-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.2-Word2Vec+TextCNN%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="冬于的博客">
<meta property="og:description" content="Task4 基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类 完整代码见：NLP-hands-on&#x2F;天池-零基础入门NLP at main · ifwind&#x2F;NLP-hands-on (github.com) 模型架构 模型结构如下图所示，主要包括WordCNNEncoder、SentEncoder、SentAttention和FC模块。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ifwind.github.io/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg">
<meta property="article:published_time" content="2021-10-13T14:22:49.000Z">
<meta property="article:modified_time" content="2021-10-16T10:14:23.881Z">
<meta property="article:author" content="冬于">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="文本分类">
<meta property="article:tag" content="Word2Vec">
<meta property="article:tag" content="TextCNN">
<meta property="article:tag" content="BiLSTM">
<meta property="article:tag" content="Attention">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ifwind.github.io/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ifwind.github.io/2021/10/13/Task4-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.2-Word2Vec+TextCNN%E5%88%86%E7%B1%BB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LVK1P2D38K","apiKey":"8cbbb0bcbb5c7448f68b4fae01d4ccd5","indexName":"DongYu","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Task4-基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-16 18:14:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">56</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">40</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">冬于的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Task4-基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-13T14:22:49.000Z" title="发表于 2021-10-13 22:22:49">2021-10-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-16T10:14:23.881Z" title="更新于 2021-10-16 18:14:23">2021-10-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7,281</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>37分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Task4-基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="task4-基于深度学习的文本分类2.2-word2vectextcnnbilstmattention分类">Task4 基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类</h1>
<p>完整代码见：<a target="_blank" rel="noopener" href="https://github.com/ifwind/NLP-hands-on/tree/main/天池-零基础入门NLP/Word2Vec_TextCNN_BiLSTM_Attention">NLP-hands-on/天池-零基础入门NLP at main · ifwind/NLP-hands-on (github.com)</a></p>
<h2 id="模型架构">模型架构</h2>
<p>模型结构如下图所示，主要包括WordCNNEncoder、SentEncoder、SentAttention和FC模块。</p>
<p><img src="/2021/10/13/Task4-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.2-Word2Vec+TextCNN%E5%88%86%E7%B1%BB/Word2Vec+TextCNN+BiLSTM+Attention模型框架.png"></p>
<p>最终需要做的是文档分类任务，从文档的角度出发，文档由多个句子序列组成，而句子序列由多个词组成，因此我们可以考虑<strong>从词的embedding-&gt;获取句子的embedding-&gt;再获得文档的embedding-&gt;最后根据文档的embedding对文档分类</strong>。</p>
<p>CNN模块常用于图像数据，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1408.5882">Convolutional Neural Networks for Sentence Classification</a>等论文将CNN用于文本数据，如下图所示，值得注意的是，CNN的卷积核在文本数据上，卷积核的宽度和word embedding的维度相同。</p>
<p><img src="https://pic3.zhimg.com/80/v2-8eb9018149e83f2db2a7772fb0d079d2_720w.jpg"></p>
<p><strong>WordCNNEncoder</strong>包括三个不同卷积核大小的CNN层和相应的三个max pooling层，用于对一个句子卷积，然后max pooling得到一个句子的embedding。</p>
<p><strong>SentEncoder</strong>包括多个BiLSTM层，将一篇文档中的句子序列作为输入，得到一篇文档中各个句子的embedding。</p>
<p><strong>Attention</strong>中输入的一篇文档中各个句子的embedding首先经过线性变化得到<code>key</code>，<code>query</code>是可学习的参数矩阵，<code>value</code>和<code>key</code>相同，得到每个句子embedding重要性加权的一篇文档的embedding。</p>
<p>每个batch由多个文档组成；文档由多个句子序列组成；句子序列由多个词组成。所以输入整体模型的batch形状为：batch_size, max_doc_len, max_sent_len；</p>
<ul>
<li>输入wordCNNEncoder的batch形状为：batch_size * max_doc_len, max_sent_len（只输入词的id）；
<ol type="1">
<li>利用word2vec embedding（固定）和随机初始化的权重（需要被训练）构建word embedding（二者相加）：batch_size * max_doc_len,1(添加的一个channel维度，方便做卷积), max_sent_len, word_embed_size；</li>
<li>分别经过卷积核为2、3、4的三个CNN层：batch_size * max_doc_len，sentence_len, hidden_size；</li>
<li>再分别经过三个相应的max pooling层：batch_size * max_doc_len，1, hidden_size；</li>
<li>拼接三个max pooling层的输出：batch_size * max_doc_len，1, 3*hidden_size(sent_rep_size)；</li>
<li>输出：batch_size * max_doc_len, sent_rep_size；</li>
</ol></li>
<li>输入SentEncoder的batch形状为：batch_size, max_doc_len，sent_rep_size；</li>
<li>输入Attention的batch形状为：batch_size, max_doc_len，2 * hidden_size of lstm；</li>
<li>输入FC的batch形状为：batch_size， 2*hidden。</li>
</ul>
<h3 id="模型代码">模型代码</h3>
<p>根据上述流程分析，模型代码就好理解了，各个模块的模型代码如下。</p>
<h4 id="wordcnnencoder">WordCNNEncoder</h4>
<p><code>WordCNNEncoder</code>包括两个<code>embedding</code>层，分别对应<code>batch_inputs1</code>，对应的embedding 层是可学习的，得到<code>word_embed</code>；<code>batch_inputs2</code>，读取的是外部训练好的词向量，这里用的是word2vec的词向量，是不可学习的，得到<code>extword_embed</code>。将 2 个词向量相加，得到最终的词向量<code>batch_embed</code>，形状是<code>(batch_size * doc_len, sent_len, 100)</code>，然后添加一个维度，变为<code>(batch_size * doc_len, 1, sent_len, 100)</code>，对应 Pytorch 里图像的<code>(B, C, H, W)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCNNEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, log,vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(WordCNNEncoder, self).__init__()</span><br><span class="line">        self.log=log</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.word_dims = <span class="number">100</span> <span class="comment"># 词向量的长度是 100 维</span></span><br><span class="line">        <span class="comment"># padding_idx 表示当取第 0 个词时，向量全为 0</span></span><br><span class="line">        <span class="comment"># 这个 Embedding 层是可学习的</span></span><br><span class="line">        self.word_embed = nn.Embedding(vocab.word_size, self.word_dims, padding_idx=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        extword_embed = vocab.load_pretrained_embs(word2vec_path,save_word2vec_embed_path)</span><br><span class="line">        extword_size, word_dims = extword_embed.shape</span><br><span class="line">        self.log.logger.info(<span class="string">&quot;Load extword embed: words %d, dims %d.&quot;</span> % (extword_size, word_dims))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># # 这个 Embedding 层是不可学习的，通过requires_grad=False控制</span></span><br><span class="line">        self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))</span><br><span class="line">        self.extword_embed.weight.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        input_size = self.word_dims</span><br><span class="line"></span><br><span class="line">        self.filter_sizes = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  <span class="comment"># n-gram window</span></span><br><span class="line">        self.out_channel = <span class="number">100</span></span><br><span class="line">        <span class="comment"># 3 个卷积层，卷积核大小分别为 [2,100], [3,100], [4,100]</span></span><br><span class="line">        self.convs = nn.ModuleList([nn.Conv2d(<span class="number">1</span>, self.out_channel, (filter_size, input_size), bias=<span class="literal">True</span>)</span><br><span class="line">                                    <span class="keyword">for</span> filter_size <span class="keyword">in</span> self.filter_sizes])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word_ids, extword_ids</span>):</span></span><br><span class="line">        <span class="comment"># word_ids: sentence_num * sentence_len</span></span><br><span class="line">        <span class="comment"># extword_ids: sentence_num * sentence_len</span></span><br><span class="line">        <span class="comment"># batch_masks: sentence_num * sentence_len</span></span><br><span class="line">        sen_num, sent_len = word_ids.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># word_embed: sentence_num * sentence_len * 100</span></span><br><span class="line">        <span class="comment"># 根据 index 取出词向量</span></span><br><span class="line">        word_embed = self.word_embed(word_ids)</span><br><span class="line">        extword_embed = self.extword_embed(extword_ids)</span><br><span class="line">        batch_embed = word_embed + extword_embed</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_embed = self.dropout(batch_embed)</span><br><span class="line">        <span class="comment"># batch_embed: sentence_num x 1 x sentence_len x 100</span></span><br><span class="line">        <span class="comment"># squeeze 是为了添加一个 channel 的维度，成为 B * C * H * W</span></span><br><span class="line">        <span class="comment"># 方便下面做 卷积</span></span><br><span class="line">        batch_embed.unsqueeze_(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        pooled_outputs = []</span><br><span class="line">        <span class="comment"># 通过 3 个卷积核做 3 次卷积核池化</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.filter_sizes)):</span><br><span class="line">            <span class="comment"># 通过池化公式计算池化后的高度: o = (i-k)/s+1</span></span><br><span class="line">            <span class="comment"># 其中 o 表示输出的长度</span></span><br><span class="line">            <span class="comment"># k 表示卷积核大小</span></span><br><span class="line">            <span class="comment"># s 表示步长，这里为 1</span></span><br><span class="line">            filter_height = sent_len - self.filter_sizes[i] + <span class="number">1</span></span><br><span class="line">            <span class="comment"># conv：sentence_num * out_channel * filter_height * 1</span></span><br><span class="line">            conv = self.convs[i](batch_embed)</span><br><span class="line">            hidden = F.relu(conv)</span><br><span class="line">            <span class="comment"># 定义池化层：word-&gt;sentence</span></span><br><span class="line">            mp = nn.MaxPool2d((filter_height, <span class="number">1</span>))  <span class="comment"># (filter_height, filter_width)</span></span><br><span class="line">            <span class="comment"># pooled：sentence_num * out_channel * 1 * 1 -&gt; sen_num * out_channel</span></span><br><span class="line">            <span class="comment"># 也可以通过 squeeze 来删除无用的维度</span></span><br><span class="line">            pooled = mp(hidden).reshape(sen_num,</span><br><span class="line">                                        self.out_channel)</span><br><span class="line"></span><br><span class="line">            pooled_outputs.append(pooled)</span><br><span class="line">        <span class="comment"># 拼接 3 个池化后的向量</span></span><br><span class="line">        <span class="comment"># reps: sen_num * (3*out_channel)</span></span><br><span class="line">        reps = torch.cat(pooled_outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            reps = self.dropout(reps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reps</span><br></pre></td></tr></table></figure>
<h4 id="sentencoder">SentEncoder</h4>
<p>LSTM 的 hidden_size 为 256，由于是双向的，经过 LSTM 后的数据维度是<code>(batch_size , doc_len, 512)</code>，然后和 mask 按位置相乘，把没有单词的句子的位置改为 0，最后输出的数据<code>sent_hiddens</code>，维度依然是<code>(batch_size , doc_len, 512)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sent_hidden_size = <span class="number">256</span></span><br><span class="line">sent_num_layers = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, sent_rep_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SentEncoder, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.sent_lstm = nn.LSTM(</span><br><span class="line">            input_size=sent_rep_size, <span class="comment"># 每个句子经过 CNN（卷积+池化）后得到 300 维向量</span></span><br><span class="line">            hidden_size=sent_hidden_size,<span class="comment"># 输出的维度</span></span><br><span class="line">            num_layers=sent_num_layers,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            bidirectional=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, sent_reps, sent_masks</span>):</span></span><br><span class="line">        <span class="comment"># sent_reps:  b * doc_len * sent_rep_size</span></span><br><span class="line">        <span class="comment"># sent_masks: b * doc_len</span></span><br><span class="line">        <span class="comment"># sent_hiddens:  b * doc_len * hidden*2</span></span><br><span class="line">        <span class="comment"># sent_hiddens:  batch, seq_len, num_directions * hidden_size</span></span><br><span class="line">        <span class="comment"># containing the output features (h_t) from the last layer of the LSTM, for each t.</span></span><br><span class="line">        sent_hiddens, _ = self.sent_lstm(sent_reps)</span><br><span class="line">        <span class="comment"># 对应相乘，用到广播，是为了只保留有句子的位置的数值</span></span><br><span class="line">        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            sent_hiddens = self.dropout(sent_hiddens)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sent_hiddens</span><br></pre></td></tr></table></figure>
<h4 id="attention">Attention</h4>
<p><code>query</code>的维度是<code>512</code>，<code>key</code>和<code>query</code>相乘，得到<code>outputs</code>并经过<code>softmax</code>，维度是<code>(batch_size , doc_len)</code>，表示分配到每个句子的权重。使用<code>sent_masks</code>，把没有单词的句子的权重置为<code>-1e32</code>，得到<code>masked_attn_scores</code>。最后把<code>masked_attn_scores</code>和<code>key</code>相乘，得到<code>batch_outputs</code>，形状是<code>(batch_size, 512)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))</span><br><span class="line">        self.weight.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">        self.bias = nn.Parameter(torch.Tensor(hidden_size))</span><br><span class="line">        b = np.zeros(hidden_size, dtype=np.float32)</span><br><span class="line">        self.bias.data.copy_(torch.from_numpy(b))</span><br><span class="line"></span><br><span class="line">        self.query = nn.Parameter(torch.Tensor(hidden_size))</span><br><span class="line">        self.query.data.normal_(mean=<span class="number">0.0</span>, std=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batch_hidden, batch_masks</span>):</span></span><br><span class="line">        <span class="comment"># batch_hidden: b * doc_len * hidden_size (2 * hidden_size of lstm)</span></span><br><span class="line">        <span class="comment"># batch_masks:  b x doc_len</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear</span></span><br><span class="line">        <span class="comment"># key： b * doc_len * hidden</span></span><br><span class="line">        key = torch.matmul(batch_hidden, self.weight) + self.bias</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute attention</span></span><br><span class="line">        <span class="comment"># matmul 会进行广播</span></span><br><span class="line">        <span class="comment">#outputs: b * doc_len</span></span><br><span class="line">        outputs = torch.matmul(key, self.query)</span><br><span class="line">        <span class="comment"># 1 - batch_masks 就是取反，把没有单词的句子置为 0</span></span><br><span class="line">        <span class="comment"># masked_fill 的作用是 在 为 1 的地方替换为 value: float(-1e32)</span></span><br><span class="line">        masked_outputs = outputs.masked_fill((<span class="number">1</span> - batch_masks).<span class="built_in">bool</span>(), <span class="built_in">float</span>(-<span class="number">1e32</span>))</span><br><span class="line">        <span class="comment">#attn_scores：b * doc_len</span></span><br><span class="line">        attn_scores = F.softmax(masked_outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0</span></span><br><span class="line">        masked_attn_scores = attn_scores.masked_fill((<span class="number">1</span> - batch_masks).<span class="built_in">bool</span>(), <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sum weighted sources</span></span><br><span class="line">        <span class="comment"># masked_attn_scores.unsqueeze(1)：# b * 1 * doc_len</span></span><br><span class="line">        <span class="comment"># key：b * doc_len * hidden</span></span><br><span class="line">        <span class="comment"># batch_outputs：b * hidden</span></span><br><span class="line">        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(<span class="number">1</span>), key).squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_outputs, attn_scores</span><br></pre></td></tr></table></figure>
<h4 id="完整模型">完整模型</h4>
<p>把 WordCNNEncoder、SentEncoder、Attention、FC 全部连接起来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,log, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.log=log</span><br><span class="line">        self.sent_rep_size = <span class="number">300</span> <span class="comment"># 经过 CNN 后得到的 300 维向量</span></span><br><span class="line">        self.doc_rep_size = sent_hidden_size * <span class="number">2</span> <span class="comment"># lstm 最后输出的向量长度</span></span><br><span class="line">        self.all_parameters = &#123;&#125;</span><br><span class="line">        parameters = []</span><br><span class="line">        self.word_encoder = WordCNNEncoder(log,vocab)</span><br><span class="line"></span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.word_encoder.parameters())))</span><br><span class="line"></span><br><span class="line">        self.sent_encoder = SentEncoder(self.sent_rep_size)</span><br><span class="line">        self.sent_attention = Attention(self.doc_rep_size)</span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.sent_encoder.parameters())))</span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.sent_attention.parameters())))</span><br><span class="line">        <span class="comment"># doc_rep_size</span></span><br><span class="line">        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=<span class="literal">True</span>)</span><br><span class="line">        parameters.extend(<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, self.out.parameters())))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            self.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parameters) &gt; <span class="number">0</span>:</span><br><span class="line">            self.all_parameters[<span class="string">&quot;basic_parameters&quot;</span>] = parameters</span><br><span class="line"></span><br><span class="line">        self.log.logger.info(<span class="string">&#x27;Build model with cnn word encoder, lstm sent encoder.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        para_num = <span class="built_in">sum</span>([np.prod(<span class="built_in">list</span>(p.size())) <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters()])</span><br><span class="line">        self.log.logger.info(<span class="string">&#x27;Model param num: %.2f M.&#x27;</span> % (para_num / <span class="number">1e6</span>))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batch_inputs</span>):</span></span><br><span class="line">        <span class="comment"># batch_inputs(batch_inputs1, batch_inputs2): b * doc_len * sentence_len</span></span><br><span class="line">        <span class="comment"># batch_masks : b * doc_len * sentence_len</span></span><br><span class="line">        batch_inputs1, batch_inputs2, batch_masks = batch_inputs</span><br><span class="line">        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[<span class="number">0</span>], batch_inputs1.shape[<span class="number">1</span>], batch_inputs1.shape[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># batch_inputs1: sentence_num * sentence_len</span></span><br><span class="line">        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)</span><br><span class="line">        <span class="comment"># batch_inputs2: sentence_num * sentence_len</span></span><br><span class="line">        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)</span><br><span class="line">        <span class="comment"># batch_masks: sentence_num * sentence_len</span></span><br><span class="line">        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)</span><br><span class="line">        <span class="comment"># sent_reps: sentence_num * sentence_rep_size</span></span><br><span class="line">        <span class="comment"># sen_num * (3*out_channel) =  sen_num * 300</span></span><br><span class="line">        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># sent_reps：b * doc_len * sent_rep_size</span></span><br><span class="line">        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)</span><br><span class="line">        <span class="comment"># batch_masks：b * doc_len * max_sent_len</span></span><br><span class="line">        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)</span><br><span class="line">        <span class="comment"># sent_masks：b * doc_len any(2) 表示在 第二个维度上判断</span></span><br><span class="line">        <span class="comment"># 表示如果如果一个句子中有词 true，那么这个句子就是 true，用于给 lstm 过滤</span></span><br><span class="line">        sent_masks = batch_masks.<span class="built_in">bool</span>().<span class="built_in">any</span>(<span class="number">2</span>).<span class="built_in">float</span>()  <span class="comment"># b x doc_len</span></span><br><span class="line">        <span class="comment"># sent_hiddens: b * doc_len * num_directions * hidden_size</span></span><br><span class="line">        <span class="comment"># sent_hiddens:  batch, seq_len, 2 * hidden_size</span></span><br><span class="line">        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># doc_reps: b * (2 * hidden_size)</span></span><br><span class="line">        <span class="comment"># atten_scores: b * doc_len</span></span><br><span class="line">        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># b * num_labels</span></span><br><span class="line">        batch_outputs = self.out(doc_reps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> batch_outputs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="数据加载及预处理">数据加载及预处理</h2>
<p>去掉可能的标点符号，并把当前竞赛给的<strong>训练集划分为三个部分：训练集、验证集、测试集</strong>。其中，训练集用于训练，验证集用于调参，测试集用于评估线下和线上的模型效果。</p>
<p>这里首先用train_test_split（注意使用分层抽样）把训练集划分为训练集和测试集（9：1），然后再将训练集进一步划分为训练集和开发集（9：1）.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_preprocess</span>():</span></span><br><span class="line">    rawdata = pd.read_csv(data_file, sep=<span class="string">&#x27;\t&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">    <span class="comment">#用正则表达式按标点替换文本</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    rawdata[<span class="string">&#x27;words&#x27;</span>]=rawdata[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: re.sub(<span class="string">&#x27;3750|900|648&#x27;</span>,<span class="string">&quot;&quot;</span>,x))</span><br><span class="line">    <span class="keyword">del</span> rawdata[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">    <span class="comment">#数据划分</span></span><br><span class="line">    <span class="comment">#如果之前已经做了就直接加载</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(test_index_file) <span class="keyword">and</span> os.path.exists(train_index_file):</span><br><span class="line">        test_index=joblib.load(test_index_file)</span><br><span class="line">        train_index=joblib.load(train_index_file)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        rawdata.reset_index(inplace=<span class="literal">True</span>, drop=<span class="literal">True</span>)</span><br><span class="line">        X = <span class="built_in">list</span>(rawdata.index)</span><br><span class="line">        y = rawdata[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.1</span>,</span><br><span class="line">                                                            stratify=y)  <span class="comment"># stratify=y表示分层抽样，根据不同类别的样本占比进行抽样</span></span><br><span class="line">        test_index = &#123;<span class="string">&#x27;X_test&#x27;</span>: X_test, <span class="string">&#x27;y_test&#x27;</span>: y_test&#125;</span><br><span class="line">        joblib.dump(test_index, <span class="string">&#x27;test_index.pkl&#x27;</span>)</span><br><span class="line">        train_index = &#123;<span class="string">&#x27;X_train&#x27;</span>: X_train, <span class="string">&#x27;y_train&#x27;</span>: y_train&#125;</span><br><span class="line">        joblib.dump(train_index, <span class="string">&#x27;train_index.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    train_x=rawdata.loc[train_index[<span class="string">&#x27;X_train&#x27;</span>]][<span class="string">&#x27;words&#x27;</span>]</span><br><span class="line">    train_y=rawdata.loc[train_index[<span class="string">&#x27;X_train&#x27;</span>]][<span class="string">&#x27;label&#x27;</span>].values</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=<span class="number">0.1</span>,</span><br><span class="line">                                                        stratify=train_y)</span><br><span class="line">    train_data = &#123;<span class="string">&#x27;label&#x27;</span>: y_train, <span class="string">&#x27;text&#x27;</span>: X_train.values&#125;</span><br><span class="line">    dev_data = &#123;<span class="string">&#x27;label&#x27;</span>: y_test, <span class="string">&#x27;text&#x27;</span>: X_test.values&#125;</span><br><span class="line">    <span class="comment">#测试集</span></span><br><span class="line">    test_x=rawdata.loc[test_index[<span class="string">&#x27;X_test&#x27;</span>]]</span><br><span class="line">    test_y=rawdata.loc[test_index[<span class="string">&#x27;X_test&#x27;</span>]][<span class="string">&#x27;label&#x27;</span>].values</span><br><span class="line">    test_data=&#123;<span class="string">&#x27;label&#x27;</span>: test_y, <span class="string">&#x27;text&#x27;</span>: test_x[<span class="string">&#x27;words&#x27;</span>].tolist()&#125;</span><br><span class="line">    <span class="comment">#预测</span></span><br><span class="line"></span><br><span class="line">    f = pd.read_csv(final_test_data_file, sep=<span class="string">&#x27;\t&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">    final_test_data = f[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: re.sub(<span class="string">&#x27;3750|900|648&#x27;</span>,<span class="string">&quot;&quot;</span>,x))</span><br><span class="line">    final_test_data = &#123;<span class="string">&#x27;label&#x27;</span>: [<span class="number">0</span>] * <span class="built_in">len</span>(final_test_data), <span class="string">&#x27;text&#x27;</span>: final_test_data.values&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_data,dev_data,test_data,final_test_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(<span class="string">&#x27;train_data.pkl&#x27;</span>):</span><br><span class="line">    train_data=joblib.load(<span class="string">&#x27;train_data.pkl&#x27;</span>)</span><br><span class="line">    dev_data = joblib.load(<span class="string">&#x27;dev_data.pkl&#x27;</span>)</span><br><span class="line">    test_data = joblib.load(<span class="string">&#x27;test_data.pkl&#x27;</span>)</span><br><span class="line">    final_test_data = joblib.load(<span class="string">&#x27;final_test_data.pkl&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    train_data, dev_data, test_data, final_test_data = data_preprocess()</span><br><span class="line">    joblib.dump(train_data, <span class="string">&#x27;train_data.pkl&#x27;</span>)</span><br><span class="line">    joblib.dump(dev_data, <span class="string">&#x27;dev_data.pkl&#x27;</span>)</span><br><span class="line">    joblib.dump(test_data, <span class="string">&#x27;test_data.pkl&#x27;</span>)</span><br><span class="line">    joblib.dump(final_test_data, <span class="string">&#x27;final_test_data.pkl&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="构建batch">构建batch</h2>
<p>根据上述模型架构的逻辑，我们需要将数据转化为所需要的batch的格式，即batch-&gt;文档-&gt;句子-&gt;词。</p>
<p>用<code>get_examples和sentence_split</code>整理文档-&gt;句子-&gt;词；</p>
<p>然后用<code>data_iter和batch_slice</code>将<code>get_examples</code>的输出进一步整合成batch。</p>
<h3 id="get_examples和sentence_split">get_examples和sentence_split</h3>
<p>遍历每一篇新闻，对每篇新闻都调用<code>sentence_split</code>来分割句子输入的<code>text</code>表示一篇新闻，最后返回的 segments 是一个 list，其中每个元素是 tuple：(句子长度，句子本身)。</p>
<p>最后返回的数据是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)。其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 作用是：根据一篇文章，把这篇文章分割成多个句子</span></span><br><span class="line"><span class="comment"># text 是一个新闻的文章</span></span><br><span class="line"><span class="comment"># vocab 是词典</span></span><br><span class="line"><span class="comment"># max_sent_len 表示每句话的长度</span></span><br><span class="line"><span class="comment"># max_segment 表示最多有几句话</span></span><br><span class="line"><span class="comment"># 最后返回的 segments 是一个list，其中每个元素是 tuple：(句子长度，句子本身)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_split</span>(<span class="params">text, vocab, max_sent_len=<span class="number">256</span>, max_segment=<span class="number">16</span></span>):</span></span><br><span class="line"></span><br><span class="line">    words = text.strip().split()</span><br><span class="line">    document_len = <span class="built_in">len</span>(words)</span><br><span class="line">    <span class="comment"># 划分句子的索引，句子长度为 max_sent_len</span></span><br><span class="line">    index = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, document_len, max_sent_len))</span><br><span class="line">    index.append(document_len)</span><br><span class="line"></span><br><span class="line">    segments = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(index) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 根据索引划分句子</span></span><br><span class="line">        segment = words[index[i]: index[i + <span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(segment) &gt; <span class="number">0</span></span><br><span class="line">        <span class="comment"># 把出现太少的词替换为 UNK</span></span><br><span class="line">        segment = [word <span class="keyword">if</span> word <span class="keyword">in</span> vocab._id2word <span class="keyword">else</span> <span class="string">&#x27;&lt;UNK&gt;&#x27;</span> <span class="keyword">for</span> word <span class="keyword">in</span> segment]</span><br><span class="line">        <span class="comment"># 添加 tuple:(句子长度，句子本身)</span></span><br><span class="line">        segments.append([<span class="built_in">len</span>(segment), segment])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(segments) &gt; <span class="number">0</span></span><br><span class="line">    <span class="comment"># 如果大于 max_segment 句话，则句数减少一半，返回一半的句子</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(segments) &gt; max_segment:</span><br><span class="line">        segment_ = <span class="built_in">int</span>(max_segment / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> segments[:segment_] + segments[-segment_:]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 否则返回全部句子</span></span><br><span class="line">        <span class="keyword">return</span> segments</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后返回的数据是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line"><span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_examples</span>(<span class="params">data, vocab, max_sent_len=<span class="number">256</span>, max_segment=<span class="number">8</span></span>):</span></span><br><span class="line">    label2id = vocab.label2id</span><br><span class="line">    examples = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> text, label <span class="keyword">in</span> <span class="built_in">zip</span>(data[<span class="string">&#x27;text&#x27;</span>], data[<span class="string">&#x27;label&#x27;</span>]):</span><br><span class="line">        <span class="comment"># label</span></span><br><span class="line">        <span class="built_in">id</span> = label2id(label)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sents_words: 是一个list，其中每个元素是 tuple：(句子长度，句子本身)</span></span><br><span class="line">        sents_words = sentence_split(text, vocab, max_sent_len, max_segment)</span><br><span class="line">        doc = []</span><br><span class="line">        <span class="keyword">for</span> sent_len, sent_words <span class="keyword">in</span> sents_words:</span><br><span class="line">            <span class="comment"># 把 word 转为 id</span></span><br><span class="line">            word_ids = vocab.word2id(sent_words)</span><br><span class="line">            <span class="comment"># 把 word 转为 ext id</span></span><br><span class="line">            extword_ids = vocab.extword2id(sent_words)</span><br><span class="line">            doc.append([sent_len, word_ids, extword_ids])</span><br><span class="line">        examples.append([<span class="built_in">id</span>, <span class="built_in">len</span>(doc), doc])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> examples</span><br></pre></td></tr></table></figure>
<h3 id="data_iter和batch_slice">data_iter和batch_slice</h3>
<p>在迭代训练时，调用<code>data_iter</code>函数，生成每一批的<code>batch_data</code>，其中data 参数就是 get_examples() 得到的。而<code>data_iter</code>函数里面会调用<code>batch_slice</code>函数，把数据分割为多个 batch，组成一个 list 并返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data 参数就是 get_examples() 得到的</span></span><br><span class="line"><span class="comment"># data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line"><span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span>(<span class="params">data, batch_size, shuffle=<span class="literal">True</span>, noise=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    randomly permute data, then sort by source length, and partition into batches</span></span><br><span class="line"><span class="string">    ensure that the length of  sentences in each batch</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    batched_data = []</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># 这里是打乱所有数据</span></span><br><span class="line">        np.random.shuffle(data)</span><br><span class="line">        <span class="comment"># lengths 表示的是 每篇文章的句子数量</span></span><br><span class="line">        lengths = [example[<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> data]</span><br><span class="line">        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) <span class="keyword">for</span> l <span class="keyword">in</span> lengths]</span><br><span class="line">        sorted_indices = np.argsort(noisy_lengths).tolist()</span><br><span class="line">        sorted_data = [data[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_indices]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sorted_data = data</span><br><span class="line">    <span class="comment"># 把 batch 的数据放进一个 list</span></span><br><span class="line">    batched_data.extend(<span class="built_in">list</span>(batch_slice(sorted_data, batch_size)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        <span class="comment"># 打乱 多个 batch</span></span><br><span class="line">        np.random.shuffle(batched_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> batched_data:</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br><span class="line"><span class="comment"># build loader</span></span><br><span class="line"><span class="comment"># data 参数就是 get_examples() 得到的</span></span><br><span class="line"><span class="comment"># data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line"><span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_slice</span>(<span class="params">data, batch_size</span>):</span></span><br><span class="line">    batch_num = <span class="built_in">int</span>(np.ceil(<span class="built_in">len</span>(data) / <span class="built_in">float</span>(batch_size)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_num):</span><br><span class="line">        <span class="comment"># 如果 i &lt; batch_num - 1，那么大小为 batch_size，否则就是最后一批数据</span></span><br><span class="line">        cur_batch_size = batch_size <span class="keyword">if</span> i &lt; batch_num - <span class="number">1</span> <span class="keyword">else</span> <span class="built_in">len</span>(data) - batch_size * i</span><br><span class="line">        docs = [data[i * batch_size + b] <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(cur_batch_size)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> docs</span><br></pre></td></tr></table></figure>
<h2 id="vocab类">Vocab类</h2>
<p>为了将文档中的word编码为词向量的形式，需要构造词典，加载并包装Word2vec的embedding。</p>
<h3 id="构造词典-build_vocab">构造词典-build_vocab</h3>
<p>在字典中加入一些特殊字符，如<code>'[PAD]', '[UNK]'</code>，记录需要被训练的word embedding中word对应的id和id对应的word，以及word2vec中固定embedding的extend_word对应id和id对应extend_word。</p>
<h3 id="加载并包装word2vec的embedding-load_pretrained_embs">加载并包装Word2vec的embedding-load_pretrained_embs</h3>
<p>从word2vec的wv矩阵中（之前保存在txt文件中）构建embedding，为了避免每次运行都重复构建一遍，可以把生成的embedding和vocab存储起来，运行时直接加载。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Vocab 的作用是：</span></span><br><span class="line"><span class="comment"># 1. 创建 词 和 index 对应的字典，这里包括 2 份字典，分别是：_id2word 和 _id2extword</span></span><br><span class="line"><span class="comment"># 其中 _id2word 是从新闻得到的， 把词频小于 5 的词替换为了 UNK。对应到模型输入的 batch_inputs1。</span></span><br><span class="line"><span class="comment"># _id2extword 是从 word2vec.txt 中得到的，有 5976 个词。对应到模型输入的 batch_inputs2。</span></span><br><span class="line"><span class="comment"># 后面会有两个 embedding 层，其中 _id2word 对应的 embedding 是可学习的，_id2extword 对应的 embedding 是从文件中加载的，是固定的</span></span><br><span class="line"><span class="comment"># 2.创建 label 和 index 对应的字典</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_data</span>):</span></span><br><span class="line">        self.min_count = <span class="number">5</span></span><br><span class="line">        self.pad = <span class="number">0</span></span><br><span class="line">        self.unk = <span class="number">1</span></span><br><span class="line">        self._id2word = [<span class="string">&#x27;[PAD]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line">        self._id2extword = [<span class="string">&#x27;[PAD]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        self._id2label = []</span><br><span class="line">        self.target_names = []</span><br><span class="line"></span><br><span class="line">        self.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line">        reverse = <span class="keyword">lambda</span> x: <span class="built_in">dict</span>(<span class="built_in">zip</span>(x, <span class="built_in">range</span>(<span class="built_in">len</span>(x))))</span><br><span class="line">        <span class="comment">#创建词和 index 对应的字典</span></span><br><span class="line">        self._word2id = reverse(self._id2word)</span><br><span class="line">        <span class="comment">#创建 label 和 index 对应的字典</span></span><br><span class="line">        self._label2id = reverse(self._id2label)</span><br><span class="line"></span><br><span class="line">        logging.info(<span class="string">&quot;Build vocab: words %d, labels %d.&quot;</span> % (self.word_size, self.label_size))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#创建词典</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(save_word_counter_path):</span><br><span class="line">            self.word_counter=joblib.load(save_word_counter_path)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.word_counter = Counter()</span><br><span class="line">            <span class="comment">#计算每个词出现的次数</span></span><br><span class="line">            <span class="keyword">for</span> text <span class="keyword">in</span> data[<span class="string">&#x27;text&#x27;</span>]:</span><br><span class="line">                words = text.split()</span><br><span class="line">                self.word_counter+=Counter(words)</span><br><span class="line">            joblib.dump(self.word_counter,save_word_counter_path)</span><br><span class="line">            <span class="comment"># for word in words:</span></span><br><span class="line">            <span class="comment">#     self.word_counter[word] += 1</span></span><br><span class="line">        <span class="comment"># 去掉频次小于 min_count = 5 的词，把词存到 _id2word</span></span><br><span class="line">        <span class="keyword">for</span> word, count <span class="keyword">in</span> self.word_counter.most_common():</span><br><span class="line">            <span class="keyword">if</span> count &gt;= self.min_count:</span><br><span class="line">                self._id2word.append(word)</span><br><span class="line"></span><br><span class="line">        label2name = &#123;<span class="number">0</span>: <span class="string">&#x27;科技&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;股票&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;体育&#x27;</span>, <span class="number">3</span>: <span class="string">&#x27;娱乐&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;时政&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;社会&#x27;</span>, <span class="number">6</span>: <span class="string">&#x27;教育&#x27;</span>, <span class="number">7</span>: <span class="string">&#x27;财经&#x27;</span>,</span><br><span class="line">                      <span class="number">8</span>: <span class="string">&#x27;家居&#x27;</span>, <span class="number">9</span>: <span class="string">&#x27;游戏&#x27;</span>, <span class="number">10</span>: <span class="string">&#x27;房产&#x27;</span>, <span class="number">11</span>: <span class="string">&#x27;时尚&#x27;</span>, <span class="number">12</span>: <span class="string">&#x27;彩票&#x27;</span>, <span class="number">13</span>: <span class="string">&#x27;星座&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">        self.label_counter = Counter(data[<span class="string">&#x27;label&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.label_counter)):</span><br><span class="line">            count = self.label_counter[label] <span class="comment"># 取出 label 对应的次数</span></span><br><span class="line">            self._id2label.append(label)</span><br><span class="line">            self.target_names.append(label2name[label]) <span class="comment"># 根据label数字取出对应的名字</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_pretrained_embs</span>(<span class="params">self, embfile,save_embfile</span>):</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(save_embfile):</span><br><span class="line">            embeddings= joblib.load(save_embfile)</span><br><span class="line">            self._id2extword=embeddings[<span class="string">&#x27;id2extword&#x27;</span>]</span><br><span class="line">            embeddings= embeddings[<span class="string">&#x27;embeddings&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(embfile, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                lines = f.readlines()</span><br><span class="line">                items = lines[<span class="number">0</span>].split()</span><br><span class="line">                <span class="comment"># 第一行分别是单词数量、词向量维度</span></span><br><span class="line">                word_count, embedding_dim = <span class="built_in">int</span>(items[<span class="number">0</span>]), <span class="built_in">int</span>(items[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            index = <span class="built_in">len</span>(self._id2extword)</span><br><span class="line">            embeddings = np.zeros((word_count + index, embedding_dim))</span><br><span class="line">            <span class="comment"># 下面的代码和 word2vec.txt 的结构有关</span></span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">1</span>:]:</span><br><span class="line">                values = line.split()</span><br><span class="line">                self._id2extword.append(values[<span class="number">0</span>]) <span class="comment"># 首先添加第一列的单词</span></span><br><span class="line">                vector = np.array(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float64&#x27;</span>) <span class="comment"># 然后添加后面 100 列的词向量</span></span><br><span class="line">                embeddings[self.unk] += vector</span><br><span class="line">                embeddings[index] = vector</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># unk 的词向量是所有词的平均</span></span><br><span class="line">            embeddings[self.unk] = embeddings[self.unk] / word_count</span><br><span class="line">            <span class="comment"># 除以标准差干嘛？</span></span><br><span class="line">            embeddings = embeddings / np.std(embeddings)</span><br><span class="line">            joblib.dump(&#123;<span class="string">&quot;embeddings&quot;</span>:embeddings,<span class="string">&quot;id2extword&quot;</span>:self._id2extword&#125;, save_embfile)</span><br><span class="line"></span><br><span class="line">        reverse = <span class="keyword">lambda</span> x: <span class="built_in">dict</span>(<span class="built_in">zip</span>(x, <span class="built_in">range</span>(<span class="built_in">len</span>(x))))</span><br><span class="line">        self._extword2id = reverse(self._id2extword)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(<span class="built_in">set</span>(self._id2extword)) == <span class="built_in">len</span>(self._id2extword)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据单词得到 id</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2id</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(xs, <span class="built_in">list</span>):</span><br><span class="line">            <span class="keyword">return</span> [self._word2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._word2id.get(xs, self.unk)</span><br><span class="line">    <span class="comment"># 根据单词得到 ext id</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extword2id</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(xs, <span class="built_in">list</span>):</span><br><span class="line">            <span class="keyword">return</span> [self._extword2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._extword2id.get(xs, self.unk)</span><br><span class="line">    <span class="comment"># 根据 label 得到 id</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">label2id</span>(<span class="params">self, xs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(xs, <span class="built_in">list</span>):</span><br><span class="line">            <span class="keyword">return</span> [self._label2id.get(x, self.unk) <span class="keyword">for</span> x <span class="keyword">in</span> xs]</span><br><span class="line">        <span class="keyword">return</span> self._label2id.get(xs, self.unk)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._id2word)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extword_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._id2extword)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">label_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._id2label)</span><br></pre></td></tr></table></figure>
<h2 id="优化器">优化器</h2>
<p>封装各个模块的优化器操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model_parameters</span>):</span></span><br><span class="line">        self.all_params = []</span><br><span class="line">        self.optims = []</span><br><span class="line">        self.schedulers = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name, parameters <span class="keyword">in</span> model_parameters.items():</span><br><span class="line">            <span class="keyword">if</span> name.startswith(<span class="string">&quot;basic&quot;</span>):</span><br><span class="line">                optim = torch.optim.Adam(parameters, lr=learning_rate)</span><br><span class="line">                self.optims.append(optim)</span><br><span class="line"></span><br><span class="line">                l = <span class="keyword">lambda</span> step: decay ** (step // decay_step)</span><br><span class="line">                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)</span><br><span class="line">                self.schedulers.append(scheduler)</span><br><span class="line">                self.all_params.extend(parameters)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                Exception(<span class="string">&quot;no nameed parameters.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.num = <span class="built_in">len</span>(self.optims)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> optim, scheduler <span class="keyword">in</span> <span class="built_in">zip</span>(self.optims, self.schedulers):</span><br><span class="line">            optim.step()</span><br><span class="line">            scheduler.step()</span><br><span class="line">            optim.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> optim <span class="keyword">in</span> self.optims:</span><br><span class="line">            optim.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lr</span>(<span class="params">self</span>):</span></span><br><span class="line">        lrs = <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.get_lr()[-<span class="number">1</span>], self.schedulers))</span><br><span class="line">        lr = <span class="string">&#x27; %.5f&#x27;</span> * self.num</span><br><span class="line">        res = lr % lrs</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="评价指标">评价指标</h2>
<p>分类任务，这里选用的评价指标包括精度、召回、F1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, precision_score, recall_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score</span>(<span class="params">y_ture, y_pred</span>):</span></span><br><span class="line">    y_ture = np.array(y_ture)</span><br><span class="line">    y_pred = np.array(y_pred)</span><br><span class="line">    f1 = f1_score(y_ture, y_pred, average=<span class="string">&#x27;macro&#x27;</span>) * <span class="number">100</span></span><br><span class="line">    p = precision_score(y_ture, y_pred, average=<span class="string">&#x27;macro&#x27;</span>) * <span class="number">100</span></span><br><span class="line">    r = recall_score(y_ture, y_pred, average=<span class="string">&#x27;macro&#x27;</span>) * <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>((reformat(p, <span class="number">2</span>), reformat(r, <span class="number">2</span>), reformat(f1, <span class="number">2</span>))), reformat(f1, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留 n 位小数点</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reformat</span>(<span class="params">num, n</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(<span class="built_in">format</span>(num, <span class="string">&#x27;0.&#x27;</span> + <span class="built_in">str</span>(n) + <span class="string">&#x27;f&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="日志记录">日志记录</h2>
<p>利用logging模块在控制台实时打印并及时记录运行日志。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span>  *</span><br><span class="line"><span class="keyword">import</span> logging  <span class="comment"># 引入logging模块</span></span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,mode=<span class="string">&#x27;w&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># 第一步，创建一个logger</span></span><br><span class="line">        self.logger = logging.getLogger()</span><br><span class="line">        self.logger.setLevel(logging.INFO)  <span class="comment"># Log等级总开关</span></span><br><span class="line">        <span class="comment"># 第二步，创建一个handler，用于写入日志文件</span></span><br><span class="line">        rq = time.strftime(<span class="string">&#x27;%Y%m%d%H%M&#x27;</span>, time.localtime(time.time()))</span><br><span class="line">        log_path = os.getcwd() + <span class="string">&#x27;/Logs/&#x27;</span></span><br><span class="line">        log_name = log_path + rq + <span class="string">&#x27;.log&#x27;</span></span><br><span class="line">        logfile = log_name</span><br><span class="line">        fh = logging.FileHandler(logfile, mode=mode)</span><br><span class="line">        fh.setLevel(logging.DEBUG)  <span class="comment"># 输出到file的log等级的开关</span></span><br><span class="line">        <span class="comment"># 第三步，定义handler的输出格式</span></span><br><span class="line">        formatter = logging.Formatter(<span class="string">&quot;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&quot;</span>)</span><br><span class="line">        fh.setFormatter(formatter)</span><br><span class="line">        <span class="comment"># 第四步，将logger添加到handler里面</span></span><br><span class="line">        self.logger.addHandler(fh)</span><br><span class="line">        ch = logging.StreamHandler()</span><br><span class="line">        ch.setLevel(logging.INFO)  <span class="comment"># 输出到console的log等级的开关</span></span><br><span class="line">        ch.setFormatter(formatter)</span><br><span class="line">        self.logger.addHandler(ch)</span><br></pre></td></tr></table></figure>
<h2 id="trainer">Trainer</h2>
<p><code>batch2tensor</code>函数最后返回的数据是：<code>(batch_inputs1, batch_inputs2, batch_masks), batch_labels</code>。形状都是<code>(batch_size, doc_len, sent_len)</code>。<code>doc_len</code>表示每篇新闻有几乎话，<code>sent_len</code>表示每句话有多少个单词。</p>
<p><code>batch_masks</code>在有单词的位置，值为 1，其他地方为 0，用于后面计算 Attention，把那些没有单词的位置的 attention 改为 0。</p>
<p><code>batch_inputs1, batch_inputs2, batch_masks</code>，形状是<code>(batch_size, doc_len, sent_len)</code>，转换为<code>(batch_size * doc_len, sent_len)</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># build trainer</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,log, model, vocab,train_data,dev_data,test_data=<span class="literal">None</span>,final_test_data=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.report = <span class="literal">True</span></span><br><span class="line">        self.log=log</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get_examples() 返回的结果是 一个 list</span></span><br><span class="line">        <span class="comment"># 每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line">        <span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(<span class="string">&#x27;Trainer_train_data.pkl&#x27;</span>):</span><br><span class="line">            self.train_data = joblib.load(<span class="string">&#x27;Trainer_train_data.pkl&#x27;</span>)</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Total %d train docs.&#x27;</span> % <span class="built_in">len</span>(self.train_data))</span><br><span class="line">            self.dev_data =  joblib.load(<span class="string">&#x27;Trainer_dev_data.pkl&#x27;</span>)</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Total %d dev docs.&#x27;</span> % <span class="built_in">len</span>(self.dev_data))</span><br><span class="line">            self.final_test_data =  joblib.load(<span class="string">&#x27;Trainer_final_test_data.pkl&#x27;</span>)</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Total %d final test docs.&#x27;</span> % <span class="built_in">len</span>(self.final_test_data))</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                self.test_data =  joblib.load(<span class="string">&#x27;Trainer_test_data.pkl&#x27;</span>)</span><br><span class="line">                self.log.logger.info(<span class="string">&#x27;Total %d test docs.&#x27;</span> % <span class="built_in">len</span>(self.test_data))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.train_data = get_examples(train_data, vocab)</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Total %d train docs.&#x27;</span> % <span class="built_in">len</span>(self.train_data))</span><br><span class="line">            self.dev_data = get_examples(dev_data, vocab)</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Total %d dev docs.&#x27;</span> % <span class="built_in">len</span>(self.dev_data))</span><br><span class="line">            self.final_test_data = get_examples(final_test_data, vocab)</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Total %d final test docs.&#x27;</span> % <span class="built_in">len</span>(self.final_test_data))</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                self.test_data = get_examples(test_data, vocab)</span><br><span class="line">                self.log.logger.info(<span class="string">&#x27;Total %d test docs.&#x27;</span> % <span class="built_in">len</span>(self.test_data))</span><br><span class="line">        self.batch_num = <span class="built_in">int</span>(np.ceil(<span class="built_in">len</span>(self.train_data) / <span class="built_in">float</span>(train_batch_size)))</span><br><span class="line">        <span class="comment"># criterion</span></span><br><span class="line">        self.criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># label name</span></span><br><span class="line">        self.target_names = vocab.target_names</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        self.optimizer = Optimizer(model.all_parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># count</span></span><br><span class="line">        self.step = <span class="number">0</span></span><br><span class="line">        self.early_stop = -<span class="number">1</span></span><br><span class="line">        self.best_train_f1, self.best_dev_f1 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        self.last_epoch = epochs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.log.logger.info(<span class="string">&#x27;Start training...&#x27;</span>)</span><br><span class="line">        pbar = tqdm(total=self.last_epoch, desc=<span class="string">&#x27;training&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">            train_f1 = self._train(epoch)</span><br><span class="line"></span><br><span class="line">            dev_f1 = self._<span class="built_in">eval</span>(epoch,test=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.best_dev_f1 &lt;= dev_f1:</span><br><span class="line">                self.log.logger.info(</span><br><span class="line">                    <span class="string">&quot;Exceed history dev = %.2f, current dev = %.2f&quot;</span> % (self.best_dev_f1, dev_f1))</span><br><span class="line">                torch.save(self.model.state_dict(), save_model)</span><br><span class="line"></span><br><span class="line">                self.best_train_f1 = train_f1</span><br><span class="line">                self.best_dev_f1 = dev_f1</span><br><span class="line">                self.early_stop = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.early_stop += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> self.early_stop == early_stops:</span><br><span class="line">                    self.log.logger.info(</span><br><span class="line">                        <span class="string">&quot;Eearly stop in epoch %d, best train: %.2f, dev: %.2f&quot;</span> % (</span><br><span class="line">                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))</span><br><span class="line">                    self.last_epoch = epoch</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            pbar.update()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">self,flag=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="comment"># flag = 1: dev</span></span><br><span class="line">        <span class="comment"># flag = 2: test</span></span><br><span class="line">        <span class="comment"># flag = 3: final_test</span></span><br><span class="line">        self.model.load_state_dict(torch.load(save_model))</span><br><span class="line">        self._<span class="built_in">eval</span>(self.last_epoch + <span class="number">1</span>, test=flag)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_train</span>(<span class="params">self, epoch</span>):</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        self.model.train()</span><br><span class="line"></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        epoch_start_time = time.time()</span><br><span class="line">        overall_losses = <span class="number">0</span></span><br><span class="line">        losses = <span class="number">0</span></span><br><span class="line">        batch_idx = <span class="number">1</span></span><br><span class="line">        y_pred = []</span><br><span class="line">        y_true = []</span><br><span class="line"></span><br><span class="line">        pbar = tqdm(total=self.batch_num,desc=<span class="string">&#x27;train in epoch %d&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> data_iter(self.train_data, train_batch_size, shuffle=<span class="literal">True</span>):</span><br><span class="line">            torch.cuda.empty_cache()</span><br><span class="line">            <span class="comment"># batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)</span></span><br><span class="line">            <span class="comment"># 形状都是：batch_size * doc_len * sent_len</span></span><br><span class="line">            <span class="comment"># batch_labels: batch_size</span></span><br><span class="line">            batch_inputs, batch_labels = self.batch2tensor(batch_data)</span><br><span class="line">            <span class="comment"># batch_outputs：b * num_labels</span></span><br><span class="line">            batch_outputs = self.model(batch_inputs)</span><br><span class="line">            <span class="comment"># criterion 是 CrossEntropyLoss，真实标签的形状是：N</span></span><br><span class="line">            <span class="comment"># 预测标签的形状是：(N,C)</span></span><br><span class="line">            loss = self.criterion(batch_outputs, batch_labels)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            loss_value = loss.detach().cpu().item()</span><br><span class="line">            losses += loss_value</span><br><span class="line">            overall_losses += loss_value</span><br><span class="line">            <span class="comment"># 把预测值转换为一维，方便下面做 classification_report，计算 f1</span></span><br><span class="line">            y_pred.extend(torch.<span class="built_in">max</span>(batch_outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().tolist())</span><br><span class="line">            y_true.extend(batch_labels.cpu().numpy().tolist())</span><br><span class="line">            <span class="comment"># 梯度裁剪</span></span><br><span class="line">            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)</span><br><span class="line">            <span class="keyword">for</span> optimizer, scheduler <span class="keyword">in</span> <span class="built_in">zip</span>(self.optimizer.optims, self.optimizer.schedulers):</span><br><span class="line">                optimizer.step()</span><br><span class="line">                scheduler.step()</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            self.step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">                elapsed = time.time() - start_time</span><br><span class="line"></span><br><span class="line">                lrs = self.optimizer.get_lr()</span><br><span class="line">                self.log.logger.info(</span><br><span class="line">                    <span class="string">&#x27;| epoch &#123;:3d&#125; | step &#123;:3d&#125; | batch &#123;:3d&#125;/&#123;:3d&#125; | lr&#123;&#125; | loss &#123;:.4f&#125; | s/batch &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                        epoch, self.step, batch_idx, self.batch_num, lrs,</span><br><span class="line">                        losses / log_interval,</span><br><span class="line">                        elapsed / log_interval))</span><br><span class="line"></span><br><span class="line">                losses = <span class="number">0</span></span><br><span class="line">                start_time = time.time()</span><br><span class="line"></span><br><span class="line">            batch_idx += <span class="number">1</span></span><br><span class="line">            pbar.update()</span><br><span class="line"></span><br><span class="line">        overall_losses /= self.batch_num</span><br><span class="line">        during_time = time.time() - epoch_start_time</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reformat 保留 4 位数字</span></span><br><span class="line">        overall_losses = reformat(overall_losses, <span class="number">4</span>)</span><br><span class="line">        score, f1 = get_score(y_true, y_pred)</span><br><span class="line"></span><br><span class="line">        self.log.logger.info(</span><br><span class="line">            <span class="string">&#x27;| epoch &#123;:3d&#125; | score &#123;&#125; | f1 &#123;&#125; | loss &#123;:.4f&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, score, f1,</span><br><span class="line">                                                                                  overall_losses, during_time))</span><br><span class="line">        <span class="comment"># 如果预测和真实的标签都包含相同的类别数目，才能调用 classification_report</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">set</span>(y_true) == <span class="built_in">set</span>(y_pred) <span class="keyword">and</span> self.report:</span><br><span class="line">            report = classification_report(y_true, y_pred, digits=<span class="number">4</span>, target_names=self.target_names)</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;\n&#x27;</span> + report)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里验证集、测试集都使用这个函数，通过 test 来区分使用哪个数据集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_eval</span>(<span class="params">self, epoch, test=<span class="number">1</span></span>):</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        <span class="keyword">if</span> test==<span class="number">1</span>:</span><br><span class="line">            data=self.dev_data</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Start testing(dev)...&#x27;</span>)</span><br><span class="line">        <span class="keyword">elif</span> test==<span class="number">2</span>:</span><br><span class="line">            data = self.test_data</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Start testing(test)...&#x27;</span>)</span><br><span class="line">        <span class="keyword">elif</span> test ==<span class="number">3</span>:</span><br><span class="line">            data = self.final_test_data</span><br><span class="line">            self.log.logger.info(<span class="string">&#x27;Start predicting...&#x27;</span>)</span><br><span class="line">        y_pred = []</span><br><span class="line">        y_true = []</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> batch_data <span class="keyword">in</span> data_iter(data, test_batch_size, shuffle=<span class="literal">False</span>):</span><br><span class="line">                torch.cuda.empty_cache()</span><br><span class="line">                            <span class="comment"># batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)</span></span><br><span class="line">            <span class="comment"># 形状都是：batch_size * doc_len * sent_len</span></span><br><span class="line">            <span class="comment"># batch_labels: batch_size</span></span><br><span class="line">                batch_inputs, batch_labels = self.batch2tensor(batch_data)</span><br><span class="line">                <span class="comment"># batch_outputs：b * num_labels</span></span><br><span class="line">                batch_outputs = self.model(batch_inputs)</span><br><span class="line">                <span class="comment"># 把预测值转换为一维，方便下面做 classification_report，计算 f1</span></span><br><span class="line">                y_pred.extend(torch.<span class="built_in">max</span>(batch_outputs, dim=<span class="number">1</span>)[<span class="number">1</span>].cpu().numpy().tolist())</span><br><span class="line">                y_true.extend(batch_labels.cpu().numpy().tolist())</span><br><span class="line"></span><br><span class="line">            score, f1 = get_score(y_true, y_pred)</span><br><span class="line"></span><br><span class="line">            during_time = time.time() - start_time</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> test==<span class="number">3</span>:</span><br><span class="line">                df = pd.DataFrame(&#123;<span class="string">&#x27;label&#x27;</span>: y_pred&#125;)</span><br><span class="line">                df.to_csv(save_test, index=<span class="literal">False</span>, sep=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.log.logger.info(</span><br><span class="line">                    <span class="string">&#x27;| epoch &#123;:3d&#125; | dev | score &#123;&#125; | f1 &#123;&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, score, f1,</span><br><span class="line">                                                                              during_time))</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">set</span>(y_true) == <span class="built_in">set</span>(y_pred) <span class="keyword">and</span> self.report:</span><br><span class="line">                    report = classification_report(y_true, y_pred, digits=<span class="number">4</span>, target_names=self.target_names)</span><br><span class="line">                    self.log.logger.info(<span class="string">&#x27;\n&#x27;</span> + report)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> f1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># data 参数就是 get_examples() 得到的，经过了分 batch</span></span><br><span class="line">    <span class="comment"># batch_data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line">    <span class="comment"># 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch2tensor</span>(<span class="params">self, batch_data</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        batch_size = <span class="built_in">len</span>(batch_data)</span><br><span class="line">        doc_labels = []</span><br><span class="line">        doc_lens = []</span><br><span class="line">        doc_max_sent_len = []</span><br><span class="line">        <span class="keyword">for</span> doc_data <span class="keyword">in</span> batch_data:</span><br><span class="line">            <span class="comment"># doc_data 代表一篇新闻，是一个 tuple: (label, 句子数量，doc)</span></span><br><span class="line">            <span class="comment"># doc_data[0] 是 label</span></span><br><span class="line">            doc_labels.append(doc_data[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># doc_data[1] 是 这篇文章的句子数量</span></span><br><span class="line">            doc_lens.append(doc_data[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># doc_data[2] 是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, extword_ids)</span></span><br><span class="line">            <span class="comment"># 所以 sent_data[0] 表示每个句子的长度（单词个数）</span></span><br><span class="line">            sent_lens = [sent_data[<span class="number">0</span>] <span class="keyword">for</span> sent_data <span class="keyword">in</span> doc_data[<span class="number">2</span>]]</span><br><span class="line">            <span class="comment"># 取出这篇新闻中最长的句子长度（单词个数）</span></span><br><span class="line">            max_sent_len = <span class="built_in">max</span>(sent_lens)</span><br><span class="line">            doc_max_sent_len.append(max_sent_len)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 取出最长的句子数量</span></span><br><span class="line">        max_doc_len = <span class="built_in">max</span>(doc_lens)</span><br><span class="line">        <span class="comment"># 取出这批 batch 数据中最长的句子长度（单词个数）</span></span><br><span class="line">        max_sent_len = <span class="built_in">max</span>(doc_max_sent_len)</span><br><span class="line">        <span class="comment"># 创建 数据</span></span><br><span class="line">        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)</span><br><span class="line">        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)</span><br><span class="line">        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)</span><br><span class="line">        batch_labels = torch.LongTensor(doc_labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            <span class="keyword">for</span> sent_idx <span class="keyword">in</span> <span class="built_in">range</span>(doc_lens[b]):</span><br><span class="line">                <span class="comment"># batch_data[b][2] 表示一个 list，是一篇文章中的句子</span></span><br><span class="line">                sent_data = batch_data[b][<span class="number">2</span>][sent_idx] <span class="comment">#sent_data 表示一个句子</span></span><br><span class="line">                <span class="keyword">for</span> word_idx <span class="keyword">in</span> <span class="built_in">range</span>(sent_data[<span class="number">0</span>]): <span class="comment"># sent_data[0] 是句子长度(单词数量)</span></span><br><span class="line">                    <span class="comment"># sent_data[1] 表示 word_ids</span></span><br><span class="line">                    batch_inputs1[b, sent_idx, word_idx] = sent_data[<span class="number">1</span>][word_idx]</span><br><span class="line">                    <span class="comment"># # sent_data[2] 表示 extword_ids</span></span><br><span class="line">                    batch_inputs2[b, sent_idx, word_idx] = sent_data[<span class="number">2</span>][word_idx]</span><br><span class="line">                    <span class="comment"># mask 表示 哪个位置是有词，后面计算 attention 时，没有词的地方会被置为 0</span></span><br><span class="line">                    batch_masks[b, sent_idx, word_idx] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            batch_inputs1 = batch_inputs1.to(device)</span><br><span class="line">            batch_inputs2 = batch_inputs2.to(device)</span><br><span class="line">            batch_masks = batch_masks.to(device)</span><br><span class="line">            batch_labels = batch_labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (batch_inputs1, batch_inputs2, batch_masks), batch_labels</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="主函数">主函数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log=Logger(mode=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">log.logger.info(<span class="string">&quot;Dataset has built.&quot;</span>)</span><br><span class="line"><span class="comment">#构建词典</span></span><br><span class="line">vocab=Vocab(train_data)</span><br><span class="line">log.logger.info(<span class="string">&quot;Vocab has built.&quot;</span>)</span><br><span class="line"></span><br><span class="line">log.logger.info(<span class="string">&quot;Creating Model.&quot;</span>)</span><br><span class="line"><span class="comment">#创建模型</span></span><br><span class="line">model=Model(log,vocab)</span><br><span class="line">log.logger.info(<span class="string">&quot;Use cuda: %s, gpu id: %d.&quot;</span>, use_cuda, gpu)</span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">trainer = Trainer(log,model, vocab,train_data,dev_data,test_data,final_test_data)</span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">trainer.test(flag=<span class="number">2</span>)</span><br><span class="line">trainer.test(flag=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="运行结果">运行结果</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2021-10-16 00:13:20,628 - train.py[line:136] - INFO: | epoch  10 | step 5747 | batch  50/633 | lr 0.00005 | loss 0.2194 | s/batch 5.05</span><br><span class="line">2021-10-16 00:17:49,770 - train.py[line:136] - INFO: | epoch  10 | step 5797 | batch 100/633 | lr 0.00005 | loss 0.2107 | s/batch 5.38</span><br><span class="line">2021-10-16 00:22:03,411 - train.py[line:136] - INFO: | epoch  10 | step 5847 | batch 150/633 | lr 0.00005 | loss 0.2299 | s/batch 5.07</span><br><span class="line">2021-10-16 00:26:03,440 - train.py[line:136] - INFO: | epoch  10 | step 5897 | batch 200/633 | lr 0.00005 | loss 0.2170 | s/batch 4.80</span><br><span class="line">2021-10-16 00:30:41,771 - train.py[line:136] - INFO: | epoch  10 | step 5947 | batch 250/633 | lr 0.00005 | loss 0.2178 | s/batch 5.57</span><br><span class="line">2021-10-16 00:35:08,509 - train.py[line:136] - INFO: | epoch  10 | step 5997 | batch 300/633 | lr 0.00005 | loss 0.2189 | s/batch 5.33</span><br><span class="line">2021-10-16 00:39:21,883 - train.py[line:136] - INFO: | epoch  10 | step 6047 | batch 350/633 | lr 0.00004 | loss 0.2196 | s/batch 5.07</span><br><span class="line">2021-10-16 00:43:11,833 - train.py[line:136] - INFO: | epoch  10 | step 6097 | batch 400/633 | lr 0.00004 | loss 0.2100 | s/batch 4.60</span><br><span class="line">2021-10-16 00:47:09,737 - train.py[line:136] - INFO: | epoch  10 | step 6147 | batch 450/633 | lr 0.00004 | loss 0.1975 | s/batch 4.76</span><br><span class="line">2021-10-16 00:51:06,759 - train.py[line:136] - INFO: | epoch  10 | step 6197 | batch 500/633 | lr 0.00004 | loss 0.2076 | s/batch 4.74</span><br><span class="line">2021-10-16 00:55:23,886 - train.py[line:136] - INFO: | epoch  10 | step 6247 | batch 550/633 | lr 0.00004 | loss 0.2091 | s/batch 5.14</span><br><span class="line">2021-10-16 00:59:17,891 - train.py[line:136] - INFO: | epoch  10 | step 6297 | batch 600/633 | lr 0.00004 | loss 0.1987 | s/batch 4.68</span><br><span class="line">2021-10-16 01:01:58,372 - train.py[line:155] - INFO: | epoch  10 | score (92.32, 91.34, 91.82) | f1 91.82 | loss 0.2124 | time 3169.87</span><br><span class="line">2021-10-16 01:01:58,631 - train.py[line:161] - INFO: </span><br><span class="line">			precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          科技     0.9311    0.9330    0.9320     31524</span><br><span class="line">          股票     0.9390    0.9467    0.9428     29926</span><br><span class="line">          体育     0.9843    0.9846    0.9844     25454</span><br><span class="line">          娱乐     0.9420    0.9530    0.9475     17928</span><br><span class="line">          时政     0.8763    0.8996    0.8878     12163</span><br><span class="line">          社会     0.8704    0.8652    0.8678      9908</span><br><span class="line">          教育     0.9354    0.9263    0.9309      8088</span><br><span class="line">          财经     0.8753    0.8295    0.8518      7161</span><br><span class="line">          家居     0.9078    0.9045    0.9061      6356</span><br><span class="line">          游戏     0.9183    0.8899    0.9039      4761</span><br><span class="line">          房产     0.9827    0.9716    0.9772      3985</span><br><span class="line">          时尚     0.8955    0.8782    0.8867      2536</span><br><span class="line">          彩票     0.9395    0.9051    0.9220      1475</span><br><span class="line">          星座     0.9272    0.9007    0.9137       735</span><br><span class="line"></span><br><span class="line">    accuracy                         0.9316    162000</span><br><span class="line">   macro avg     0.9232    0.9134    0.9182    162000</span><br><span class="line">weighted avg     0.9315    0.9316    0.9315    162000</span><br><span class="line"></span><br><span class="line">2021-10-16 01:01:58,634 - train.py[line:171] - INFO: Start testing(dev)...</span><br><span class="line">2021-10-16 01:07:47,322 - train.py[line:201] - INFO: | epoch  10 | dev | score (93.68, 93.08, 93.36) | f1 93.36 | time 348.69</span><br><span class="line">2021-10-16 01:07:47,352 - train.py[line:206] - INFO: </span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          科技     0.9312    0.9546    0.9428      3502</span><br><span class="line">          股票     0.9618    0.9456    0.9536      3325</span><br><span class="line">          体育     0.9838    0.9866    0.9852      2828</span><br><span class="line">          娱乐     0.9540    0.9583    0.9562      1992</span><br><span class="line">          时政     0.8670    0.9267    0.8959      1351</span><br><span class="line">          社会     0.8862    0.8556    0.8706      1101</span><br><span class="line">          教育     0.9421    0.9232    0.9326       899</span><br><span class="line">          财经     0.9133    0.8869    0.8999       796</span><br><span class="line">          家居     0.9508    0.9037    0.9267       706</span><br><span class="line">          游戏     0.9576    0.8960    0.9258       529</span><br><span class="line">          房产     0.9932    0.9910    0.9921       443</span><br><span class="line">          时尚     0.9204    0.9433    0.9317       282</span><br><span class="line">          彩票     0.9387    0.9329    0.9358       164</span><br><span class="line">          星座     0.9157    0.9268    0.9212        82</span><br><span class="line"></span><br><span class="line">    accuracy                         0.9423     18000</span><br><span class="line">   macro avg     0.9368    0.9308    0.9336     18000</span><br><span class="line">weighted avg     0.9427    0.9423    0.9423     18000</span><br><span class="line"></span><br><span class="line">2021-10-16 01:07:47,353 - train.py[line:65] - INFO: Exceed history dev = 93.22, current dev = 93.36</span><br><span class="line">2021-10-16 16:05:00,917 - train.py[line:180] - INFO: Start testing(test)...</span><br><span class="line">2021-10-16 16:11:41,817 INFO: | epoch  11 | dev | score (93.68, 93.29, 93.45) | f1 93.45 | time 400.90</span><br><span class="line">2021-10-16 16:11:41,817 - train.py[line:207] - INFO: | epoch  11 | dev | score (93.68, 93.29, 93.45) | f1 93.45 | time 400.90</span><br><span class="line">2021-10-16 16:11:41,849 INFO: </span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          科技     0.9325    0.9514    0.9419      3892</span><br><span class="line">          股票     0.9541    0.9461    0.9501      3694</span><br><span class="line">          体育     0.9879    0.9876    0.9877      3143</span><br><span class="line">          娱乐     0.9554    0.9575    0.9564      2213</span><br><span class="line">          时政     0.8804    0.9361    0.9074      1502</span><br><span class="line">          社会     0.8826    0.8733    0.8779      1223</span><br><span class="line">          教育     0.9470    0.9309    0.9389       998</span><br><span class="line">          财经     0.9056    0.8360    0.8694       884</span><br><span class="line">          家居     0.9501    0.9223    0.9360       785</span><br><span class="line">          游戏     0.9711    0.9133    0.9413       588</span><br><span class="line">          房产     0.9899    0.9959    0.9929       492</span><br><span class="line">          时尚     0.9214    0.9361    0.9287       313</span><br><span class="line">          彩票     0.9602    0.9286    0.9441       182</span><br><span class="line">          星座     0.8776    0.9451    0.9101        91</span><br><span class="line"></span><br><span class="line">    accuracy                         0.9431     20000</span><br><span class="line">   macro avg     0.9368    0.9329    0.9345     20000</span><br><span class="line">weighted avg     0.9434    0.9431    0.9430     20000</span><br></pre></td></tr></table></figure>
<p>最终在线上的成绩为：0.9324。</p>
<h2 id="参考资料">参考资料</h2>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40276005">自然语言中的CNN--TextCNN（基础篇） - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/albert.html">ALBERT — transformers 4.11.3 documentation (huggingface.co)</a></p>
<p><a href="https://ifwind.github.io/2021/08/22/BERT相关——（5）Pre-train%20Model/#重新配置模型">BERT相关——（5）Pre-train Model | 冬于的博客 (ifwind.github.io)</a></p>
<p><a href="https://ifwind.github.io/2021/08/26/BERT实战——（1）文本分类/#定义评估方法">BERT实战——（1）文本分类 | 冬于的博客 (ifwind.github.io)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/az9996/article/details/109219652">阅读源码-理解pytorch_pretrained_bert中BertTokenizer工作方式_枪枪枪的博客-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jb51.net/article/156111.htm">详解Python logging调用Logger.info方法的处理过程_python_脚本之家 (jb51.net)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xianyulouie/p/11041777.html">python中logging日志模块详解 - 咸鱼也是有梦想的 - 博客园 (cnblogs.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/fc3b80a64fa8">NLP学习1 - 使用Huggingface Transformers框架从头训练语言模型 - 简书 (jianshu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/231180925?spm=5176.21852664.0.0.73c93248P19kNq">零基础入门NLP-阿里云tianchi新闻文本分类大赛rank4分享-代码+经验/Huggingface Bert tutorial - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/183862056?spm=5176.21852664.0.0.41663dd7s5zbnp">阿里天池 NLP 入门赛 TextCNN 方案代码详细注释和流程讲解 - 知乎 (zhihu.com)</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">冬于</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ifwind.github.io/2021/10/13/Task4-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.2-Word2Vec+TextCNN%E5%88%86%E7%B1%BB/">https://ifwind.github.io/2021/10/13/Task4-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.2-Word2Vec+TextCNN%E5%88%86%E7%B1%BB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ifwind.github.io" target="_blank">冬于的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a><a class="post-meta__tags" href="/tags/Word2Vec/">Word2Vec</a><a class="post-meta__tags" href="/tags/TextCNN/">TextCNN</a><a class="post-meta__tags" href="/tags/BiLSTM/">BiLSTM</a><a class="post-meta__tags" href="/tags/Attention/">Attention</a></div><div class="post_share"><div class="social-share" data-image="/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/11/Task4-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.1-%E5%88%A9%E7%94%A8Word2Vec%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F/"><img class="next-cover" src="/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Task4-基于深度学习的文本分类2-Word2Vec</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/10/11/Task4-基于深度学习的文本分类1-FastText/" title="Task4-基于深度学习的文本分类1-FastText"><img class="cover" src="/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-11</div><div class="title">Task4-基于深度学习的文本分类1-FastText</div></div></a></div><div><a href="/2021/10/11/Task4-基于深度学习的文本分类2.1-利用Word2Vec训练词向量/" title="Task4-基于深度学习的文本分类2-Word2Vec"><img class="cover" src="/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-11</div><div class="title">Task4-基于深度学习的文本分类2-Word2Vec</div></div></a></div><div><a href="/2021/09/29/Task1-Task2数据集加载和分析/" title="Task3-基于机器学习的文本分类"><img class="cover" src="/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-29</div><div class="title">Task3-基于机器学习的文本分类</div></div></a></div><div><a href="/2021/10/04/Task3-基于机器学习的文本分类/" title="Task3-基于机器学习的文本分类"><img class="cover" src="/2021/10/04/Task3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/text%20classification.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-04</div><div class="title">Task3-基于机器学习的文本分类</div></div></a></div><div><a href="/2021/08/15/Transformer相关——（1）Encoder-Decoder框架/" title="Transformer相关——（1）Encoder-Decoder框架"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-15</div><div class="title">Transformer相关——（1）Encoder-Decoder框架</div></div></a></div><div><a href="/2021/08/16/Transformer相关——（2）Seq2Seq模型/" title="Transformer相关——（2）Seq2Seq模型"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-16</div><div class="title">Transformer相关——（2）Seq2Seq模型</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#task4-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.2-word2vectextcnnbilstmattention%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">Task4 基于深度学习的文本分类2.2-Word2Vec+TextCNN+BiLSTM+Attention分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.1.</span> <span class="toc-text">模型代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#wordcnnencoder"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">WordCNNEncoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sentencoder"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">SentEncoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#attention"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">完整模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%8F%8A%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">数据加载及预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BAbatch"><span class="toc-number">1.3.</span> <span class="toc-text">构建batch</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get_examples%E5%92%8Csentence_split"><span class="toc-number">1.3.1.</span> <span class="toc-text">get_examples和sentence_split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#data_iter%E5%92%8Cbatch_slice"><span class="toc-number">1.3.2.</span> <span class="toc-text">data_iter和batch_slice</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vocab%E7%B1%BB"><span class="toc-number">1.4.</span> <span class="toc-text">Vocab类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E8%AF%8D%E5%85%B8-build_vocab"><span class="toc-number">1.4.1.</span> <span class="toc-text">构造词典-build_vocab</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%B9%B6%E5%8C%85%E8%A3%85word2vec%E7%9A%84embedding-load_pretrained_embs"><span class="toc-number">1.4.2.</span> <span class="toc-text">加载并包装Word2vec的embedding-load_pretrained_embs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.5.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">1.6.</span> <span class="toc-text">评价指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95"><span class="toc-number">1.7.</span> <span class="toc-text">日志记录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#trainer"><span class="toc-number">1.8.</span> <span class="toc-text">Trainer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.9.</span> <span class="toc-text">主函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.10.</span> <span class="toc-text">运行结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.11.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 冬于</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
      appKey: 'YKdl4HKX9W6jLSdPlypgEtDM',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: 'https://e4lpmfet.lc-cn-n1-shared.com',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.17.0/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://e4lpmfet.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
        "X-LC-Key": 'YKdl4HKX9W6jLSdPlypgEtDM',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>