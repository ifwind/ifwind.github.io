<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>李宏毅深度学习2021春p5-9：神经网络训练技巧 | 冬于的博客</title><meta name="keywords" content="深度学习,训练"><meta name="author" content="冬于"><meta name="copyright" content="冬于"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="李宏毅深度学习2021春p5-9：神经网络训练技巧训练遇到的问题 参数不断的更新,training loss一开始下降，然后不会再下降，但距离0还有很远的gap； 一开始model就train不起来，不管怎么update参数，loss一直比较大。   导致上述问题的原因可能有很多，我们先回忆一下梯度下降算法在现实世界中面临的挑战：  问题1：局部最优（Stuck at local minima）">
<meta property="og:type" content="article">
<meta property="og:title" content="李宏毅深度学习2021春p5-9：神经网络训练技巧">
<meta property="og:url" content="https://ifwind.github.io/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/index.html">
<meta property="og:site_name" content="冬于的博客">
<meta property="og:description" content="李宏毅深度学习2021春p5-9：神经网络训练技巧训练遇到的问题 参数不断的更新,training loss一开始下降，然后不会再下降，但距离0还有很远的gap； 一开始model就train不起来，不管怎么update参数，loss一直比较大。   导致上述问题的原因可能有很多，我们先回忆一下梯度下降算法在现实世界中面临的挑战：  问题1：局部最优（Stuck at local minima）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ifwind.github.io/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%81%E9%9D%A2.jpg">
<meta property="article:published_time" content="2021-09-22T11:07:19.000Z">
<meta property="article:modified_time" content="2021-09-23T08:08:27.102Z">
<meta property="article:author" content="冬于">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ifwind.github.io/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%81%E9%9D%A2.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ifwind.github.io/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"LVK1P2D38K","apiKey":"8cbbb0bcbb5c7448f68b4fae01d4ccd5","indexName":"DongYu","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '李宏毅深度学习2021春p5-9：神经网络训练技巧',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-09-23 16:08:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">89</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">54</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%81%E9%9D%A2.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">冬于的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 分类</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Spark</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-duotone fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">李宏毅深度学习2021春p5-9：神经网络训练技巧</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-22T11:07:19.000Z" title="发表于 2021-09-22 19:07:19">2021-09-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-23T08:08:27.102Z" title="更新于 2021-09-23 16:08:27">2021-09-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8,348</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>31分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="李宏毅深度学习2021春p5-9：神经网络训练技巧"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="李宏毅深度学习2021春p5-9：神经网络训练技巧"><a href="#李宏毅深度学习2021春p5-9：神经网络训练技巧" class="headerlink" title="李宏毅深度学习2021春p5-9：神经网络训练技巧"></a>李宏毅深度学习2021春p5-9：神经网络训练技巧</h1><h2 id="训练遇到的问题"><a href="#训练遇到的问题" class="headerlink" title="训练遇到的问题"></a>训练遇到的问题</h2><ol>
<li>参数不断的更新,training loss一开始下降，然后不会再下降，但距离0还有很远的gap；</li>
<li>一开始model就train不起来，不管怎么update参数，loss一直比较大。</li>
</ol>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314152447908.png" alt></p>
<p>导致上述问题的原因可能有很多，我们先回忆一下梯度下降算法在现实世界中面临的挑战：</p>
<ul>
<li>问题1：局部最优（Stuck at local minima）</li>
<li>问题2：等于0（Stuck at saddle point）</li>
<li>问题3：趋近于0（Very slow at the plateau）</li>
</ul>
<p><img src="https://datawhalechina.github.io/leeml-notes/chapter3/res/chapter3-13.png" alt></p>
<p>像这种gradient为0的点，统称critical point，我们先从问题1和问题2来看看如何“炼丹”。</p>
<h2 id="局部最小值local-minima和鞍点saddle-point"><a href="#局部最小值local-minima和鞍点saddle-point" class="headerlink" title="局部最小值local minima和鞍点saddle point"></a>局部最小值local minima和鞍点saddle point</h2><h3 id="Critical-Point"><a href="#Critical-Point" class="headerlink" title="Critical Point"></a>Critical Point</h3><p>gradient为0的点。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314153005913.png" alt></p>
<h4 id="local-minima"><a href="#local-minima" class="headerlink" title="local minima"></a>local minima</h4><p><strong>现在所在的位置已经是局部loss最低的点，</strong>往四周走 loss都会比较高，可能没有路可以走。</p>
<h4 id="saddle-point"><a href="#saddle-point" class="headerlink" title="saddle point"></a>saddle point</h4><p><strong>saddle point从某个方向还是有可能到达loss更低的位置，</strong>只要逃离saddle point，就有可能让loss更低。</p>
<h3 id="如何判断某个位置是local-minima还是saddle-point？"><a href="#如何判断某个位置是local-minima还是saddle-point？" class="headerlink" title="如何判断某个位置是local minima还是saddle point？"></a>如何判断某个位置是local minima还是saddle point？</h3><p>通过泰勒级数展开估计（Tayler Series Approximation）loss function的形状。</p>
<p>也就是，虽然无法完整、准确写出$L(\theta)$，但如果给定某一组参数$\theta’$，在$\theta’$附近的loss function可以通过泰勒级数展开来估计：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314155802228.png" alt></p>
<ul>
<li>第一项$L(\theta’)$：当$\theta$跟$\theta’$很近的时候,$L(\theta)$跟$L(\theta’)$比较靠近，但还有一些差距；</li>
<li>第二项$(\theta-\theta’)^Tg$：<strong>是一个向量,这个$g$是gradient</strong>，这个<strong>gradient会来弥补$\theta’$跟$\theta$之间的差距</strong>。有时候gradient会写成$\nabla L(\theta’)$，<strong>它的第$i$个component,就是$θ$的第$i$个component对$L$的微分</strong>，加上这一项之后仍然还有差距；</li>
<li>第三项中$(\theta-\theta’)^TH(\theta-\theta’)$：其中$H$跟Hessian有关，是一个矩阵，第三项会再补足与真正的L(θ)之间的差距。<strong>$H$是L的二次微分构成的矩阵</strong>,<strong>它第$i$个row,第$j$个column的值$H_{ij}$，是把$θ$的第$i$个component,对$L$作微分,再把$θ$的第$j$个component,对$L$作微分,也就是做两次微分以后的结果</strong> 。</li>
</ul>
<p>总的来说，$L(\theta)$跟两个东西有关,<strong>跟gradient有关，跟hessian有关。gradient就是一次微分,hessian是内含二次微分的项目</strong>。</p>
<p>如果我们今天走到了一个critical point，意味着上式中$g=0$，只剩下$L(\theta’)$和红色的这一项：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314160538203.png" alt></p>
<p>于是可以<strong>通过红色这一项判断$\theta’$附近的error surface长什么样，从而判断现在是在local minima、local max还是saddle point。</strong></p>
<h4 id="通过Hession矩阵判断-theta’-附近的error-surface"><a href="#通过Hession矩阵判断-theta’-附近的error-surface" class="headerlink" title="通过Hession矩阵判断$\theta’$附近的error surface"></a>通过Hession矩阵判断$\theta’$附近的error surface</h4><p>把$(\theta-\theta’)$用向量$v$来表示，根据$v^THv$的值来判断：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314161411744.png" alt></p>
<p>线性代数中，如果所有的$v$带入$v^THv$的值都大於零，那$H$叫做<strong>positive definite 正定矩阵</strong>。所以我们不需要通过穷举所有的点来判断$v^THv$是大于零还是小于零，而是直接利用$H$是否正定来判断。而判断$H$是否是正定矩阵可以通过求解$H$的特征值来判断。如果<strong>所有的eigen value特征值都是正的</strong>，那么$H$就是<strong>positive definite 正定矩阵</strong>。</p>
<p>所以判断条件就转化为：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314182013101.png" alt></p>
<h3 id="如何逃离saddle-point？"><a href="#如何逃离saddle-point？" class="headerlink" title="如何逃离saddle point？"></a>如何逃离saddle point？</h3><p><strong>$H$不只可以帮助我们判断,现在是不是在一个saddle point,还指出了参数可以update的方向。</strong>注意这个时候$g=0$。</p>
<p>根据$\lambda x=Ax$，可以对式子进行转化：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314200048825.png" alt></p>
<p>于是如果$λ&lt;0$（eigen value&lt;0）,那$λ‖u‖²&lt;0$，所以eigen value是负的,那这一整项就会是<strong>负的</strong>,也就是$u^THu$是负的，也就是<strong>红色整项是负的</strong>，于是$L(\theta)&lt;L(\theta’)$。也就是说令$\theta-\theta’=\mu$，<strong>在$θ’$的位置加上$\mu$,沿$\mu$的方向做update得到$θ$,就可以让loss变小</strong>。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314203757805.png" alt></p>
<p><strong>这个方法也有一点问题：$H$的运算量非常非常的大，还需要算其特征值和特征向量，运算量惊人，实际操作中还有其他方法可以逃离saddle point，在最糟糕的情况下还有这种方法可以逃离。</strong></p>
<h3 id="Saddle-Point-v-s-Local-Minima"><a href="#Saddle-Point-v-s-Local-Minima" class="headerlink" title="Saddle Point v.s. Local Minima"></a>Saddle Point v.s. Local Minima</h3><p><strong>事实上Local Minima没有那么常见。</strong>一个可能的解释是：在低维的空间中,低维的一个参数的error surface,好像到处都是local minima,但是在高维空间来看,它可能只是一个saddle point。</p>
<p>如下图所示，几乎找不到完全所有eigen value都是正的critical point。下图这个例子种，minimum ratio代表正的eigen value的数目占总数的比例，最大也在0.5~0.6,代表只有一半的eigen value是正的,还有一半的eigen value是负的。</p>
<p>在这个图上,越往右代表critical point越像local minima,<strong>但是它们都没有真的,变成local minima</strong>。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210314205517453.png" alt></p>
<h2 id="batch-mini-batch和动量Momentum"><a href="#batch-mini-batch和动量Momentum" class="headerlink" title="batch/mini-batch和动量Momentum"></a>batch/mini-batch和动量Momentum</h2><h3 id="batch-mini-batch"><a href="#batch-mini-batch" class="headerlink" title="batch/mini-batch"></a>batch/mini-batch</h3><h4 id="Optimization-with-Batch"><a href="#Optimization-with-Batch" class="headerlink" title="Optimization with Batch"></a>Optimization with Batch</h4><p>每次<strong>在 Update 参数的时候，拿一个batch出来,算个 Loss,算个 Gradient,Update 参数</strong>,然后再拿另外个batch,再算个 Loss,算gradient，更新参数，以此类推。</p>
<p>mini-batch就是不把所有训练数据拿出来一起算loss，而是分小块。</p>
<p><strong>所有的 Batch 训练过一遍,叫做一个 Epoch。</strong></p>
<p>在生成batch的时候长春会做shuffle。</p>
<p>Shuffle 有很多不同的做法，常见的一个做法是<strong>在每一个 Epoch 开始之前,会分一次 Batch，每一个 Epoch 的 Batch 都不一样</strong>。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315142626597.png" alt></p>
<h4 id="为什么要batch"><a href="#为什么要batch" class="headerlink" title="为什么要batch"></a>为什么要batch</h4><p>直接上对比图：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315164405953.png" alt></p>
<p>之前提到，更大的batch在看完更多的example后才会更新一次参数，但更新方向可能比小batch更准确（powerful，而小batch可能更noisy）如果都是串行的话，看上去更大的batch的参数更新速度更慢。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315145816823.png" alt></p>
<p>但实际上，在<strong>有并行计算条件下，比较大的 Batch Size算 Loss再进而算 Gradient所需要的时间,不一定比小的 Batch Size 要花的时间长</strong>。<strong>而更小的batch一个epoch的时间更长。</strong>（一个用于MINIST数据集上的实验如下图。）</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315151645146.png" alt></p>
<p>所以看上去big batch的时间劣势消失了，那是不是big batch更好呢？</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315152534804.png" alt></p>
<p>答案是否定的，神奇的地方是 <strong>Noisy 的 Gradient,反而可以帮助 Training</strong>。</p>
<h5 id="同一个model，batchsize过大效果反而更差？"><a href="#同一个model，batchsize过大效果反而更差？" class="headerlink" title="同一个model，batchsize过大效果反而更差？"></a>同一个model，batchsize过大效果反而更差？</h5><p><strong>用过大的batch size optimizer可能会有问题。</strong></p>
<p>拿不同的 Batch 来训练模型，可能会得到下图的结果：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315153147903.png" alt></p>
<p>Batch Size 越大,Validation Acc 上的结果越差。<strong>但这个不是 Overfitting，因为如果 Training也是 Batch Size 越大，Training 的结果越差。</strong></p>
<p>现在用的是同一个模型，它们可以表示的 Function 就是一模一样的，所以这个不是 Model Bias 的问题。<strong>这个是 Optimization 的问题,代表当用大的 Batch Size 的时候, Optimization 可能会有问题，小的 Batch Size,Optimization 的结果反而是比较好的</strong>。</p>
<h5 id="为什么小的-Batch-Size在-Training-Set-上会得到比较好的结果？"><a href="#为什么小的-Batch-Size在-Training-Set-上会得到比较好的结果？" class="headerlink" title="为什么小的 Batch Size在 Training Set 上会得到比较好的结果？"></a>为什么小的 Batch Size在 Training Set 上会得到比较好的结果？</h5><p>一个可能的解释是这样子的，如下图所示：</p>
<ul>
<li>假设是 Full Batch，沿著一个 Loss Function 来 Update 参数，走到一个 Local Minima/saddle Point，显然就停下来了，Gradient 是零，可能就没办法更新参数了；</li>
<li>假如是 Small Batch 的话，因为<strong>每次挑一个 Batch 出来算它的 Loss，所以每一次 Update 你的参数的时候，用的 Loss Function 都是略有差异的。</strong>选到第一个 Batch 的时候,，是用 L1 来算 Gradient；选到第二个 Batch 的时候，用 L2 来算Gradient。<strong>假设用 L1 算 Gradient 的时候,发现 Gradient 是零，卡住了，但 L2 它的 Function 和 L1 又不一样，L2 就不一定会卡住。</strong>所以在某一个batch卡住了没关系，还是有办法 Training   Model，还是有办法让 Loss 变小。所以mini-batch这种 Noisy 的 Update 的方式，结果反而对 Training是有帮助的。</li>
</ul>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315155345489.png" alt></p>
<h5 id="另外一个神奇的事：小的batch在testing也有帮助"><a href="#另外一个神奇的事：小的batch在testing也有帮助" class="headerlink" title="另外一个神奇的事：小的batch在testing也有帮助"></a>另外一个神奇的事：小的batch在testing也有帮助</h5><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04836">On Large-Batch Training For Deep Learning,Generalization Gap And Sharp Minima</a>中，就做了这样一个实验：努力的调big- Batch 的 Learning Rate,然后想办法把big-Batch的训练模型,跟small-Batch 训练得得一样好，结果发现<strong>small-Batch在 Testing 的时候是比较好的</strong>。</p>
<p><strong>注意这个时候big-batch在训练集中表现和small-batch性能相似，但在测试集表现更差说明的问题才是over fitting。</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315160405510.png" alt></p>
<p>为什么会这样呢？文章也给了一个解释：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315161935349.png" alt></p>
<p>假设这个实线是 Training Loss，那在这个 Training Loss 上面，可能有很多个 Local Minima，这些 Local Minima 它们的 Loss 都很低，它们 Loss 可能都趋近于 0。但是 Local Minima还是有好 Minima 跟坏 Minima 之分。</p>
<ul>
<li>好的Local Minima：在一个平原上（左边的）；</li>
<li>坏的Local Minima：在一个峡谷里面（右边的）。</li>
</ul>
<p>为什么这么判定呢？</p>
<p>假设现<strong>在 Training 跟 Testing 中间,有一个 Mismatch</strong>，Training 的 Loss 跟 Testing 的 Loss,它们的Function 不一样。导致这种mismatch的原因可能有两个：</p>
<ol>
<li>可能是本来 Training 跟 Testing 的 Distribution就不一样；</li>
<li>那也有可能是因为 Training 跟 Testing都是从 Sample 的 Data 算出来的，也许 Training 跟 Testing Sample（采样）到的 Data 不一样，那它们算出来的 Loss,自然有一点差距。</li>
</ol>
<p>那我们就假设这个 Training 跟 Testing的差距就是把 Training 的 Loss,这个 Function 往右平移一点（上图虚线为testing loss）。这时候会发现，<strong>对左边这个在一个盆地的 Minima 来说，它的在 Training 跟 Testing 上面的结果,不会差太多；但是对右边这个在峡谷的 Minima 来说，就差别很大。</strong></p>
<p>它在这个 Training Set 上算出来的 Loss 很低，但是因为 Training 跟 Testing 之间的mismatch，所以 Testing 的时候，这个 Error Surface 变化后，算出来的 Loss 就变得很大。所以猜想这个<strong>大的 Batch Size，会让模型倾向于走到峡谷里面,而小的 Batch Size,倾向于让模型走到盆地裡面</strong>：</p>
<ul>
<li>因为小的 Batch有很多的 Loss，每次 Update 的方向都不太一样，所以如果这个峡谷非常地窄，可能一个不小心就跳出去了，因为每次 Update 的方向都不太一样，它的 Update 的方向也就随机性，所以一个很小的峡谷，没有办法困住小的 Batch。如果峡谷很小，它可能动一下就跳出去，之后如果有一个非常宽的盆地，它才会停下来；</li>
<li>而对于大的 Batch Size，就是顺着规定 Update，它就很有可能，走到一个比较小的峡谷里面。</li>
</ul>
<p><strong>但这只是一个解释，实际上这个还是一个尚待研究的问题</strong>。</p>
<h4 id="总结batch-mini-batch"><a href="#总结batch-mini-batch" class="headerlink" title="总结batch/mini-batch"></a>总结batch/mini-batch</h4><p>总的来说，大的 Batch 跟小的 Batch,它们各自有它们擅长的地方，<strong>Batch Size,变成另外一个需要去调整的 Hyperparameter。</strong>一些论文也讨论了如何兼顾鱼和熊掌，需要用一些特殊的方法来解决大的Batch Size 可能会带来的劣势：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315165345030.png" alt></p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum是另外一个有可能可以对抗 Saddle Point,或 Local Minima 的技术。</p>
<h4 id="传统的梯度下降"><a href="#传统的梯度下降" class="headerlink" title="传统的梯度下降"></a>传统的梯度下降</h4><p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315170131552.png" alt></p>
<h4 id="Gradient-Descent-Momentum"><a href="#Gradient-Descent-Momentum" class="headerlink" title="Gradient Descent + Momentum"></a>Gradient Descent + Momentum</h4><p>加上 Momentum 以后,每一次在移动参数的时候,不是只往 Gradient 的反方向来移动参数,是 <strong>Gradient 的反方向+前一步移动的方向去调整去到参数</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315171104120.png" alt></p>
<p>具体来看步骤如下：把蓝色的虚线加红色的虚线,前一步指示的方向跟 Gradient 指示的方向,当做参数下一步要移动的方向。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315173052976.png" alt></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315192909326.png" alt></p>
<p>下面有一个例子来展示：</p>
<p>在第三步，Gradient 变得很小，但是没关系，如果有 Momentum 的话，根据上一步的Momentum 可以继续往前走；</p>
<p>甚至走到第四步时，Gradient 表示应该要往左走了，但是如果前一步的影响力,比 Gradient 要大的话，还是有可能继续往右走,甚至翻过一个小丘,可能可以走到更好 Local Minima。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210315193703079.png" alt></p>
<h2 id="自动调整学习率Adaptive-Learning-Rate"><a href="#自动调整学习率Adaptive-Learning-Rate" class="headerlink" title="自动调整学习率Adaptive Learning Rate"></a>自动调整学习率Adaptive Learning Rate</h2><p>critical point不一定是训练过程中最大的阻碍，思考一个问题：</p>
<h3 id="loss不再下降的时候，gradient真的很小吗？"><a href="#loss不再下降的时候，gradient真的很小吗？" class="headerlink" title="loss不再下降的时候，gradient真的很小吗？"></a>loss不再下降的时候，gradient真的很小吗？</h3><p>并不是的。如下图所示<strong>虽然loss不再下降,但是这个gradient的大小并没有真的变得很小</strong>。它可能在error surface山谷的两个谷壁间,<strong>不断的来回的震荡</strong>。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319093237570.png" alt></p>
<p>所以有的时候训练不下去的原因不是critical point,而是其他的原因。</p>
<p>举个例子：</p>
<p>在下面这个error surface中，纵轴的参数变化很小就会导致结果变化很大，而横轴参数变化很大结果变化很小。</p>
<p>如果两个参数使用同一个学习率，当学习率调的过大，在纵轴方向可能直接震荡：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319100229839.png" alt></p>
<p>如果学习率调的很小，在纵轴可以逐渐找到好的参数，但是在横轴更新时，由于学习率过小，更新速度太慢，也很难找到最优解。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319100647667.png" alt></p>
<p><strong>所以可以考虑为不同的参数和不同的iteration设置不同的learning rate</strong>。</p>
<p>上面的案例展示了学习率选择的两个大原则：</p>
<ol>
<li><strong>如果在某一个方向上,gradient的值很小,非常的平坦,那learning rate调大一点；</strong></li>
<li><strong>如果在某一个方向上非常的陡峭,坡度很大,那其实learning rate可以设得小一点</strong>；</li>
</ol>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319103709570.png" alt></p>
<h3 id="learning-rate自动调整策略：为不同的参数和不同的iteration设置不同的learning-rate"><a href="#learning-rate自动调整策略：为不同的参数和不同的iteration设置不同的learning-rate" class="headerlink" title="learning rate自动调整策略：为不同的参数和不同的iteration设置不同的learning rate"></a>learning rate自动调整策略：为不同的参数和不同的iteration设置不同的learning rate</h3><p>以一个参数$\theta_i$为例：</p>
<p>原始策略：</p>
<script type="math/tex; mode=display">
\theta_i^{t+1}\leftarrow \theta_i^{t}-\eta g_i^t</script><p><strong>不同的参数和不同的iteration自动调整策略：</strong></p>
<script type="math/tex; mode=display">
\theta_i^{t+1}\leftarrow \theta_i^{t}-\frac{\eta}{\sigma_t^t} g_i^t</script><p>上式中$\sigma_i^t$的<strong>下标$i$代表它是depend on $i$的，也就是不同参数有不同的$\sigma$</strong>，而<strong>上标$t$表示它是iteration dependent的,不同的iteration会有不同的σ</strong>。因此$\frac{\eta}{\sigma_t^t}$变成parameter dependent的learning rate。</p>
<p>那么如何计算$\sigma$呢？</p>
<p><strong>常见的计算$σ$可以用gradient的Root Mean Square，这种计算方法也被用于Adagrad这一优化器中。</strong></p>
<h4 id="用于Adagrad中的Root-mean-square"><a href="#用于Adagrad中的Root-mean-square" class="headerlink" title="用于Adagrad中的Root mean square"></a>用于Adagrad中的Root mean square</h4><p>Root mean square计算$σ$是通过求历史梯度的均方根得到的，计算步骤如下图所示：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319150808494.png" alt></p>
<h5 id="为什么Root-mean-square可以做到坡度比较大的时候-learning-rate就减小-坡度比较小的时候-learning-rate就放大呢"><a href="#为什么Root-mean-square可以做到坡度比较大的时候-learning-rate就减小-坡度比较小的时候-learning-rate就放大呢" class="headerlink" title="为什么Root mean square可以做到坡度比较大的时候,learning rate就减小,坡度比较小的时候,learning rate就放大呢?"></a>为什么Root mean square可以做到坡度比较大的时候,learning rate就减小,坡度比较小的时候,learning rate就放大呢?</h5><p>看看下面这个例子：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319160639783.png" alt></p>
<p>现在我们有两个参数:$θᵢ¹$和$θᵢ²$ ，$θᵢ¹$坡度小 $θᵢ²$坡度大。</p>
<p><strong>$θᵢ¹$因为坡度小,所以在$θᵢ¹$这个参数上面,算出来的gradient值都比较小，那么$σ$也比较小，$\frac{\eta}{\sigma}$就比较大。</strong></p>
<p><strong>所以有了$σ$这一项以后,就可以随iteration gradient的不同,每一个参数的gradient的不同,来自动的调整learning rate的大小。</strong></p>
<p>但是，就算是同一个参数,它需要的learning rate,也可能会随时间而改变，于是还有进阶版的策略自适应learning rate。</p>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>同一个参数同一个方向也希望可以自适应调整，比如下面这个新月形的error surface。</p>
<p>在水平方向（同一参数同一方向）在绿色箭头这个地方坡度比较陡峭，需要比较小的learning rate，走到了中间这一段，到了红色箭头坡度又变得平滑，需要比较大的learning rate。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319211631968.png" alt></p>
<p>RMSProp和Apagrad在计算$\sigma$时，第一步时一样的，在第二步更新时，通过一个超参数$\alpha$赋予了历史梯度和当前梯度不一样的权重（而Apagrad中是一样的权重，也就是每一个gradient同样重要）。</p>
<p>计算过程如下图所示：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319212301760.png" alt></p>
<p><strong>α就像learning rate一样是一个hyperparameter，需要自己调整</strong>：</p>
<ul>
<li>如果<strong>α设很小趋近于0</strong>,就代表我觉得<strong>$gᵢ¹$相较于之前所算出来的gradient而言,比较重要</strong>；</li>
<li><strong>α设很大趋近于1</strong>,那就代表<strong>现在算出来的$gᵢ¹$比较不重要,之前算出来的gradient比较重要</strong>。</li>
</ul>
<p>举个例子看一下RMSProp的效果：</p>
<p>下图中，一开始梯度很小，学习率较大；</p>
<p>到第三步时，<strong>梯度变大，原来的Adagrad反应比较慢，可能还会用比较大的学习率</strong>，但如<strong>果用RMS Prop，把α设小一点，也就是让新的gradient影响比较大，可以很快的让σ的值变大，于是很快的让学习率变小</strong>。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319220231462.png" alt></p>
<p>还记得前面提到了momentum这个方法，RMSProp+momentum就得到了一个高级版的策略：Adam。</p>
<h4 id="Adam论文链接"><a href="#Adam论文链接" class="headerlink" title="Adam论文链接"></a>Adam<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1412.6980.pdf">论文链接</a></h4><p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319220458633.png" alt></p>
<h4 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h4><p>这些优化器在pytorch等框架中都写好了，其中也包括很多超参数。李宏毅老师还提到往往用默认的超参数就够好了，自己调有时候反而会调到比较差的，炼丹玄学~</p>
<h3 id="learning-rate-scheduling"><a href="#learning-rate-scheduling" class="headerlink" title="learning rate scheduling"></a>learning rate scheduling</h3><p>现在让我们回到前面提到的error surface上，加上Adagrad方法后，训练效果如下图右下角的子图所示：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319221217246.png" alt></p>
<p>加了Adagrad以后，在横轴方向learning rate会自动变大，从而能尽快接近最优解。但我们可以看到一个奇怪的现象，走到非常接近终点的位置，梯度突然爆炸了，这是为什么呢？</p>
<p>从Adagrad的$\sigma$计算方式可以回答这个问题，计算$σ$时，把过去所有看到的gradient,都拿来作平均：</p>
<ul>
<li>所以这个纵轴的方向，在初始的时候gradient很大，学习率比较小；</li>
<li><p>可是继续走了很长一段路以后，gradient算出来都很小，于是y轴的方向就开始积累很小的σ；</p>
</li>
<li><p>累积到一个地步以后，学习率就变很大，于是发生爆炸；</p>
</li>
<li><p>爆炸后其实也没关系，因为爆炸后就走到gradient比较大的地方，于是σ又慢慢的变大，参数update学习率又慢慢的变小。</p>
</li>
<li>但是累计一段时间又会爆炸，然后恢复，反复。</li>
</ul>
<p>解决这种问题的策略是<strong>learning rate scheduling</strong>，这里介绍两种：</p>
<ol>
<li>Learning Rate Decay（学习率衰减策略）</li>
<li>warm up</li>
</ol>
<h4 id="Learning-Rate-Decay（学习率衰减策略）"><a href="#Learning-Rate-Decay（学习率衰减策略）" class="headerlink" title="Learning Rate Decay（学习率衰减策略）"></a>Learning Rate Decay（学习率衰减策略）</h4><p>在前面使用的Adagrad中，$\eta$被当作一个固定的值，而learning rate scheduling是指让$\eta$和时间有关，常见的策略为：Learning Rate Decay（学习率衰减策略），随着训练不断进行，参数不断更新，$\eta$越来越小。</p>
<p>于是：一开始距离终点很远，随着参数不断update，距离终点越来越近，把learning rate减小，让参数更新减速，所以前面的那个问题，加上Learning Rate Decay可以解决。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319222132512.png" alt></p>
<h4 id="warm-up"><a href="#warm-up" class="headerlink" title="warm up"></a>warm up</h4><p>Warm Up的方法是<strong>让learning rate,要先变大后变小</strong>。<strong>其中变大到多大、变大速度、变小到多小、变小速度都是超参数。</strong>如下图所示：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319223155277.png" alt></p>
<p>但是为什么需要Warm Up，为什么Warm Up有效都是待研究的问题，但在很多文献中，它就是work了，比如BERT、transformer等等模型中都用到了warm up这个技术。</p>
<p>李宏毅老师提到了一个可能的解释：</p>
<p>​    当我们在用Adam RMS Prop，或Adagrad的时候，需要计算σ，它是一个统计的结果，<strong>σ告诉我们，某一个方向它到底有多陡,或者是多平滑</strong>，这个统计的结果，<strong>要看得够多笔数据以后才比较精确，所以一开始我们的统计是不精确的</strong>。于是我们一开始不要让参数走离初始的地方太远，先让它在初始的做一些探索。所以<strong>一开始learning rate比较小,是让它探索 收集一些有关error surface的情报</strong>，先收集有关σ的统计数据，<strong>等σ统计得比较精準以后，在让learning rate呢慢慢地增大</strong>。</p>
<h3 id="自动调整学习率方法总结"><a href="#自动调整学习率方法总结" class="headerlink" title="自动调整学习率方法总结"></a>自动调整学习率方法总结</h3><p>我们逐步将原始梯度下降方法进行优化：</p>
<ol>
<li>加入momentum；</li>
<li>加入自适应iteration和参数的$\sigma$；</li>
<li>使用自适应iteration的$\eta$；</li>
</ol>
<p>注意：momentum和$σ$虽然都与过去所有的gradient有关，一个放在分母，一个放在分子，但是momentum考虑了方向，而$σ$只考虑了gradient的大小。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319224032465.png" alt></p>
<h2 id="另一个角度-铲平error-surface？"><a href="#另一个角度-铲平error-surface？" class="headerlink" title="另一个角度-铲平error surface？"></a>另一个角度-铲平error surface？</h2><p>之前考虑的常见都是假设error surface非常崎岖情况下怎么找到比较好的参数，但其实我们可以从error surface出发，考虑如何把error surface变成一个比较平坦的，好训练的error surface，如下图所示：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210319224221772.png" alt></p>
<p>通过修改损失函数和batch normalization可能可以做到修改error surface，让模型更好训练。</p>
<h2 id="损失函数loss-function"><a href="#损失函数loss-function" class="headerlink" title="损失函数loss function"></a>损失函数loss function</h2><p>在讲分类模型的时候，李宏毅老师解释了为什么cross entropy更常用在分类上 ，并且用一个例子展示了loss function对error surface的影响。</p>
<p>我们这里直接看这个例子：</p>
<p>现在我们用下面这个模型做一个3个Class的分类任务，下面两个坐标系中的图是当loss function分别设定为Mean Square Error和Cross-entropy的时候，算出来的Error surface（$y₁、y₂$的变化对loss的影响）。在红色部分（左上角）loss很大，蓝紫色部分（右下角）loss比较小。所以我们希望在训练后，参数走到右下角的地方。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210320205913652.png" alt></p>
<p>假设<strong>我们开始的地方,都是左上角</strong>：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210320210341112.png" alt></p>
<ul>
<li>如果<strong>loss function 是Cross-Entropy</strong>，那么error surface的左上角这个地方是有斜率的，所以可以通过gradient descent一路往右下的地方走；</li>
<li>如果<strong>loss function 是Mean square error</strong>的话，Mean square error在左上角这种Loss很大的地方，是非常平坦的，也就是说它的gradient是非常小趋近于0的。如果初始的时候在这个地方，离目标非常远，它gradient又很小，就会没有办法用gradient descent顺利的走到右下角的地方去（当然用Adam还是有可能可以成功训练起来的，不过训练起步比较慢，训练更困难）。</li>
</ul>
<h3 id="总结loss-function"><a href="#总结loss-function" class="headerlink" title="总结loss function"></a>总结loss function</h3><p>由此例可以发现，<strong>Loss function的定义也可能影响error surface从而影响Training，选一个合适的loss function可以改变optimization的难度。</strong></p>
<p>这里补充一个资料，对于一些常见任务应该选择什么样的loss function进行了总结。<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39226755/article/details/89355974">深度学习中常见的激活函数与损失函数的选择与介绍</a></p>
<h2 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h2><p>Batch Normalization是修改error surface，让模型更好训练的其中一个方法。</p>
<p>比如现在有两个参数，<strong>它们对 Loss 的斜率差别非常大</strong>，在$w_1$这个方向上面斜率变化很小，在$w_2$这个方向上斜率变化很大。如下图左边所示。</p>
<p>之前提到用daptive 的 learning rate，比如Adam 等等比较进阶的 optimization 的方法去更新参数，其实另一个角度就是<strong>把这种很难训练的error surface改掉，</strong>改成下图中右边所示。</p>
<p>于是我们需要思考：<strong>斜率差很多</strong>的这种状况，到底是哪里来的？</p>
<p><img src="/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/image-20210923104855993.png" alt></p>
<p>假设我们的模型上图中下半部分的简单模型：</p>
<ul>
<li>输入的$x_1$很小，$w_1$有小小的改变，对输出$y$的影响比较小，对$L$的影响也小；</li>
<li>如果$x_2$取值很大，$w_2$变化很小也会造成$y$很大的变化，于是对$L$的影响也大；</li>
<li>所以如果$x_1,x_2$的取值范围差别过大（<strong>每一个 dimension 的值scale 差距很大</strong>）会导致不同方向斜率差别很大。</li>
</ul>
<p>因此，<strong>希望给不同的 dimension，同样的数值范围（转为上图右边部分），将原本的error surface变成比较好训练的error surface。</strong>可以完成这个操作的方法统称为feature normalization。</p>
<h3 id="feature-normalization"><a href="#feature-normalization" class="headerlink" title="feature normalization"></a>feature normalization</h3><p>feature normalization可以<strong>让Loss 收敛更快一点，让梯度下降更顺利一点。</strong></p>
<p>下面举一个常用的feature normalization方法：利用均值和方差进行标准化standardization。</p>
<p>假设$x^1$到$x^R$是所有的训练数据的 feature vector，把所有训练数据的 feature vector ,统统都集合起来，$x_1^1$代表$x_1$的第一个 element,$x_1^2$就代表$x_2$的第一个 element,以此类推。</p>
<p>把<strong>不同样本即不同 feature vector,同一个 dimension</strong> 里面的数值计算均值mean $m_i$，并且计算第 $i$ 个 dimension 的标准差standard deviation $\sigma_i$。</p>
<p>然后用下式进行<strong>标准化</strong>standardization，然后<strong>用标准化后的$\tilde x_i^r$作为模型的输入。</strong></p>
<script type="math/tex; mode=display">
\tilde x_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i}</script><p>示意图如下：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210426204629991.png" alt></p>
<p>做完 normalize 以后，这个 dimension 上面的数值平均是 0， variance是 1，所以<strong>一排数值的分布就都会在 0 上下</strong>。对每一个 dimension都做一样的 normalization,就会发现所有 feature 不同 dimension 的数值都在 0 上下，那可能就可以<strong>构造比较好的 error surface</strong>。</p>
<h3 id="feature-normalization-for-deep-learning"><a href="#feature-normalization-for-deep-learning" class="headerlink" title="feature normalization for deep learning"></a>feature normalization for deep learning</h3><p>我们先用feature normalization对最原始的输入做了处理得到$\tilde x$，那么经过第一层参数时，我们的输入各个维度的scale就是差不多的，但是深度学习的模型都是有很多层的，像下图这样<strong>：上一层的输出是下一层的输入，那么经过了一层神经网络的特征（输出$z^1$或经过sigmoid层的输出$a^1$）各个维度可能又会有不同的scale。</strong></p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210426210742842.png" alt></p>
<p>所以我们可以对$z$或者$a$做normalization。这里也有一个tips：</p>
<ul>
<li><strong>一般来说，这个 normalization,要放在 activation function 之前,或之后都是可以的，在实操上，可能没有太大的差别</strong>。</li>
<li>不过，如果选择的是 Sigmoid作为激活函数，那可能比较推荐对 $z$ 做 Feature Normalization因为Sigmoid 是一个 s 的形状，它在 0 附近斜率比较大。所以如果对$ z$ 做 Feature Normalization，把所有的值都挪到 0 附近，算 gradient 的时候值会比较大。</li>
<li>因为激活函数不一定是选 sigmoid，所以也不一定要对$z$做 Feature Normalization，如果是选别的激活函数，也许对$a$做normalization可能会有好的结果。</li>
</ul>
<p>我们这边<strong>假设对$z$做feature normalization，举个例子</strong>：</p>
<p>和之前对$x$做的操作类似，对$z$也是计算均值和方差，然后进行归一化：</p>
<p><img src="/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/image-20210923123114107.png" alt></p>
<p>这里有个地方需要特别注意：</p>
<p>上图中的$\mu$跟$\sigma$ ,它们其实都是根据$z^1,z^2,z^3$算出来的，那么：</p>
<ul>
<li>如果没有做feature normalization，因为$\tilde x$是处理后单独输入的，修改$z^1$只会影响$\tilde z^1$和$a^1$；</li>
<li>做了feature normalization后，修改$z^1$会影响$\mu$和$\sigma$，进而不仅影响$\tilde z^1$和$a^1$，还会影响$\tilde z^2$和$a^2$、$\tilde z^3$和$a^3$；</li>
<li>于是这三个  example,它们变得<strong>彼此关联</strong>了。如下图所示：</li>
</ul>
<p><img src="/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/image-20210923124510380.png" alt></p>
<p>因为彼此关联，于是整个process（包括根据feature算均值方差）就变成了network的一部分，所以现在变成一个比较大的network，如下图所示。</p>
<ul>
<li>之前的 network,都只输入一个 input,得到一个 output；</li>
<li>现在有一个比较大的 network是输入一堆 input,用这堆 input 在这个 network 里面，要算出均值和方差,然后输出一堆 output。</li>
</ul>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210427085228640.png" alt></p>
<p>自然而然地产生一个问题，是不能一次性把很大的一个训练集全部输入网络中训练的，因为内存有限。</p>
<p>再自然而然地就可以考虑不输入全部的数据，而是输入一部分（batch），每次只考虑一个batch里的样本，所以在实际操作中，是对一个batch做normalization，于是这个方法叫batch normalization。</p>
<h3 id="normalization-in-each-batch-batch-normalization"><a href="#normalization-in-each-batch-batch-normalization" class="headerlink" title="normalization in each batch - batch normalization"></a>normalization in each batch - batch normalization</h3><p>显然一定要有一个够大的 batch，才算得出均值和方差，假设batch size=1，那么均值和方差就没啥好算的。</p>
<p>所以<strong>Batch Normalization适用于 batch size 比较大的时候</strong>。<strong>相当于我们用一个batch size的训练数据分布估计整个数据集的数据分布。如果 batch size 比较大，也许这个 batch size 里的 data足以表示整个 corpus 的分布</strong>。这个时候就可以把对整个 corpus做 Feature Normalization改成只在一个 batch做 Feature Normalization。</p>
<h4 id="进阶的batch-normalization"><a href="#进阶的batch-normalization" class="headerlink" title="进阶的batch normalization"></a>进阶的batch normalization</h4><p>在做 Batch Normalization 的时候，往往还会对计算出来的$\tilde z$进行下一步操作，得到最终的$\hat z$：</p>
<script type="math/tex; mode=display">
\hat z^i=\gamma \odot \tilde z^i+\beta</script><p>其中$\gamma$和$\beta$是 network 的参数，在训练过程中被学习到的。如下图所示：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210427090245986.png" alt></p>
<h5 id="为什么要加上-beta-和-gamma-？"><a href="#为什么要加上-beta-和-gamma-？" class="headerlink" title="为什么要加上$\beta $和$\gamma$？"></a>为什么要加上$\beta $和$\gamma$？</h5><p>有一种猜测是，<strong>做 normalization 以后feature的平均就一定是 0，这可能会给 network 一些限制，也许这个限制会带来什麼负面的影响</strong>。所以在训练的时候，将$\beta $和$\gamma$作为训练参数，重新调整其分布，让它的 hidden layer 的 output平均不是 0 。</p>
<p>如果是这样，那可能又会问：<strong>上一步做Batch Normalization 就是要让每一个不同的 dimension的 range 都是一样的，现在如果做这一步，不是又让 dimension 的分布不一样了吗？</strong></p>
<p>答案是：确实有可能。以我们<strong>在初始的时候，将$\beta $内的元素都设置为 1，$\gamma$内的元素都设置为0。</strong>于是 network 在一开始训练的时候,每一个 dimension 的分布,是比较接近的，也许训练到后来，已经找到一个比较好的 error surface，走到一个比较好的地方以后，再慢慢调整$\beta $和$\gamma$。</p>
<h3 id="batch-normalization-for-testing-inference"><a href="#batch-normalization-for-testing-inference" class="headerlink" title="batch normalization for testing/inference"></a>batch normalization for testing/inference</h3><p>之前说的都是training的时候要做什么，在testing（inference）的时候，想用batch normalization会有什么问题呢？</p>
<p>batch normalization需要一个batch的数据计算$\mu$和$\sigma$，在testing的时候，首先遇到的问题是，可能输入不够一个batch size，因为在工程上不可能等数据攒到一个batch size才开始计算输出。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210427101815357.png" alt></p>
<p><strong>也就是可能根本就不是一个batch的输入，应该怎么获得$\mu$和$\gamma$呢？</strong></p>
<p>在实际操作中，如果是PyTorch 的话，Batch Normalization 在 testing 的时候，其使用的$\mu$和$\gamma$是通过：如果training的时候有做 Batch Normalization 的话，在 training 的时候，<strong>每一个 batch 计算出来的$\mu$和$\sigma$ 都计算moving average，然后得到平均值（$\bar \mu$和$\bar \sigma$），将这两个值作为testing的$\mu$和$\sigma$</strong>。如下图所示。</p>
<p>moving average的计算过程是：每一次取一个 batch 出来的时候,就会算一个$\mu^1$，取第二个 batch 出来的时候算$\mu^2$ ,一直到取第 t 个 batch 出来的时候算$\mu^t$，利用$\mu^1…\mu^{t-1}$算一个平均值得到$\bar \mu$然后结合一个超参数$p$更新$\bar\mu$。</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210427101956211.png" alt></p>
<p>看看加了batch normalization的训练效果：横轴是训练过程，纵轴是在验证集上的精度。</p>
<ol>
<li>训练速度变快，收敛速度变快；</li>
<li>更好训练（sigmoid比较难训练，但是加入batch normalization后成功训练起来了）；</li>
<li>如果<strong>做 Batch Normalization 的话,error surface 会比较平滑 比较容易训练，所以可以把 learning rate 设大一点</strong>，（但是这里有个问题是：learning rate 设 30 倍的时候比 5 倍差，作者也没有解释为什么）。</li>
</ol>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210427102536377.png" alt></p>
<h3 id="internal-covariate-shift"><a href="#internal-covariate-shift" class="headerlink" title="internal covariate shift"></a>internal covariate shift</h3><p>在原始的 Batch Normalization那篇 paper 作者提出来一个概念,叫做 internal covariate shift。（友情先提醒这个猜测已经被推翻了）</p>
<p>其中<strong>covariate shift</strong>是本身存在的概念：<strong>训练集和预测集样本分布不一致的问题就叫做“<em>covariate shift</em>”现象</strong>。</p>
<p>internal covariate shift指的是：当我们在计算 B,update 到 B′ 的 gradient 的时候，这个时候前一层的参数是 A （或者说是前一层的 output 是a），那当前一层从 A 变成 A′ 的时候,它的 output 就从 a 变成 a′ 。但是我们计算这个 gradient 的时候,我们是根据这个 a 算出来的，所以这个 update 的方向,也许它<strong>适合用在 a 上,但不适合用在 a′ 上面</strong>。</p>
<p>所以如果每次都有做 normalization,可能就会让 a 跟 a′ 的分布比较接近,也许这样就会对训练有帮助。</p>
<p>如下图所示：</p>
<p><img src="https://gitee.com/unclestrong/deep-learning21_note/raw/master/imgbed/image-20210427105049445.png" alt></p>
<p>但是！</p>
<p>有一篇 paperHow Does Batch Normalization,Help Optimization就<strong>推翻了internal covariate shift 的这一个观点</strong>。</p>
<p>作者从各种各样的角度说明i<strong>nternal covariate shift它不一定是 training network 的时候的一个问题，且Batch Normalization会比较好不一定是因为解决了 internal covariate shift问题</strong>。</p>
<p>作者通过比较训练时a 分布的变化发现：</p>
<ol>
<li><strong>不管有没有做 Batch Normalization,它的变化都不大</strong>；</li>
<li>就算是变化很大，对 training 也没有太大的伤害；</li>
<li>不管你是根据 a 算出来的 gradient,还是根据 a′ 算出来的 gradient,方向都差不多。</li>
</ol>
<p>这篇 How Does Batch Normalization,Help Optimization 论文中从实验上、也从理论上至少<strong>支持了 Batch Normalization,可以改变 error surface,让 error surface 比较不崎岖这个观点</strong>。并且说：<strong>如果我们要让 network的error surface 变得比较不崎岖，不一定要做 Batch Normalization，还有很多其他的方法（作者也试了一些其他的方法，见原文实验）</strong>，batch normalization只是一个<strong>serendipitous</strong>（意料之外的发现），恰好work了而已。</p>
<h3 id="其他normalization的方法"><a href="#其他normalization的方法" class="headerlink" title="其他normalization的方法"></a>其他normalization的方法</h3><p>主要包括以下几种方法：BatchNorm（2015年）、LayerNorm（2016年）、InstanceNorm（2016年）、GroupNorm（2018年）。这几个方法在之前总结Transformer时简单总结过了，可以参考：<a href="https://ifwind.github.io/2021/08/17/Transformer相关——（6）Normalization方式/#为什么要做normalization">Transformer相关——（6）Normalization方式</a></p>
<p><img src="https://ifwind.github.io/2021/08/17/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%886%EF%BC%89Normalization%E6%96%B9%E5%BC%8F/normalization%20method.png" alt></p>
<p>normalization方法和paper原文如下：</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1702.03275">Batch Renormalization</a>  </p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>  </p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.08022">Instance Normalization</a> </p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.08494">Group Normalization</a> </p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07868">Weight Normalization</a>  </p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.10941">Spectrum Normalization</a> </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://unclestrong.github.io/DeepLearning_LHY21_Notes/Notes_html/05_Batch and Momentum.html">李宏毅深度学习2021春季笔记</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Wv411h7kN?p=15">(强推)李宏毅2021春机器学习课程</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">冬于</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ifwind.github.io/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/">https://ifwind.github.io/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ifwind.github.io" target="_blank">冬于的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E8%AE%AD%E7%BB%83/">训练</a></div><div class="post_share"><div class="social-share" data-image="/2021/09/22/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%81%E9%9D%A2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/09/27/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%AC%AC8%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"><img class="prev-cover" src="/2021/07/13/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%AC%AC1%E7%AB%A0-%E7%BB%AA%E8%AE%BA/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">西瓜书阅读笔记——第8章-集成学习</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><img class="next-cover" src="/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/huggingFace.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">BERT实战——（7）生成任务-语言模型</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/15/Transformer相关——（1）Encoder-Decoder框架/" title="Transformer相关——（1）Encoder-Decoder框架"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-15</div><div class="title">Transformer相关——（1）Encoder-Decoder框架</div></div></a></div><div><a href="/2021/08/16/Transformer相关——（2）Seq2Seq模型/" title="Transformer相关——（2）Seq2Seq模型"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-16</div><div class="title">Transformer相关——（2）Seq2Seq模型</div></div></a></div><div><a href="/2021/08/17/Transformer相关——（4）Poisition encoding/" title="Transformer相关——（4）Poisition encoding"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-17</div><div class="title">Transformer相关——（4）Poisition encoding</div></div></a></div><div><a href="/2021/08/17/Transformer相关——（5）残差模块/" title="Transformer相关——（5）残差模块"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-17</div><div class="title">Transformer相关——（5）残差模块</div></div></a></div><div><a href="/2021/08/17/Transformer相关——（6）Normalization方式/" title="Transformer相关——（6）Normalization方式"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-17</div><div class="title">Transformer相关——（6）Normalization方式</div></div></a></div><div><a href="/2021/08/17/Transformer相关——（7）Mask机制/" title="Transformer相关——（7）Mask机制"><img class="cover" src="/2021/08/15/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89Encoder-Decoder%E6%A1%86%E6%9E%B6/Transformer1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-17</div><div class="title">Transformer相关——（7）Mask机制</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02021%E6%98%A5p5-9%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><span class="toc-number">1.</span> <span class="toc-text">李宏毅深度学习2021春p5-9：神经网络训练技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">训练遇到的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BClocal-minima%E5%92%8C%E9%9E%8D%E7%82%B9saddle-point"><span class="toc-number">1.2.</span> <span class="toc-text">局部最小值local minima和鞍点saddle point</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Critical-Point"><span class="toc-number">1.2.1.</span> <span class="toc-text">Critical Point</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#local-minima"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">local minima</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#saddle-point"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">saddle point</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%9F%90%E4%B8%AA%E4%BD%8D%E7%BD%AE%E6%98%AFlocal-minima%E8%BF%98%E6%98%AFsaddle-point%EF%BC%9F"><span class="toc-number">1.2.2.</span> <span class="toc-text">如何判断某个位置是local minima还是saddle point？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87Hession%E7%9F%A9%E9%98%B5%E5%88%A4%E6%96%AD-theta%E2%80%99-%E9%99%84%E8%BF%91%E7%9A%84error-surface"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">通过Hession矩阵判断$\theta’$附近的error surface</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%80%83%E7%A6%BBsaddle-point%EF%BC%9F"><span class="toc-number">1.2.3.</span> <span class="toc-text">如何逃离saddle point？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Saddle-Point-v-s-Local-Minima"><span class="toc-number">1.2.4.</span> <span class="toc-text">Saddle Point v.s. Local Minima</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#batch-mini-batch%E5%92%8C%E5%8A%A8%E9%87%8FMomentum"><span class="toc-number">1.3.</span> <span class="toc-text">batch&#x2F;mini-batch和动量Momentum</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#batch-mini-batch"><span class="toc-number">1.3.1.</span> <span class="toc-text">batch&#x2F;mini-batch</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Optimization-with-Batch"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Optimization with Batch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81batch"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">为什么要batch</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%8C%E4%B8%80%E4%B8%AAmodel%EF%BC%8Cbatchsize%E8%BF%87%E5%A4%A7%E6%95%88%E6%9E%9C%E5%8F%8D%E8%80%8C%E6%9B%B4%E5%B7%AE%EF%BC%9F"><span class="toc-number">1.3.1.2.1.</span> <span class="toc-text">同一个model，batchsize过大效果反而更差？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%B0%8F%E7%9A%84-Batch-Size%E5%9C%A8-Training-Set-%E4%B8%8A%E4%BC%9A%E5%BE%97%E5%88%B0%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%9F"><span class="toc-number">1.3.1.2.2.</span> <span class="toc-text">为什么小的 Batch Size在 Training Set 上会得到比较好的结果？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%A6%E5%A4%96%E4%B8%80%E4%B8%AA%E7%A5%9E%E5%A5%87%E7%9A%84%E4%BA%8B%EF%BC%9A%E5%B0%8F%E7%9A%84batch%E5%9C%A8testing%E4%B9%9F%E6%9C%89%E5%B8%AE%E5%8A%A9"><span class="toc-number">1.3.1.2.3.</span> <span class="toc-text">另外一个神奇的事：小的batch在testing也有帮助</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93batch-mini-batch"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">总结batch&#x2F;mini-batch</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum"><span class="toc-number">1.3.2.</span> <span class="toc-text">Momentum</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">传统的梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Descent-Momentum"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Gradient Descent + Momentum</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87Adaptive-Learning-Rate"><span class="toc-number">1.4.</span> <span class="toc-text">自动调整学习率Adaptive Learning Rate</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#loss%E4%B8%8D%E5%86%8D%E4%B8%8B%E9%99%8D%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8Cgradient%E7%9C%9F%E7%9A%84%E5%BE%88%E5%B0%8F%E5%90%97%EF%BC%9F"><span class="toc-number">1.4.1.</span> <span class="toc-text">loss不再下降的时候，gradient真的很小吗？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#learning-rate%E8%87%AA%E5%8A%A8%E8%B0%83%E6%95%B4%E7%AD%96%E7%95%A5%EF%BC%9A%E4%B8%BA%E4%B8%8D%E5%90%8C%E7%9A%84%E5%8F%82%E6%95%B0%E5%92%8C%E4%B8%8D%E5%90%8C%E7%9A%84iteration%E8%AE%BE%E7%BD%AE%E4%B8%8D%E5%90%8C%E7%9A%84learning-rate"><span class="toc-number">1.4.2.</span> <span class="toc-text">learning rate自动调整策略：为不同的参数和不同的iteration设置不同的learning rate</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E4%BA%8EAdagrad%E4%B8%AD%E7%9A%84Root-mean-square"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">用于Adagrad中的Root mean square</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88Root-mean-square%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%88%B0%E5%9D%A1%E5%BA%A6%E6%AF%94%E8%BE%83%E5%A4%A7%E7%9A%84%E6%97%B6%E5%80%99-learning-rate%E5%B0%B1%E5%87%8F%E5%B0%8F-%E5%9D%A1%E5%BA%A6%E6%AF%94%E8%BE%83%E5%B0%8F%E7%9A%84%E6%97%B6%E5%80%99-learning-rate%E5%B0%B1%E6%94%BE%E5%A4%A7%E5%91%A2"><span class="toc-number">1.4.2.1.1.</span> <span class="toc-text">为什么Root mean square可以做到坡度比较大的时候,learning rate就减小,坡度比较小的时候,learning rate就放大呢?</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RMSProp"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adam%E8%AE%BA%E6%96%87%E9%93%BE%E6%8E%A5"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">Adam论文链接</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tips"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">tips</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#learning-rate-scheduling"><span class="toc-number">1.4.3.</span> <span class="toc-text">learning rate scheduling</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Learning-Rate-Decay%EF%BC%88%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E7%AD%96%E7%95%A5%EF%BC%89"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">Learning Rate Decay（学习率衰减策略）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#warm-up"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">warm up</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.4.</span> <span class="toc-text">自动调整学习率方法总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%A6%E4%B8%80%E4%B8%AA%E8%A7%92%E5%BA%A6-%E9%93%B2%E5%B9%B3error-surface%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">另一个角度-铲平error surface？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0loss-function"><span class="toc-number">1.6.</span> <span class="toc-text">损失函数loss function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93loss-function"><span class="toc-number">1.6.1.</span> <span class="toc-text">总结loss function</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#batch-normalization"><span class="toc-number">1.7.</span> <span class="toc-text">batch normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-normalization"><span class="toc-number">1.7.1.</span> <span class="toc-text">feature normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-normalization-for-deep-learning"><span class="toc-number">1.7.2.</span> <span class="toc-text">feature normalization for deep learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#normalization-in-each-batch-batch-normalization"><span class="toc-number">1.7.3.</span> <span class="toc-text">normalization in each batch - batch normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E9%98%B6%E7%9A%84batch-normalization"><span class="toc-number">1.7.3.1.</span> <span class="toc-text">进阶的batch normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%A0%E4%B8%8A-beta-%E5%92%8C-gamma-%EF%BC%9F"><span class="toc-number">1.7.3.1.1.</span> <span class="toc-text">为什么要加上$\beta $和$\gamma$？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#batch-normalization-for-testing-inference"><span class="toc-number">1.7.4.</span> <span class="toc-text">batch normalization for testing&#x2F;inference</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#internal-covariate-shift"><span class="toc-number">1.7.5.</span> <span class="toc-text">internal covariate shift</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96normalization%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.7.6.</span> <span class="toc-text">其他normalization的方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.8.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 冬于</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
      appKey: 'YKdl4HKX9W6jLSdPlypgEtDM',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: 'https://e4lpmfet.lc-cn-n1-shared.com',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.17.0/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://e4lpmfet.lc-cn-n1-shared.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'E4LPmFetYyaT4NwjEGOl0u8Q-gzGzoHsz',
        "X-LC-Key": 'YKdl4HKX9W6jLSdPlypgEtDM',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>